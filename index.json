[{"categories":["Backend"],"content":" Ein Schritt wirklicher Bewegung ist wichtiger als ein Dutzend Programme. ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:0:0","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"前言 本文旨在从“思考”、“方法”与“实践”三个层面，系统性地勾勒出后台开发的完整蓝图。 思考篇将深入探讨软件开发的底层逻辑，追溯后台技术的演进历史，并明晰其在当今软件行业中的定位与核心技术栈。 方法篇将聚焦于通用的软件开发方法论，重点介绍领域驱动设计（DDD）与属性驱动设计（ADD）等现代架构思想，并分享一套实用的架构设计工具箱。 实践篇将以一个真实项目为例，综合运用前述的理论与方法，将抽象概念付诸工程实践。 最后，本文还将对AI时代下的云原生架构形态进行前瞻性探讨。 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:1:0","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"思考 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:2:0","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"软件架构与团队 软件开发的根本目的，在于运用计算机这一强大工具，高效地满足用户的多样化需求。正因需求的千变万化，才催生了形态各异的软件。从本质上讲，任何软件都可以视为一个庞大的状态机（State Machine）。正是其状态空间的浩瀚与复杂，使其能够精准地对现实世界进行建模，从而解决实际问题。 然而，现实世界问题的复杂性，意味着软件的状态空间也随之变得极其庞大，远超人脑的处理极限。我们的工作记忆（Working Memory）是有限的，心理学研究表明，我们通常只能同时处理大约七个（±2）事务。因此，我们不可能像解决一道算法题那样，将整个软件系统的所有细节都装入脑中。 解决方案在于系统化思维与抽象（Abstraction）。 我们可以将软件看作一个由不同层次构成的系统。每一层都通过若干子系统的协作来解决一个范畴的问题，而每个子系统内部又可以继续分层。抽象的本质是信息隐藏（Information Hiding）。通过层层抽象，隐藏非必要的细节，我们便能将一个原本无法一手掌握的复杂系统，简化到可以被我们的大脑理解和容纳的程度。 当我们能够自顶向下地清晰阐述系统在最高层如何通过子系统交互实现目标，并能逐层深入、分而治之，直至每个子系统的复杂度都能被工作记忆轻松处理时，我们才算真正“理解”了该系统。所谓的“理解”，就是能够清晰地阐述系统如何达成特定目标，甚至有能力从零开始重新构建它。 以一个简单的例子说明：在 IDE 中运行 Java 代码，就可以看作一个多层次的抽象系统： 最高层抽象（用户视角）： 在 IDE 中点击“运行”按钮，代码成功执行并输出结果。 往下一层（IDE 与 JDK 工具链）： IDE 调用 javac 编译器，将 .java 源文件编译成 .class 字节码文件。 IDE 调用 java 命令，启动 Java 虚拟机（JVM）来执行这些字节码。 再往深一层（JVM 内部）： JVM 启动，进行类加载、验证、准备和解析。 字节码被解释器逐行执行，或由即时编译器（JIT）编译为本地机器码以提升效率。 执行过程中，JVM 会管理内存（堆、栈、方法区等）和线程。 继续深入（操作系统与硬件）： 操作系统调度进程，将机器码指令送往 CPU 执行。 CPU 执行指令，与内存、磁盘等硬件交互。 通过这样逐层分解，一个看似简单的“一键运行”背后所隐藏的复杂链路，便清晰地展现在我们面前。当我们遇到问题，例如“如何提升 Java 程序的执行效率？”，就可以基于这些不同的抽象层次进行分析： 从 Java 语言和应用层来看： 是否可以优化算法、减少不必要的对象创建？ 从 JVM 层来看： 是否可以通过调整 JVM 参数（如堆大小、垃圾收集器）来优化？ 从操作系统/硬件层来看： 是否是 I/O 瓶颈或 CPU 资源不足？ 这种精心设计的抽象分层，正是我们所说的软件架构（Software Architecture）。 架构之所以至关重要，在于它的普适性与稳定性。一个优秀的架构模式可以被复用于无数个不同的软件项目。所有高阶的软件设计思想，都是建立在架构这一基础之上的，例如： 罗伯特·马丁（Robert C. Martin） 强调的模块化与依赖关系原则。 领域驱动设计（DDD） 中对业务复杂度的划分与隔离。 架构权衡分析方法（ATAM） 对质量属性（如性能、安全性）的系统性考量。 然而，无论架构设计得多么精妙，终究需要人来落地，而且通常需要一个团队协作完成。这就引出了一个著名的定律——康威定律（Conway’s Law）： “设计系统的组织，其产生的设计等价于组织间的沟通结构。” (Organizations which design systems … are constrained to produce designs which are copies of the communication structures of these organizations.) 简而言之，软件架构会趋向于复制组织的结构。这是因为团队内部的沟通成本远低于跨团队的沟通成本，因此系统的边界自然而然地会沿着团队的边界进行划分。 循着这条线索，我们便能勾勒出一幅清晰的图景： 软件架构 ⟺ 组织结构 ⟺ 团队与岗位 ⟺ 个人能力 对于我们求职者而言，理解这套对应关系就显得尤为重要。当你研究一家公司的岗位需求时，你不仅仅是在看一个孤立的职位描述，更是在窥探其背后的软件架构理念和组织运作方式。理解了这一点，你就能更好地展现自己的价值，并找到真正适合自己的位置。 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:2:1","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"互联网企业的基本图景 互联网企业的核心业务模式，是通过构建和运营软件服务来获取商业价值。无论是游戏、电商、即时通讯还是本地生活，这些看似迥异的赛道，其底层的软件与组织结构却有着惊人的相似性。 技术与组织：前后端的天然分野 一套完整的服务，必然包含运行在用户设备上的客户端（前端）和运行在公司服务器上的服务端（后台）。前端的核心关注点是用户体验、交互效果和跨平台兼容性；而后台则聚焦于高性能、高可用、高并发和数据一致性。迥异的目标与质量属性要求，使得前后端的技术栈（Tech Stack）天然分离。这种技术上的分野，直接映射到组织结构上，形成了我们熟知的“大前端”与“后台/服务端”两大技术族群。为了保障这条核心研发流水线的顺畅，测试（QA）、运维（SRE/DevOps）等支持性岗位应运而生，共同构成了研发体系的主体。 后台深潜：基础、中台与业务 庞大的后台系统，自身也存在着精细的分层。我们可以将其大致分为三个层次： 基础软件/平台（Infrastructure）： 这是最底层，提供计算、存储、网络等最通用、最稳固的能力，如数据库、缓存系统、消息队列、容器化平台等。这一层离业务最远，追求极致的性能和稳定性，其功能相对固定。 中台（Middle Platform）： 位于中间层，它将一些通用的业务能力（如用户中心、订单系统、支付网关）从具体的业务线中沉淀出来，形成共享服务，供多个前台业务复用。它承上启下，旨在提升研发效率，避免重复造轮子。 业务后台（Business Backend）： 这是最贴近用户的业务逻辑层，直接服务于特定的业务场景（如电商的商品详情页、外卖的点餐流程）。这一层的核心是快速响应市场变化，敏捷迭代，因此对业务理解的要求最高。 从基础软件到业务后台，可以看作是从“左”到“右”的依赖关系：越靠右的层级，越关注具体业务，功能迭代快，但会依赖左边更稳固的底层服务；越靠左的层级，功能越通用、稳定，但对性能、可用性等质量属性的要求也越严苛。 数据的价值：驱动决策与智能 数据是现代互联网服务的核心资产。除了支撑业务运行的在线事务处理数据库（OLTP），还有用于深度分析的在线分析处理（OLAP）数据仓库。正是海量数据的积累，催生了新的价值创造方式：通过数据分析洞察商业机会，通过算法模型优化业务效果（如推荐系统、广告投放），从而提升用户体验，增强盈利能力。这条价值链，也创造了算法工程师、数据科学家、数据分析师等炙手可- 热的岗位。 以上这套描述，在逻辑上概括了企业的组织全貌，即业界所称的企业架构（Enterprise Architecture）。 了解了这张“图景”，下一个关键问题是：在这张复杂的版图中，哪些团队、哪些岗位是核心？我们又该如何评估一个职位的价值？ 归根结底，企业是一个追求商业成功的组织。在任何科层制组织中，资源和决策权都向高层管理者集中。因此，一个岗位的重要性，很大程度上取决于它与“权力来源”和“价值创造”的关系。我们可以从两个核心维度来构建一个分析框架： 不可替代性（Irreplaceability）： 指的是岗位所依赖的技术壁垒或业务壁垒的高度。当一个岗位的工作需要极高的专业技能、深厚的行业知识或长时间的经验积累，以至于在市场上难以找到替代者时，该岗位就具备了强大的议价能力。 高不可替代性岗位： 例如，能够构建和维护超大规模分布式数据库的工程师，或深谙特定行业（如金融风控、广告推荐）复杂逻辑的业务架构师。 价值可见性（Visibility of Impact）： 指的是岗位成果被关键决策者（管理者）或价值来源（客户/市场）感知的清晰度和直接性。俗话说“近水楼台先得月”，离最终成果越近，功劳就越容易被看见。 高价值可见性岗位： 直接面向用户的业务团队，其功能的优劣能迅速反映在用户增长、留存等核心指标上。同样，能够为管理层提供精准数据洞察、辅助关键决策的算法或数据团队，其成果也极易被感知。 现在，我们可以将求职者最关心的三个要素——薪资、稳定、轻松——与上述两个维度进行匹配，来分析不同岗位的特点： 高价值可见性 (贴近业务/用户/决策) 低价值可见性 (成果不易直接感知) 高不可替代性 核心引擎 (Core Engine) 如：核心业务算法、商业化策略 特点：薪资最高、地位核心，但压力巨大，与“轻松”无缘。 稳定基石 (Stable Foundation) 如：基础架构、数据库内核 特点：技术护城河深，非常稳定，薪资优厚，但属于“幕后英雄”。 低不可替代性 前线部队 (Frontline Troop) 如：部分业务CRUD、运营活动开发 特点：能快速产出成果，易获短期认可，但竞争激烈，稳定性相对较差。 支持单元 (Support Unit) 如：内部工具、非核心流程开发 特点：可能相对边缘和安逸，但薪资和长期发展或受限。 因此，作为求职者，在审视一个岗位时，不应只看其薪资待遇和技术栈，更应深入思考： 这个岗位处在公司价值创造链条的哪个环节？ 它的不可替代性体现在哪里？是深厚的技术，还是复杂的业务？ 它的工作成果如何被衡量？是否容易被关键人物看到？ ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:2:2","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"后台系统的演进 我们已经描绘了互联网企业的基本组织版图。现在，让我们将焦点汇聚于其技术心脏——后台系统。在这个领域中，我们能看到负责业务逻辑实现的应用开发、保障数据生命线的数据库（DBA/数据工程师），以及构建系统间桥梁的中间件等多种岗位。 然而，为何今天的后台架构会演变成微服务、服务网格、Serverless 等复杂形态？任何架构的出现都不是空穴来风。技术，如同生物演化，每一个新物种的诞生都带有旧时代的印记。要理解“现在”的合理性，我们必须回溯“过去”的足迹。 下面，我们将沿着时间线，追溯后台系统的演进历程。 第一阶段：单体巨石时代 (约 2000-2008年) 特征： 一体化架构（Monolithic Application）、垂直扩展（Scale-up）。 在世纪之初的“网站时代”，大多数应用都被构建为单体服务。所有功能，从用户界面到业务逻辑再到数据访问，都被打包在同一个进程中。这种架构在项目初期开发效率极高，易于部署和测试。当流量增长时，主要的应对方式是“垂直扩展”——购买性能更强的服务器。 一个值得关注的技术路线分岔： 大约在2008年，中美后台技术的发展出现了一个有趣的分野。 在美国： Google 的上市和 Facebook 的崛起，带来了海量数据存储和处理的挑战。美国科技界开始将注意力转向水平扩展（Horizontal Scaling），思考如何通过成百上千的廉价服务器集群来分散压力，分布式系统的思想开始萌芽。 在中国： 当时正值大型多人在线网络游戏（MMO）的黄金年代。游戏行业的核心诉求是在单台服务器上支撑尽可能多的玩家进行高并发、低延迟的实时交互。因此，国内顶尖工程师的主要精力聚焦于榨干单机性能，在网络编程、内存管理、并发模型上深耕。 可以说，当时美国关注“分布式”，而中国关注“高并发”。 第二阶段：中间件与SOA时代 (约 2009-2013年) 特征： 面向服务的架构（SOA）、中间件（Middleware）、水平扩展。 随着 Facebook 等社交网站的爆发式增长，用户间的频繁互动（搜索、推荐、消息）产生了传统单体架构无法应对的巨大压力。将巨石应用拆分成若干个独立的服务，通过标准接口进行通信的 SOA 思想开始流行。 为了解耦这些服务，处理异步任务，中间件迎来了发展的春天。消息队列（如 RabbitMQ）、服务通信框架（如 ZeroMQ、Dubbo）等技术被广泛应用，它们构成了服务化架构的“神经系统”。后台系统开始从“单体作战”走向“协同服务”。 第三阶段：大数据与云时代 (约 2014-2017年) 特征： 大数据（Big Data）、云计算（Cloud Computing）、微服务（Microservices）。 大约从2014年起，中美两国的技术路线开始重新交汇，并以前所未有的速度融合。在中国，从“千团大战”到 O2O 创业浪潮的兴起，催生了对大数据实时处理、机器学习推荐、LBS 服务等复杂业务的真实需求。 这标志着一个转折点的到来：技术开始真正驱动业务，算法开始驱动产品。 这个时代的技术热点，至今仍是业界主流： 大数据生态： 以 Hadoop、Spark 为代表的批处理和流式处理框架成为标配。 推荐与挖掘： 数据挖掘和推荐系统从边缘功能走向业务核心。 容器化： 以 Docker 为代表的容器技术，极大地简化了应用部署和环境一致性问题，为微服务的彻底落地铺平了道路。 DevOps 文化： A/B 测试、持续集成/持续部署（CI/CD）等理念深入人心。 可以说，至今绝大多数公司，无论国内外，其技术体系仍构建在这个“云 + 大数据”的时代基石之上。微服务架构在此时被正式确立为复杂系统的主流设计模式。 第四阶段：云原生与AI时代 (约 2018年至今) 特征： AI/ML 驱动、云原生（Cloud Native）、服务网格（Service Mesh）、无服务（Serverless）。 当大数据和云计算成为基础设施后，站在技术浪潮之巅的公司开始迈向下一个纪元。从2017年底开始，“AI驱动”从口号变为了现实。 AI/ML 深入核心： 机器学习不再仅仅是离线分析工具，而是通过复杂的机器学习平台（如 Facebook 的 FBLearner、Uber 的 Michelangelo）深度融入业务核心，驱动着内容推荐、智能定价、风险控制等关键环节。 云原生技术栈成熟： 为了管理成千上万的微服务，以 Kubernetes (K8s) 为核心的容器编排系统一统江湖，成为云时代事实上的“操作系统”。为解决微服务治理难题，服务网格（Service Mesh）技术应运而生。为了极致的弹性和运维效率，无服务计算（Serverless/FaaS）也开始崭露头角。 回望这段历史，从笨重的单体，到灵活的服务；从依赖昂贵的硬件，到拥抱分布式的云；从工程师编写固定逻辑，到由数据和AI驱动决策，后台系统的每一次演进，都是为了应对更复杂的业务挑战，追求更高的研发效率和更强的系统能力。这条演进之路，仍在继续。 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:2:3","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"后台开发的 Roadmap 了解了后台系统在企业中的定位和演进脉络后，下一个自然的问题是：作为一名有志于此的开发者，应当构建怎样的能力模型？后台开发涉及的知识领域广泛，从底层技术到上层业务，都需要有扎实的积累。以下是一个后台开发工程师的成长路径图（Roadmap），它系统性地梳理了所需的核心能力：Back-End Development Roadmap。其关键能力可归纳为： 计算机科学基础 软件工程知识 业务领域知识 组织与管理能力 产品与商业思维 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:2:4","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"方法 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:3:0","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"软件开发方法 软件的本质，是利用计算机技术来解决现实世界中的特定问题、满足特定需求。然而，将一个模糊的“需求”转化为一个稳定、可靠、高效的软件系统，其过程远比想象中复杂。 “软件开发行业是如此混乱，以至于对于大多数开发人员来说，常识性的基本实践可能很陌生。然而，忽视它们做再多的事情都不会加快进度。导致问题的行为不可能用来解决问题。” 这句引言点出了软件工程的核心挑战：在混乱中建立秩序。为了驾驭这种复杂性，行业沉淀出了两大核心概念：软件开发生命周期模型 (SDLC) 和 软件开发方法 (Methodology)。理解它们的区别与联系，是在混乱中理清头绪的第一步。 软件开发生命周期模型 (Software Development Life Cycle, SDLC) 它是什么？ 软件从构思到退役的高层战略蓝图。它定义了开发过程需要经历哪些主要阶段（如需求分析、设计、编码、测试等）以及这些阶段的先后顺序。 它回答什么？ 更侧重于 “做什么” (What) 和 “什么时候做” (When)。 软件开发方法 (Software Development Methodology) 它是什么？ 指导团队如何执行具体开发活动的战术手册和实践集合。它包含了一套明确的原则、实践、角色和工具。 它回答什么？ 更侧重于 “如何做” (How) 以及 “如何组织团队和工作” (How to organize)。 一言以蔽之：生命周期模型提供了“做什么”的宏观框架，而开发方法则填充了“如何做”的具体实践。 一个项目会选择一个生命周期模型（例如，迭代式生命周期），并在这个框架下采用一种或多种开发方法（例如，在迭代中采用Scrum进行管理）。 SDLC 模型为项目提供了基础结构。目前业界有超过50种模型，但以下几种最为经典和重要： 瀑布模型 (Waterfall Model) 核心思想： 严格线性的、阶段化的开发模式。如同瀑布流水，一个阶段必须完成后才能进入下一个阶段。 流程： 需求分析 → 系统设计 → 编码实现 → 测试验证 → 部署上线 → 维护 适用场景： 需求非常明确、在项目周期内几乎不会变更的项目。 V模型 (V-Model) 核心思想： 瀑布模型的演进版，也被称为“验证与确认模型”。它强调了测试活动与开发阶段的并行关系。 特点： 每个开发阶段都有一个与之对应的测试阶段（例如，单元测试对应详细设计，集成测试对应概要设计），从而尽早发现问题。 迭代与增量模型 (Iterative and Incremental Model) 核心思想： 将庞大的开发任务分解成一系列更小、更易于管理的部分。产品在多次迭代 (Iterations) 中被增量 (Increments) 构建。 特点： 每个迭代都会产出一个可用的、但功能不完整的软件版本。这允许团队根据早期版本的反馈进行调整，灵活性高于瀑布模型。 螺旋模型 (Spiral Model) 核心思想： 一种风险驱动的模型，它融合了原型模型和瀑布模型的优点。 特点： 开发过程呈螺旋状，项目会反复经历四个阶段：制定计划、风险分析、工程实现、客户评估。每一次螺旋迭代都会产生一个更完善的版本，非常适合大型、复杂且高风险的项目。 如果说生命周期是骨架，那么开发方法就是血肉。它们让开发过程充满活力和效率。其中敏捷开发 (Agile Development)是当今业界的核心开发思想之一。 敏捷并非一种具体的方法，而是一套旨在提升软件开发灵活性与响应能力的价值观和原则。它强调迭代开发、快速响应变化、与客户的紧密协作，并推崇“个体和互动”高于“流程和工具”。在敏捷这把大伞下，催生了众多高效的开发框架。 Scrum 定义： 敏捷开发中最流行的框架。它通过一系列固定时长的短周期——冲刺 (Sprint)（通常为2-4周）来迭代地交付产品。 关注点： 侧重于如何在开发环境中管理任务，通过定义明确的角色（如产品负责人、Scrum大师）和事件（如每日站会、评审会）来改善团队协作和生产力。 看板 (Kanban) 定义： 一种用于敏捷开发的可视化管理方法，旨在实现持续交付和流程改进。 核心实践： 通过可视化工作流、限制在制品 (WIP) 数量来识别瓶颈，优化流程效率。看板的周期非常灵活，目标是尽快完成单个任务，非常反感多任务并行。 极限编程 (eXtreme Programming, XP) 定义： 一系列卓越的工程实践集合，可以与Scrum等框架结合使用，以提高代码质量和开发效率。 核心实践： 测试驱动开发 (TDD)、结对编程、持续集成 (CI)、简单设计等。 精益软件开发 (Lean Software Development) 定义： 借鉴丰田精益生产思想的开发哲学。 核心原则： 消除浪费、内建质量、延迟决策、快速交付、尊重团队。 此外，DevOps也是一种非常重要的软件开发方法，它旨在统一软件开发 (Dev) 和IT运维 (Ops) 的文化、实践和工具的集合。通过自动化基础设施、工作流和持续监控，缩短开发生命周期，实现高质量、高频率的持续交付 (CI/CD)。 掌握上述方法是走向专业的必经之路。但软件工程的探索永无止境。 形式化方法 (Formal Methods): 除了通过优秀的工程实践来保证质量，我们能否用数学的严谨性来定义和验证软件？形式化方法，如演绎验证 (Deductive Verification) 和 模型检查 (Model Checking)，正是在尝试回答这个问题，以确保软件的最高可靠性。 架构设计： 当我们对各种方法了然于胸后，下一个高地便是架构设计。如何从宏观上设计一个健壮、可扩展、易于维护的系统？ 接下来，我们将深入探讨两种业界核心的架构设计方法——领域驱动设计 (DDD) 与 属性驱动设计 (ADD)。 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:3:1","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"DDD 方法 2003年，软件开发大师 Eric Evans 发布了他的里程碑式著作《领域驱动设计：软件核心复杂性应对之道》（Domain-Driven Design: Tackling Complexity in the Heart of Software）。这本书并非提出了一种全新的技术，而是提出了一套旨在解决软件核心复杂性的设计思想和方法论。 从标题中我们可以直观地理解，DDD 的核心目标是驾驭业务复杂度。在传统的软件开发中，我们常常看到这样的场景： 业务逻辑散落在代码的各个角落，无人能说清完整的业务规则。 开发人员和业务专家之间存在巨大的沟通鸿沟，使用的词汇和理解完全不同。 随着系统迭代，代码变得越来越难以维护，牵一发而动全身，最终形成一个巨大的、无法撼动的“泥球系统”（Big Ball of Mud）。 DDD 正是为了应对这些挑战而生。它不是一个框架，而是一种战略和战术的集合，指导我们如何将软件的核心关注点放在业务本身，通过建立清晰的模型来构建出易于理解、演化和维护的系统。 DDD 的精髓在于“模型驱动设计” (Model-Driven Design)。它强调，一个软件系统的核心价值在于它对业务领域的建模能力。这个“模型”不是指UML图或数据库ER图，而是一个经过精心提炼、能够准确反映业务规则和流程的知识体系。 为了构建和维护这个强大的模型，DDD 提供了两大支柱：战略设计和战术设计。 1. 战略设计：划分边界，聚焦核心 战略设计关注的是宏观层面，它帮助我们理解复杂的业务全景，并将其分解为一系列更小、更内聚、更易于管理的部分。 通用语言 (Ubiquitous Language): 这是DDD的基石。它要求开发团队和业务专家共同创建并使用一套统一的、无歧义的语言来描述业务领域中的所有概念和规则。当代码中的类名、方法名、变量名都与业务专家口中的术语完全一致时，沟通的壁垒就被打破了，模型也变得更加精确。 限界上下文 (Bounded Context): 这是DDD中最重要的战略模式。一个复杂的业务系统通常包含多个子领域，同一个词汇在不同子领域可能有完全不同的含义（例如，“商品”在销售上下文和在仓储上下文中的属性和行为就不同）。限界上下文就是一个明确的边界，它圈定了一个特定模型的应用范围。在这个边界内，通用语言有其唯一的、确定的含义。通过划分限界上下文，我们可以将一个庞大的单体系统分解为多个高度内聚、低耦合的服务或模块，这为微服务架构提供了坚实的理论基础。 上下文映射图 (Context Map): 它描述了不同限界上下文之间的关系，例如它们是如何集成、通信和协作的。这帮助我们从全局视角理解整个系统的架构和依赖关系。 2. 战术设计：精炼模型，构建模块 战术设计则关注于限界上下文内部的模型实现，提供了一系列具体的构建块（Building Blocks）来精细地表达业务逻辑。 实体 (Entity): 具有唯一标识符（ID），并且其状态会随着时间发生变化的对象。例如，“用户”、“订单”等。我们关心的是它“是谁”，而不是它的属性是什么。 值对象 (Value Object): 用于描述事物属性，没有唯一标识符的对象。它的核心在于其包含的属性值，例如“地址”（由省、市、区组成）或“金额”（由数值和币种组成）。值对象通常是不可变的。 聚合 (Aggregate) 与聚合根 (Aggregate Root): 在复杂的业务场景中，一些实体和值对象在逻辑上紧密相关，必须作为一个整体来维护数据的一致性。这个整体就是“聚合”。聚合根是这个整体的管理者，是聚合中唯一允许被外部直接访问的实体。所有对聚合内部的修改都必须通过聚合根来完成，从而保证业务规则的完整性。例如，“订单”聚合可能包含订单头（聚合根）、多个订单行（实体）和收货地址（值对象）。 领域服务 (Domain Service): 当某个业务操作不属于任何一个实体或值对象时，就可以将其逻辑放在领域服务中。它封装了核心的业务规则。 仓储 (Repository): 封装了数据访问的细节，为领域层提供一个面向集合的接口来访问聚合根。它使得领域模型可以不关心数据是如何持久化的（是存入MySQL、MongoDB还是其他地方）。 工厂 (Factory): 负责创建复杂的对象或聚合，将创建过程的复杂性与业务逻辑分离。 知道了DDD的过程，那么自然有了问题：领域驱动设计（DDD）的内核究竟是什么？一言以蔽之，就是以业务为绝对中心，并利用依赖倒置原则（DIP），让所有外部服务都依赖于业务。 这恰好与 Robert C. Martin 在《整洁架构》中反复强调的“稳定抽象原则”不谋而合：一个模块的抽象程度越高，其稳定性就越强。DDD的架构理想，正是将业务领域（Domain）设计成那个最稳定、最抽象的核心。在这个理想模型中，数据库、消息队列、外部API等基础设施被视为易变的细节，它们会因技术升级或需求变更而频繁更换；唯有核心业务逻辑，是那块需要被精心守护、不轻易动摇的基石。这种思想，与传统以数据或技术框架为起点的开发模式，形成了鲜明的对比。 这个疑问切中了要害，也正是理解DDD架构精髓的关键。要解开这个悖论，我们必须区分两种“稳定”： 架构稳定度 (Architectural Stability): 这是 Robert C. Martin 定义的稳定。一个模块被依赖得越多，依赖出去的模块越少，它在架构上就越稳定。因为任何对它的改动，都可能引发连锁反应。 业务稳定性 (Business Stability): 指模块内部的代码变化频率。在产品探索期，业务规则的调整速度往往远超基础设施的更迭速度。 传统架构下，业务逻辑依赖于数据库实现，导致业务的“代码变化”与架构的“不稳定”绑定，改动成本极高。而DDD的魔力在于，它通过依赖倒置这一利器，将这两种稳定度彻底解耦了。 我们通过在领域层定义接口，强制让基础设施层（具体实现）来依赖领域层（抽象定义）。如此一来，无论业务逻辑（代码）如何频繁地变化，它在架构中的稳定地位始终不变——它永远是那个被依赖的核心。这种解耦赋予了我们巨大的灵活性：我们可以在一个稳固的架构上，安全、高效地应对易变的业务需求。 一旦想通了这点，我们便会豁然开朗：既然架构的稳定核心是可以通过设计来“指定”的，那么什么应该成为核心？ 答案不言而喻：应用的战略焦点 (Core Concern)。 我们应当让应用的核心关注点，即核心业务逻辑，成为一个自洽、独立、不依赖任何技术细节的“基础模块”。核心开发团队的使命，就是心无旁骛地实现业务价值本身。而与外部世界打交道的一切“脏活累活”——例如适配特定的数据库、集成第三方支付接口——都应被视为“适配器”，交给边缘团队或在架构外围去完成。 这正是DDD在当今如此流行的根本原因。对于一家操作系统公司，操作系统本身就是其核心价值和战略焦点，所有应用都必须适配它。但对于绝大多数互联网企业和现代软件公司而言，独特且复杂的业务逻辑，才是它们在市场中安身立命的根本。因此，选择一种能够守护并赋能这一核心的架构思想，便成了必然之选。 当然，DDD并非“银弹”，它最适用于那些业务逻辑复杂、需要长期演进的系统。对于简单的CRUD（增删改查）应用，使用DDD可能会显得“杀鸡用牛刀”。 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:3:2","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"ADD 方法 属性驱动设计（Attribute-Driven Design, ADD） 是由卡内基梅隆大学软件工程研究所（SEI）提出的一种架构设计方法。它的核心理念很简单：不要让功能需求主导一切，而要让最重要的质量属性（如性能、安全性、可修改性）成为架构决策的首要驱动力。 ADD方法提供了一套系统性的流程，引导我们从这些关键属性出发，一步步推导出稳固、合理、且面向未来的架构。 “我们的系统需要高性能”，这句话是一个糟糕的需求，因为它无法衡量，也无法验证。ADD方法的第一步，就是使用质量属性场景（Quality Attribute Scenarios），将这些模糊的愿望变得具体、可执行。 一个清晰的场景包含六个要素。让我们以“高性能”为例： 刺激源 (Source): 1000名用户。 刺激 (Stimulus): 同时点击“查询商品”按钮。 制品 (Artifact): 商品查询系统。 环境 (Environment): 在正常负载下。 响应 (Response): 系统处理所有请求并返回结果。 响应度量 (Response Measure): 99%的请求必须在1.5秒内完成。 一旦确定了最重要的质量属性场景，下一步就是选择合适的**架构策略（Tactics）和架构模式（Patterns）**来实现它。 策略（Tactics）是实现某个质量属性的“小招式”。例如，为了实现高可用性，我们可以使用“心跳检测”、“数据冗余”、“故障转移”等策略。 模式（Patterns）则是经过验证的、解决一类问题的“大套路”，它往往是多个策略的组合。例如： 目标：高可修改性 -\u003e 可选模式： 微服务架构、分层架构。 目标：高性能 -\u003e 可选模式： CQRS（命令查询职责分离）、负载均衡。 目标：高安全性 -\u003e 可选模式： 零信任网络。 ADD强调，这些技术和模式的选择不是基于时髦或个人偏好，而是严格服务于我们之前定义好的质量目标。 下面，我将总结一些不同质量属性场景下常用的策略和模式。 高并发 高并发旨在解决大量用户同时访问时，系统不被压垮的问题。关键在于解耦和异步，将瞬间的洪峰流量削平，变为平稳的处理流。 常用策略: 异步处理 (Asynchronous Processing): 通过消息队列（如 Kafka, RabbitMQ）将非核心、耗时的操作异步化，让主流程快速响应。 连接池 (Connection Pooling): 复用数据库连接、HTTP连接等昂贵资源，避免频繁创建和销毁带来的开销。 限流与熔断 (Rate Limiting \u0026 Circuit Breaking): 防止恶意或突发流量冲垮下游服务，在下游服务不可用时快速失败，避免雪崩效应。 常用模式: 事件驱动架构 (Event-Driven Architecture): 系统各部分通过生产和消费事件进行松耦合通信，天然适合处理并发流量。 负载均衡 (Load Balancing): 将请求流量均匀分散到多个服务器上，实现水平扩展。 Actor模型 (Actor Model): 将状态和行为封装在独立的Actor中，通过消息传递进行通信，可以非常高效地处理并发任务。 高性能 高性能的目标是“快”，即降低请求的响应延迟（Latency）和提高单位时间内的处理能力（Throughput）。 常用策略: 缓存 (Caching): 将热点数据存储在访问速度更快的介质上（如内存），是提升性能最立竿见影的手段。包括本地缓存和分布式缓存（如 Redis）。 读写分离 (Read-Write Separation): 将数据库的读操作和写操作分离到不同的实例上，分散压力。 数据索引 (Data Indexing): 在数据库中创建合适的索引，加速数据检索过程。 CDN (内容分发网络): 将静态资源（图片、视频、JS/CSS文件）分发到离用户最近的节点，加速访问。 常用模式: CQRS (命令查询职责分离): 将数据的写入（Command）和读取（Query）操作使用不同的模型和数据存储，进行分别优化。 数据分片 (Sharding): 将海量数据水平切分到多个数据库中，降低单一数据库的负载和数据量。 高可用 高可用的目标是“稳”，确保系统即使在部分组件发生故障时，依然能够对外提供服务，减少停机时间。 常用策略: 冗余 (Redundancy): 对关键组件（服务器、数据库、网关等）部署多个副本，实现“有备无患”。 健康检测 (Health Monitoring): 持续监控系统各组件的运行状态，例如通过心跳检测。 故障转移 (Failover): 当主节点发生故障时，自动或手动将流量切换到备用节点。 常用模式: 故障转移集群 (Failover Cluster): 采用主备（Active-Passive）或双主（Active-Active）模式，实现服务的不间断。 Leader选举模式 (Leader Election Pattern): 在分布式系统中，当主节点失效时，通过选举算法在集群中选出新的主节点来接管服务。 可扩展性 可扩展性（Scalability）指的是系统在应对日益增长的负载时，能够通过增加资源来维持性能水平的能力。它分为垂直扩展（Scale-up）和水平扩展（Scale-out）。 常用策略: 水平扩展 (Horizontal Scaling): 通过增加更多的机器来分担负载，是分布式系统的核心思想。 无状态服务 (Stateless Services): 将服务设计为无状态，使得任何一个实例都可以处理任何请求，便于轻松地添加或移除实例。 数据分片 (Data Sharding): 将数据分散到多个数据库或存储节点上，突破单点存储瓶颈。 常用模式: 负载均衡 (Load Balancing): 自动将进入的流量分配到后端的多个服务器上。 微服务架构 (Microservices Architecture): 将系统拆分为独立部署和扩展的服务，允许对高负载服务进行针对性的资源扩充。 基于队列的负载均衡 (Queue-Based Load Leveling): 使用队列作为缓冲区，平滑请求高峰，让后端服务可以按照自己的节奏处理任务。 可观测性 可观测性旨在让我们能够深入理解系统内部的运行状态，而不仅仅是监控其表面现象。它主要由三驾马车构成：Logging, Metrics, Tracing。 常用策略: 结构化日志 (Structured Logging): 输出机器友好的JSON格式日志，便于后续的收集、查询和分析。 指标收集 (Metrics Collection): 采集关键的性能指标（如CPU、内存、QPS、延迟）并进行聚合和可视化。 分布式追踪 (Distributed Tracing): 在微服务架构中，追踪一个请求从入口到结束所经过的完整链路，快速定位瓶颈和错误。 常用模式: 集中式日志系统 (Centralized Logging): 使用ELK/EFK (Elasticsearch, Logstash/Fluentd, Kibana) 等技术栈，将所有服务的日志汇集到一处进行管理。 服务网格 (Service Mesh): 如 Istio，可以无侵入地为微服务提供丰富的流量管理、安全和可观测性能力。 安全性 安全性旨在保护系统和数据免受威胁，遵循纵深防御原则，层层设防。 常用策略: 身份认证与授权 (Authentication \u0026 Authorization): 确认用户是谁（认证），并判断其有权限做什么（授权）。 数据加密 (Data Encryption): 对传输中（TLS）和静止（在数据库或文件中）的敏感数据进行加密。 输入校验 (Input Validation): 绝不相信任何来自外部的输入，严格校验以防止注入、XSS等攻击。 限制攻击面 (Limit Attack Surface): 关闭不必要的端口，移除不使用的服务，最小化系统暴露给外界的接口。 常用模式: API网关 (API Gateway): 作为所有请求的统一入口，集中处理认证、授权、限流等安全策略。 零信任架构 (Zero Trust Architecture): 默认不信任网络内部和外部的任何人/设备/系统，每次访问都需要经过严格的身份验证。 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:3:3","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"实践 “纸上得来终觉浅，绝知此事要躬行”。再多的理论知识，也无法替代一次完整的实践所带来的深刻体悟。 为了将理论付诸实践，我计划以螺旋模型为软件开发生命周期（SDLC）的基础，通过两个开发周期，从零开始构建一个功能完善的论坛系统。 周期一：原型构建。此阶段的核心目标是快速实现核心业务需求，构建一个可交互的基础原型。 周期二：架构演进。在原型之上，我们将重点关注性能、可扩展性、可维护性等质量属性，对系统架构进行重构与优化。 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:4:0","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"需求分析 作为一个论坛项目，其核心需求相对明确，我们可以将其归纳为以下几个模块： 用户域: 用户注册、登录与会话管理 个人资料查看与编辑 内容域: 帖子的发布、编辑与删除 对帖子进行评论与回复 对帖子和评论的点赞与收藏 社交与通知域: 当用户的帖子被回复或点赞时，发送系统通知 用户可以举报违规内容 AI 辅助域: 引入 AI 用户进行内容互动 ","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:4:1","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"DDD 驱动设计与实现 在第一个开发周期，我们的目标是清晰地梳理业务逻辑并构建原型，因此，领域驱动设计（Domain-Driven Design, DDD） 成为了我们的首选方法。 基本设计 DDD 的第一步是建立团队（在这里即是我自己）内部统一的、无歧义的“通用语言”。这能确保代码的命名和设计直接反映业务概念。 术语 (Term) 英文标识 (Identifier) 核心职责 (Core Responsibility) 用户 User 系统的核心参与者，可以注册、登录、发帖、回复等。 作者 Author 创建主题或回复的用户，是 User 在特定场景下的角色。 主题/帖子 Thread 用户发起的一个讨论话题，是论坛内容的核心载体。 回复 Post 对一个 Thread 的具体回应或评论。 版块 Forum 用于组织和归类 Thread 的特定区域。 点赞 Upvote 用户对 Thread 或 Post 表示赞同的行为。 收藏 Favorite 用户将感兴趣的 Thread 加入个人收藏列表的行为。 通知 Notification 当发生与用户相关的事件时，系统发送的提醒。 下一步是划分限界上下文 (Bounded Contexts)。 在 DDD 出现之前，许多项目试图为整个系统建立一个宏大而统一的“万能模型”。这种天真的想法往往会迅速导致灾难，其根源在于**“语义陷阱”（Semantic Ambiguity）**——同一个词，在不同业务场景下，其含义、属性和行为可能天差地别。例如，“用户”在身份认证上下文中关心的是密码和角色，而在社区上下文中则更关心声望和发帖数。 DDD 通过限界上下文（Bounded Context） 来解决这个问题。每个上下文都是一个独立的业务领域，内部共享一套统一的模型和语言。 对于我们的论坛项目，可以划分为以下几个内聚的限界上下文： 身份认证上下文 (Identity \u0026 Access Context): 负责用户注册、登录、密码管理、角色权限等。这里的 User 模型专注于安全和身份验证。 论坛核心上下文 (Forum Context): 负责帖子、评论、点赞、收藏等核心业务。这里的 User (通常称为 Author) 更像一个内容的创作者。 通知上下文 (Notification Context): 负责生成和推送通知。它只关心接收通知的 User ID 和通知内容。 搜索上下文 (Search Context): 专用于内容索引和搜索。它关心的是可被索引的文本和元数据。 AI 上下文 (AI Context): 负责处理由 AI 生成内容、审核或提供智能建议等相关任务。 这种划分在大型团队中尤为关键，它能有效降低沟通成本，实现团队自治和并行开发。对于单人开发，这更是一次宝贵的架构思维训练，能让系统在未来更容易扩展和维护。 此外在代码实现上，我们将遵循分层架构（常与 DDD 结合使用），为每个限界上下文创建一个独立的 package。每个 package 内部结构如下： application: 应用层。负责编排领域对象以完成业务用例（Use Case），例如“用户发布新帖子”。它很薄，不包含业务规则，是领域层和外部世界的“协调者”。 domain: 领域层。项目的核心，包含所有业务逻辑、实体（Entities）、值对象（Value Objects）和领域服务。它不依赖任何其他层，是纯粹的业务模型。 infrastructure: 基础设施层。负责所有技术细节的实现，如数据库访问、消息队列、第三方API调用等。通过依赖倒置原则（DIP），该层依赖于应用层和领域层定义的接口，从而实现业务逻辑与技术实现的解耦。 web: 负责暴露 RESTful API 接口。 persistence: 负责将领域对象持久化到数据库。 其他的技术选型： 数据库: 单体关系型数据库。在原型阶段，这能提供最快的开发速度和事务一致性保证。 通信方式: 同步通信。例如，发布帖子后直接调用通知服务，这在系统初期逻辑简单，易于实现和调试。 API 风格: RESTful API。作为行业标准，它拥有成熟的生态和清晰的规范。 关键流程的实现 身份认证与授权 系统的入口是身份认证与授权。我们首先要解决的问题是，如何高效且安全地识别用户身份。 传统的方案是 Cookie-Session 机制。其原理是：用户登录后，服务端在内存或数据库中创建一个 Session 记录，并将一个唯一的 Session ID 通过 Cookie 返回给客户端。后续客户端的每次请求都会携带此 Cookie，服务端据此查找对应的 Session 信息来识别用户。这种方式的弊端在于其有状态性：在分布式系统中，所有服务器节点都必须能访问到同一个 Session 存储，这增加了架构的复杂度和状态同步的开销。 为了解决这个问题，我们采用了 JWT (JSON Web Token)。JWT 的核心原理是无状态认证。它将用户信息、权限、过期时间等数据通过密码学手段（HMAC 或 RSA 签名）打包成一个安全的字符串（Token），并直接返还给客户端。客户端在后续请求的 Header 中携带此 Token。服务端收到后，无需查询任何存储，只需用预置的密钥或公钥验证 Token 的签名是否有效，即可信任其中的用户信息。这使得服务端完全无状态，极大地方便了应用的水平扩展。在项目中，我们通过 Spring Security 框架来集成和管理 JWT 的生成与校验，以最小的配置代价构建起强大的安全体系。 分层架构与持久化 论坛的核心功能，如帖子和评论的管理，采用分层架构与领域驱动设计 (DDD) 思想构建，确保业务逻辑的内聚和层级间的解耦。请求的处理流程严格遵循 Web 层 -\u003e 应用层 -\u003e 领域层 -\u003e 基础设施层的顺序。其中，基础设施层的持久化方案选择尤为关键。 我们选择了 MyBatis，而不是像 Hibernate/JPA 这样的全自动 ORM 框架。 ORM (对象关系映射) 的原理是将数据库表映射为程序中的对象，开发者可以直接操作对象来完成数据库交互，框架会自动生成并执行 SQL。这种方式在简单的 CRUD 场景下非常高效，但对于复杂的关联查询或需要精细优化 SQL 性能的场景，自动生成的 SQL 往往难以控制，灵活性不足。 MyBatis 的原理则是一种半自动的 SQL 映射。它不试图隐藏 SQL，而是让开发者将 SQL 语句写在 XML 文件或注解中，然后将其映射到 Mapper 接口的方法上。MyBatis 负责处理 JDBC 的繁琐细节，如创建连接、参数设置、结果集映射等，但把 SQL 的控制权完全交还给开发者。这在灵活性和性能调优上具有巨大优势。 为了进一步提升开发效率，我们使用了 MyBatis-Plus。它的原理是在 MyBatis 的基础上进行功能增强。它通过分析实体类（Entity）的注解（如 @TableName, @TableId），在程序启动时自动生成一套完整的、针对单表的 CRUD SQL 语句。开发者只需让自己的 Mapper 接口继承 MyBatis-Plus 提供的 BaseMapper 接口，就能免费获得这些通用的增删改查方法，无需手写任何简单 SQL。当遇到复杂业务逻辑时，开发者依然可以像使用原生 MyBatis 一样，在 XML 中编写自定义的 SQL。MyBatis-Plus 完美结合了 ORM 的便利性和 MyBatis 的灵活性。 搜索功能 为实现高效的全文搜索，我们引入了 Elasticsearch。传统关系型数据库使用 LIKE '%keyword%' 进行模糊查询，在数据量大时会进行全表扫描，效率极低且无法实现复杂的相关性排序。 Elasticsearch 的核心原理是倒排索引 (Inverted Index)。传统的正向索引是“文档 -\u003e 词语”，而倒排索引则反过来，建立“词语 -\u003e 文档列表”的映射。它首先会对文档内容进行分词，得到一系列词元（Term），然后为每个词元创建一个列表，记录下所有包含该词元的文档 ID。当用户进行搜索时，Elasticsearch 只需根据搜索词快速找到对应的文档 ID 列表，然后进行合并、排序等操作，即可返回结果。这个过程避免了全量扫描，因此速度极快。 在我们的架构中，核心挑战在于数据同步，即保证主数据库 MySQL 与 Elasticsearch 索引之间的数据一致性。我们初期的实现方式是在业务代码中同步调用，即当一个帖子在 MySQL 中创建成功后，程序立刻调用接口将其写入 Elasticsearch。这种方式实现简单，但缺点是强耦合：一旦 Elasticsearch 服务出现故障，会导致发帖等核心功能失败。同时，需要理解 ES 的更新和删除机制：它们并非立即执行物理删除，而是将文档标记为“已删除”（软删除），真正的空间回收发生在后台的段合并 (Segment Merging) 过程中。这是 ES 为保证写入性能所做的设计。后续，我们将通过引入消息队列等异步机制来解耦，以提高系统的健壮性。 AI 用户 为了让论坛更具智能，我们设计了一个 AI 用户，其核心技术是 RAG (Retrieval-Augmented Generation)，即“检索增强生成”。其原理是让大语言模型（LLM）在生成回答前，先从一个知识库中检索相关信息作为参考，以避免模型产生幻觉或回答空洞无物。 在我们的项目中，这个流程被具体实现为：当有用户发布新帖时，一个后台 AI Agent 会被触发。首先，它执行检索 (Ret","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:4:2","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["Backend"],"content":"ADD 驱动设计与实现 高性能设计 系统性能的优劣直接影响用户体验，其核心指标是低响应延迟与高吞吐量。为达此目标，我们可以从代码和架构两个层面着手。代码层面，通过并行化处理、特化优化、对象池化以及数据库索引优化等手段提升执行效率。而在架构层面，引入缓存与异步机制是关键所在。 1. 缓存策略：Redis深度应用 缓存是提升读性能最有效的手段之一。将“读多写少”的数据置于Redis中，可以极大减轻数据库压力。 深度解析：Redis有序集合（ZSET）与实时排行榜 为了实现用户活跃度实时排行这类动态功能，Redis的有序集合（ZSET）是绝佳工具。 核心原理：ZSET之所以高效，得益于其精妙的内部数据结构：它同时使用了哈希表（Hash Map）和跳表（Skip List）。 哈希表将成员（member）映射到其分数（score），这使得我们可以在 O(1) 的时间复杂度内直接获取任意成员的分数。 跳表则将所有成员按照分数进行排序，它是一种通过多层链表实现快速查找的概率性数据结构，其增加、删除、查找操作的平均时间复杂度均为 O(logN)。 这种双重结构，使得ZSET既能快速查询单个成员，又能高效地进行范围查询和排序。 使用示例（以论坛用户活跃度为例）： 假设我们有一个名为 user:activity:leaderboard 的排行榜。 增加/更新用户活跃度：当用户 “user:123” 完成一次加分行为（如发帖），我们增加其5分。 Bash # ZADD key score member # 如果 \"user:123\" 已存在，则更新其分数；如果不存在，则创建。 ZADD user:activity:leaderboard 50 \"user:123\" # 使用 ZINCRBY 更方便地增加分数 # ZINCRBY key increment member ZINCRBY user:activity:leaderboard 5 \"user:123\" 获取Top 10活跃用户：我们需要按分数从高到低查询。 Bash # ZREVRANGE key start stop [WITHSCORES] # 从高到低返回排名 0 到 9 的用户（即前10名），并带上他们的分数。 ZREVRANGE user:activity:leaderboard 0 9 WITHSCORES 返回结果会是 [\"user:888\", \"150\", \"user:123\", \"55\", ...] 查询某用户的排名： Bash # ZREVRANK key member # 查询 \"user:123\" 在排行榜中的名次（从0开始） ZREVRANK user:activity:leaderboard \"user:123\" 通过这几个简单命令，我们就能以极高性能构建一个功能完备的实时排行榜系统。 缓存更新模式 Cache-Aside（旁路缓存）：最经典模式。应用逻辑为：读 -\u003e 查缓存 -\u003e 未命中 -\u003e 查数据库 -\u003e 回写缓存。写 -\u003e 先更新数据库，再删除缓存。这是业界更推崇的方案，能有效降低数据不一致的风险。 Read/Write-Through（读穿/写穿）：将缓存作为抽象层，应用只与缓存交互。此模式简化了应用逻辑，但对缓存服务的实现要求更高。 2. 异步解耦：消息队列的选型与应用 消息队列是实现系统解耦、提升并发处理能力和削峰填谷的利器。 主流消息队列对比 特性 RabbitMQ Kafka RocketMQ 模型 遵循AMQP协议，Broker/Exchange/Queue模型，路由灵活 发布-订阅模型，以分区日志（Log）为核心，持久化存储 主题/队列模型，借鉴了Kafka和RabbitMQ的设计 吞吐量 中等，万级到十万级/秒 极高，百万级/秒，为大数据而生 非常高，十万级到百万级/秒 延迟 较低，微秒级，适合实时性要求高的场景 相对较高，毫秒级，受批量处理影响 较低，毫秒级 可靠性与功能 功能丰富，支持事务、确认机制、优先级队列，路由策略强大 可靠性极高，通过副本保证数据不丢失，但不支持标准事务 功能全面，支持事务消息、延迟消息、顺序消息等，可靠性高 适用场景 业务逻辑复杂、实时性要求高的中小规模系统，如任务调度、金融交易 大数据日志采集、流式处理、数据总线，需要高吞吐和数据回溯的场景 业务集成、分布式事务、金融级应用，尤其在Java生态中表现优异 场景化选型：我们的论坛系统 用户评论、点赞、系统通知：这类操作要求低延迟和高可靠性，且业务逻辑可能比较复杂（如通知需要推送给不同类型的用户）。RabbitMQ 的灵活路由和低延迟特性非常适合。或者 RocketMQ 的普通消息和延迟消息功能也能很好地满足需求。 数据库与Elasticsearch同步：这个场景的核心是高吞吐和数据可回溯性。我们希望所有数据库的变更都能被顺序地、可靠地传输给多个下游系统（不仅是ES，未来可能还有数据分析平台）。Kafka 在此场景下是最佳选择。其基于日志的存储模型，允许不同的消费组（ES同步服务、数据仓库服务等）从各自的位置独立消费数据，互不干扰，且数据可被持久化保存以供回放。 在Java中，我们可以通过spring-boot-starter-amqp快速集成RabbitMQ，或通过spring-kafka集成Kafka。 高可用与高并发 1. 高可用（HA）架构落地 高可用旨在消除单点故障。我们需要在系统的每一层都建立冗余和自动故障转移机制。 网关层：使用 Nginx 作为反向代理和负载均衡器。通过部署至少两个Nginx实例，并结合 Keepalived 使用虚拟IP（VIP）漂移技术，实现网关层的高可用。当主Nginx宕机，Keepalived会自动将VIP指向备用Nginx。 应用层：将每个微服务部署多个实例，分布在不同的物理服务器上。Nginx或专门的服务网关（如Spring Cloud Gateway）会将流量负载均衡到这些健康的实例上。 缓存层： Redis Sentinel（哨兵模式）：适用于主从复制架构。哨兵节点负责监控Redis主从节点的健康状况，当主节点故障时，会自动从从节点中选举一个新的主节点，并通知应用方更新连接地址。 Redis Cluster（集群模式）：提供了原生的分布式方案，数据自动分片到多个节点，每个节点都有主从备份。它同时解决了容量和高可用的问题。 数据库层：通常采用**主从复制（Primary-Replica）**模式。写入操作在主库，读取操作可在从库（读写分离）。当主库故障时，可以通过手动或自动机制（如MHA、Orchestrator）将一个从库提升为新的主库，实现故障转移。 2. 高并发（HC）应对策略 高并发的核心是“削峰填谷”和防止系统被流量压垮。 流量控制与服务保护：在Java微服务体系中，Alibaba Sentinel 或 Resilience4j 是实现流量控制、熔断降级的首选库。 限流（Rate Limiting）：保护系统免受瞬时高流量冲击。例如，我们可以限制“发送验证码”接口每秒只能被调用100次。 Java // 使用 Sentinel 注解的示例 @SentinelResource(value = \"sendVerificationCode\", blockHandler = \"handleBlock\") public Result\u003cString\u003e sendCode(String phone) { // ... 业务逻辑 } // 当流量超出阈值时，会调用此方法 public Result\u003cString\u003e handleBlock(String phone, BlockException ex) { return Result.fail(\"请求过于频繁，请稍后再试\"); } 熔断降级（Circuit Breaking）：当下游服务不稳定（如大量超时或错误）时，暂时切断对它的调用，直接返回一个降级响应，避免雪崩效应。熔断器具有“关闭”、“打开”和“半开”三种状态，能实现自动恢复。 Java // 使用 Resilience4j 注解的示例 @CircuitBreaker(name = \"downstreamService\", fallbackMethod = \"fallbackForService\") public UserProfile getUserProfile(long userId) { // ... 调用下游用户服务 } // 当熔断器打开时，会调用此降级方法 public UserProfile fallbackForService(long userId, Throwable t) { return new UserProfile(\"默认用户\", \"default_avatar.png\"); } 异步处理：除了消息队列，在应用内部，可以使用 @Async 注解或 CompletableFuture 将耗时操作（如发送邮件、记录日志）异步化，让主线程快速返回。 连接池优化：数据库连接是宝贵资源。确保使用高性能的连接池，如 HikariCP（Spring Boot 2.x默认），并合理配置其大小，以匹配并发需求。 可扩展性 微服务架构与数据分片是实现水平扩展的关键。通过领域驱动设计（DDD）指导服务拆分，并采用服务注册与发现（如Nacos, Consul）机制管理服务实例。当数据量巨大时，采用Sharding-JDBC或MyCAT等中间件实现数据库的透明分库分表。 可观测性 一个复杂的分布式系统必须具备良好的可观测性。ELK S","date":"2025-07-06","objectID":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/:4:3","tags":["Code","Architecture"],"title":"后台开发全景图：思考、方法与实践","uri":"/posts/%E5%90%8E%E5%8F%B0%E5%BC%80%E5%8F%91%E5%85%A8%E6%99%AF%E5%9B%BE%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/"},{"categories":["AI"],"content":" Gott ist tot… Wir müssen selbst zu Göttern werden. ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:0:0","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"Intro 我们在预训练后，就得到了一个擅长续写的 AI。但是光是续写并不是十分有用，我们需要费尽思 prompt 才能得到想要的结果。为了更好的应对下游任务，如 QA Chatbot，总结，推理等，需要做一些适配。 下游任务往往需要模型有特定的回答模式，主题或者是新的知识。我们可以通过许多技术来适配。 SFT (Supervised Fine-Tuning) ：通过在数千条“提示-回答”对上继续训练，以塑造模型风格、提升指令遵循能力和解决安全性的经典方法，尤其适合风格控制。比较难注入新知识。目前也有 LoRA 等技术实现低成本微调。 RLHF (Reinforcement Learning from Human Feedback)：因为人类偏好的复杂性，我们很难有完善的预训练数据。但在很多情况下，即使我们不知道怎么回答，也很容易判断什么是好的回答。因此就有了 RLHF，通过训练一个奖励模型来学习人类对回答的偏好，再利用强化学习（如PPO）优化语言模型，使其生成更符合人类期望、更流畅且安全的输出。 Prompt：虽然简单有效，但其效果高度依赖设计技巧且对复杂任务有天花板。 RAG：通过从外部数据库查找资料，插入 prompt 作为上下文，有效解决了模型知识陈旧和事实性不足的问题，降低幻觉、增强专业性的经济高效手段，其核心在于高质量知识库与精准检索。 在这篇博客中，我们会在有一张 4090D 的服务器上，以 Qwen2.5-7B 为基础模型，对其进行微调，并在这个过程中学习相关知识。 ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:1:0","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"Evaluation 在进行任何微调之前，建立一个健全的评估框架至关重要。如果没有评估标准，我们就无法衡量微调的效果，也无法判断模型是否在朝着期望的方向改进。在一般的深度学习任务中，交叉熵通常作为主要的衡量标准，但对于语言模型的某些能力（如结构化输出），单纯使用交叉熵并不合适。评估时，我们通常通过特定的 prompt 让模型生成规定格式的答案，然后通过文本解析或其他模型来评价这些答案的质量。 在进行评估时，我们需要选择合适的推理服务来运行模型。目前主流的选择包括： vllm：最高性能的推理服务 ollama：专注于一键式部署，可自动在 CPU 和 GPU 间平衡计算 Hugging Face Transformers：提供标准化的训练和微调接口，但不专注于推理性能 考虑到我们的 4090D 只有 24GB 显存，在 4096 序列长度下仅能勉强容纳 7B 参数模型及其 KV cache，而对于更大的模型（如 32B）则显存不足。因此，我们选择使用 vllm 作为基础模型的推理服务。 评估语言模型通常涉及以下几个方面： 首先是通用能力评估，我们使用 MMLU (Massive Multitask Language Understanding) 数据集来测试模型的事实性知识掌握程度。该数据集包含多个领域的选择题，每题提供 A、B、C、D 四个选项。我们使用如下统一格式的 prompt 来获取模型回答： Answer the following multiple choice question about {subject}. Respond with a single sentence of the form \"The correct answer is _\", filling the blank with the letter corresponding to the correct answer (i.e., A, B, C or D). Question: {question} A. {options[0]} B. {options[1]} C. {options[2]} D. {options[3]} Answer: 通过解析模型输出，我们可以计算正确率。在测试中，我们的 Qwen2.5-7B 在 MMLU 上获得了 68.64% 的准确率。 其次是推理能力评估，我们采用 GSM8K 数据集。这个数据集包含一系列需要多步推理的数学问题，要求模型给出具体的数字答案。使用类似的评估方法，我们的 Qwen2.5-7B 在 GSM8K 上的得分为 22.52%。 此外，还有专门评估对话质量的 AlpacaEval 数据集，它使用一个强大的评估模型来对比被测模型与参考模型（通常是 GPT-4 Turbo）的表现。评估结果用被测模型相对于参考模型的胜率来表示。 最后，SimpleSafetyTests 用于评估模型输出的安全性，但同样需要一个较强的评估模型来判断输出是否安全。 受限于我们 4090D 的计算能力，以及成本考虑，我们只能进行 MMLU 和 GSM8K 这两项基础测试来评估模型的通用能力和推理能力。 ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:2:0","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"Supervised Fine Tune 如前所述，SFT 是通过在“提示-回答”对的数据集上继续训练预训练模型来适配下游任务的常用方法。这个过程相对直接，目标是让模型学习到在给定特定类型的提示时，生成特定格式或内容的回答。能够显著提升模型理解和执行指令的能力。不过它很难向模型注入新的事实性知识，或者纠正预训练阶段学到的错误知识，而且模型可能学会了模仿期望的输出格式，但并没有真正理解任务的本质。最后，数据非常重要，garbage in，garbage out。 SFT 的效果在很大程度上取决于数据的质量和数量。数据来源可以多种多样，包括人工标注的数据、开源指令数据集（如 Alpaca, Dolly, OpenOrca 等）、用户反馈数据等。数据通常需要处理成统一的“提示-回答”格式。例如： { \"instruction\": \"解释一下什么是黑洞。\", \"output\": \"黑洞是时空展现出极端强大的引力，以至于任何形式的物质和辐射都无法从中逃逸的区域。\" } 或者对于对话格式： { \"messages\": [ {\"role\": \"user\", \"content\": \"你好，你是谁？\"}, {\"role\": \"assistant\", \"content\": \"我是一个大型语言模型，由 Llama 团队训练。\"} ] } 最经典的方法是全参数微调，即更新模型的所有参数。虽然效果通常最好，但计算资源消耗大，训练时间长。作为替代，我们可以进行参数高效微调 (Parameter-Efficient Fine-Tuning - PEFT)，只更新模型的一小部分参数，或者引入少量额外的可训练参数。 常见的 PEFT 方法包括： LoRA (Low-Rank Adaptation)：通过在模型的某些层（通常是注意力层）注入低秩适配器矩阵来进行微调。它能以极少的额外参数（通常不到原模型参数的1%）达到接近全参数微调的效果，大大降低了微调的成本和存储需求。 QLoRA：是 LoRA 的进一步优化，结合了量化技术，使得在更小的硬件上微调更大的模型成为可能 其他方法：如 Adapter Tuning、Prefix Tuning、Prompt Tuning 等 在我们的实验中，先尝试使用经典的全参数微调。使用 transformers 框架，微调的代码非常简单： def main(): config = parse_args() if config.use_wandb: wandb.init(project=\"qwen-finetuning\") # Initialize model and tokenizer tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained(config.model_name, trust_remote_code=True) # Create dataset train_dataset = CustomDataset(config.train_file, tokenizer, config.max_length) # Training arguments training_args = TrainingArguments( output_dir=config.output_dir, num_train_epochs=config.num_epochs, per_device_train_batch_size=config.batch_size, gradient_accumulation_steps=config.gradient_accumulation_steps, learning_rate=config.learning_rate, weight_decay=config.weight_decay, warmup_steps=config.warmup_steps, logging_steps=config.logging_steps, save_steps=config.save_steps, eval_steps=config.eval_steps, logging_dir=\"./logs\", fp16=True, report_to=\"wandb\" if config.use_wandb else None, ) # Initialize trainer trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, ) # Start training trainer.train() 但是很快，显存不够，训练被迫中止了… 😢 那么让我们计算一下微调到底需要多少显存。结合 Building a Transformer LM 与 Optimize the Performance of a LM 中的估算和测量，我们知道 20M 参数在 FP32 精度下训练时，包括优化器状态和激活值在内，峰值显存约为 2GB。由于显存占用和参数量成正比，7B 参数的模型理论上需要约 700GB 显存！ 不过，我们可以通过一些优化方法来降低显存需求： 混合精度训练：可以将显存需求减半至 400GB Gradient Checkpointing：通过重计算中间激活值，可以节省约 70% 的激活值显存（以 30% 的计算时间开销为代价） Gradient Accumulation：通过减少每次计算的 batch size 来降低显存使用 即便采用这些优化，估计也需要至少 80GB 显存，或者使用 DeepSpeed 等分布式训练框架将参数、激活值和优化器状态分散到多个 GPU 上。但我们只有一块 24GB 显存的 4090D，似乎只能尝试 1.5B 规模的小模型了。 不过，我们还有最后一招 —— PEFT。使用 QLoRA 技术，我们不直接调整原始参数，而是在特定层旁增加并调节新的参数（约原参数量的1%），再配合 4bit 量化等技术，24GB 显存就足够了： def main(): config = parse_args() if config.use_wandb: wandb.init(project=\"qwen-finetuning\") # Configure quantization bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16 ) # Initialize model with quantization model = AutoModelForCausalLM.from_pretrained( config.model_name, quantization_config=bnb_config, trust_remote_code=True, device_map=\"auto\" ) # Prepare model for k-bit training model = prepare_model_for_kbit_training(model) # LoRA configuration lora_config = LoraConfig( r=8, lora_alpha=32, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\" ) # Apply LoRA model = get_peft_model(model, lora_config) # Initialize tokenizer tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True) ... trainer.train() 此外，我们还可以使用 wandb（Weights \u0026 Biases）来监控训练过程。 但即便使用了 QLoRA，训练速度依然非常慢！ 让我们估算一下训练时间：每条数据平均 512 个 token，共 39438 条微调数据，3 个 epoch，总 token 数约为 39438 * 256 * 3 ≈ 30M tokens。即使按照 vllm 推理的速度（约 1000 tokens/s）来算，也需要大约 8 小时才能完成训练。 所以…也许知道理论上可行就好了吧～ ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:3:0","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"RLHF 在 SFT 中，我们训练模型模仿一组高质量样本中的回答。然而，这往往不足以消除预训练过程中模型学到的不良行为。与 SFT 依赖外部优质样本不同，为了更好地对齐语言模型，我们通常需要从待改进的模型本身获取回答，并根据这些回答的质量和适当性来进行奖励或惩罚。 RLHF（基于人类反馈的强化学习）是一种通过人类反馈来优化语言模型的方法。它的核心思想是：即使我们难以直接给出完美的回答示例，但我们通常能够判断哪个回答更好。 最初的 RLHF 实现采用 PPO 算法，但目前更多地使用 DPO 算法。接下来我们将简要介绍强化学习的基础知识，并在最后深入理解 DPO 算法的原理，然后进行实践操作。 参考资料： https://hrl.boyuai.com/chapter/intro https://spinningup.openai.com/en/latest/user/introduction.html ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:0","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"RL Basic 广泛地讲，强化学习是机器通过与环境交互来实现目标的一种计算方法。机器（Agent）可以与环境进行多轮交互，每一轮交互机器都可以采取一个动作，而环境会给出反馈。机器的目标是最大化在多轮交互过程中的获得的累积奖励的期望。 强化学习的基础理论建立在马尔可夫决策过程（Markov decision process，MDP）之上。MDP的基础是马尔可夫过程（Markov process）和马尔可夫奖励过程（Markov reward process，MRP）。agent 通过策略（Policy）决定如何和环境互动，以达到最大累积期望。而价值函数（Value Function）常常用来辅助求解策略。 随机过程和马尔可夫过程 马尔可夫过程（Markov process）指具有马尔可夫性质的随机过程，也被称为马尔可夫链（Markov chain）。随机过程（stochastic process）是概率论的“动力学”部分。 给定一个概率空间 $(\\Omega, F,P)$ （$\\Omega$是样本的集合；$F$是$\\Omega$幂集的满足某些条件的非空子集，表示事件集合；$P$ 是概率，是从$F$到$R$的函数，满足全概率为1且可数可加），一个随机变量 $X: \\Omega \\to R$ 是定义在样本空间上的实函数，满足 $\\forall t \\in R . \\{w \\in \\Omega:X(w) \\le t\\} \\in F $。（我们可以通过概率密度函数，分布函数，各阶矩对其进行研究） 如果 $T$ 是一个集合，有单射 $S: T \\to P(\\Omega \\times R) $，$\\forall t \\in T$，$S(t)$或者$S_t$ 均为定义在 $\\Omega$ 上的随机变量，则 $S$ 为 $(\\Omega, F,P)$ 上的随机过程。（也就是，随机过程就是一串随机变量，$t$ 可以理解为时间）随机过程研究的是随机变量间的相互关联，因为关联的复杂性，我们只研究某些种类的关联，比如有马尔可夫性质的随机过程。 当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质（Markov property），也就是 $P(S_{t + 1} = j|S_t = i_t) = P(S_{t+1} = j |S_1 = i_1,…,S_t = i_t)$。（离散时间下，其中 $i_0..i_{t},j$为任意值/状态），这个随机过程也被称为马尔可夫链（Markov chain）。如果马尔可夫链的一步转移概率只和状态相关，就被称为为时齐马尔可夫链(homogeneous)，强化学习中一般只考虑时齐马尔可夫链。 （时齐）马尔可夫链可用元组 $(S, P)$ 描述，其中： $S$ 是有限状态集合 $P$ 是状态转移矩阵（state transition matrix） 假设共有 $n$ 个状态，则状态集合定义为 $S = \\{ s_1, s_2, \\cdots, s_n \\}$ 。状态转移矩阵 $P$ 中第 $i$ 行第 $j$ 列元素表示状态转移概率：$ P(s_j \\mid s_i) = P(S_{t+1} = s_j \\mid S_t = s_i) $ 即从状态 $s_i$ 转移到状态 $s_j$ 的概率，简记为 $P(s’ \\mid s)$。从任意状态 $s_i$ 出发，转移到所有可能状态的概率和为 1。 给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列（episode），这个步骤也被叫做采样（sampling）。 马尔可夫奖励过程和累积回报 马尔可夫奖励过程（MRP）是马尔可夫链的扩展，它在状态转移的基础上引入了奖励机制和折扣因子，用于量化状态的价值。MRP可定义为元组 $(S, P, R, \\gamma)$，其中： $S$ 是有限状态集合 $P$ 是状态转移矩阵，满足 $P(s’ \\mid s) = \\mathbb{P}(S_{t+1}=s’ \\mid S_t=s)$ $R(s)$ 是奖励函数，表示从状态 $s$ 转移到下一状态的即时奖励期望，即 $R(s) = \\mathbb{E}[r_{t+1} \\mid S_t = s]$ ，大部分情况中只由状态 $s$ 决定，可以去掉期望 $\\gamma \\in [0, 1]$ 是折扣因子，用于权衡当前奖励与未来奖励的重要性。 在强化学习中，智能体的目标是最大化从当前时刻开始的累积回报（Return），定义为未来所有奖励的加权和： $$ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1}. $$ 折扣因子 $\\gamma$ 的作用包括： 数学便利性：避免无限时间步下的回报发散； 远期不确定性：远期奖励的“现值”应低于即时奖励； 实际意义：模仿人类对即时收益的偏好。 价值函数 价值函数用于衡量某状态的长期价值，定义为从该状态出发的（遍历所有可能序列）期望累积回报： $$ v(s) = \\mathbb{E}[G_t \\mid S_t = s]. $$ 对MRP而言，价值函数满足贝尔曼方程（Bellman Equation）： $$ v(s) = R(s) + \\gamma \\sum_{s’ \\in S} P(s’ \\mid s) v(s’), $$ 其含义是：当前状态的价值 = 即时奖励 + 折扣后的未来状态价值期望。 该方程可写成矩阵形式 $\\mathbf{v} = \\mathbf{R} + \\gamma \\mathbf{P} \\mathbf{v}$，其解析解为 $\\mathbf{v} = (I - \\gamma \\mathbf{P})^{-1} \\mathbf{R}$。（也就是MRP价值函数有解析解） 马尔可夫决策过程 MDP在MRP的基础上引入动作（Action），用于建模智能体的决策行为。MDP定义为元组 $(S, A, P, R, \\gamma)$，其中： $A$ 是有限动作集合； 状态转移概率扩展为 $P(s’ \\mid s, a) = \\mathbb{P}(S_{t+1}=s’ \\mid S_t=s, A_t=a)$； 奖励函数扩展为 $R(s, a) = \\mathbb{E}[r_{t+1} \\mid S_t=s, A_t=a]$，大部分情况中只由状态 $s$ 和动作 $a$ 决定，可以去掉期望。 $\\gamma$ 仍为折扣因子。 在MDP中，智能体通过策略（Policy）选择动作。策略可以是确定性的，也可以是随机性的： 确定性策略 $\\pi: S \\to A$ 将状态映射到具体动作 随机性策略 $\\pi(a \\mid s)$ 表示在状态 $s$ 下选择动作 $a$ 的概率分布（值） 大部分情况下我们使用随机性策略，因为它更加通用且具有更好的探索性质。可以证明，某些标准条件的情况下，马尔可夫决策过程（MDP）的最优策略一定存在。（也就是得到最大累积回报的策略） 状态价值函数与动作价值函数 状态价值函数 $v^\\pi(s)$： 表示在状态 $s$ 下遵循策略 $\\pi$ 的期望累积回报： $$ v^\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s]. $$ 动作价值函数 $q^\\pi(s, a)$： 表示在状态 $s$ 执行动作 $a$ 后继续遵循策略 $\\pi$ 的期望累积回报： $$ q^\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a]. $$ 两者关系为： $$ v^\\pi(s) = \\sum_{a \\in A} \\pi(a \\mid s) q^\\pi(s, a). $$ 贝尔曼期望方程 在给定策略 $\\pi$ 下，价值函数满足以下递归关系： 状态价值函数的贝尔曼方程： $$ v^\\pi(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\left[ R(s, a) + \\gamma \\sum_{s’ \\in S} P(s’ \\mid s, a) v^\\pi(s’) \\right]. $$ 动作价值函数的贝尔曼方程： $$ q^\\pi(s, a) = R(s, a) + \\gamma \\sum_{s’ \\in S} P(s’ \\mid s, a) \\sum_{a’ \\in A} \\pi(a’ \\mid s’) q^\\pi(s’, a’). $$ 贝尔曼最优方程 当策略 $\\pi$ 达到最优时（记为 $\\pi^\\star$），其对应的价值函数 $v^\\star(s)$ 和 $q^\\star(s, a)$ 满足： 最优状态价值函数方程： $$ v^\\star(s) = \\max_{a \\in A} \\left[ R(s, a) + \\gamma \\sum_{s’ \\in S} P(s’ \\mid s, a) v^\\star(s’) \\right]. $$ 最优动作价值函数方程： $$ q^\\star(s, a) = R(s, a) + \\gamma \\sum_{s’ \\in S} P(s’ \\mid s, a) \\max_{a’ \\in A} q^\\star(s’, a’). $$ 贝尔曼最优方程表明：最优策略下，每一步动作的选择都追求全局最大化累积回报，且最优价值函数是唯一的。通过动态规划或时序差分方法求解这些方程，即可得到强化学习的最优策略。 ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:1","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"价值函数的求法 在强化学习中，价值函数的求解是核心目标之一。根据问题场景的不同，可以采用以下方法： 1. 解析解法 对马尔可夫奖励过程（MRP），价值函数可通过贝尔曼方程的矩阵形式直接求解： $$ \\mathbf{v} = (I - \\gamma \\mathbf{P})^{-1} \\mathbf{R}, $$ 其中 $I$ 是单位矩阵。然而，此方法需要矩阵可逆且计算复杂度为 $O(n^3)$（$n$ 为状态数），仅适用于小规模问题。 2. 动态规划算法 针对MDP，动态规划通过迭代更新价值函数逐步逼近最优解（都可证明迭代一定终止且达到最优）： 策略评估（Policy Evaluation）：固定策略 $\\pi$，迭代计算其状态价值函数，迭代方程类似于贝尔曼方程： $$ v_{k+1}(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\left[ R(s, a) + \\gamma \\sum_{s’} P(s’ \\mid s, a) v_k(s’) \\right]. $$ 值迭代（Value Iteration）：直接迭代最优价值函数： $$ v_{k+1}(s) = \\max_{a} \\left[ R(s, a) + \\gamma \\sum_{s’} P(s’ \\mid s, a) v_k(s’) \\right]. $$ 3. 蒙特卡洛方法 通过采样轨迹（episode）计算经验回报的均值，直接估计价值函数。例如，状态 $s$ 的价值可估计为： $$ v^\\pi(s) \\approx \\frac{1}{N} \\sum_{i=1}^N G_t^{(i)}, $$ 其中 $G_t^{(i)}$ 是第 $i$ 条轨迹中从状态 $s$ 出发的累积回报。 蒙特卡洛方法的优点是可以从经验中直接学习，不需要环境模型，且能处理非马尔可夫环境。其缺点是需要等到轨迹结束才能更新价值，导致学习效率较低，且方差较大。 另外，蒙特卡洛方法也可以增量式更新价值估计： $$ v(s) \\leftarrow v(s) + \\alpha(G_t - v(s)), $$ 其中 $\\alpha$ 是学习率。$\\alpha$ 被设置为 $1/ N(s)$ 时，得到和经典蒙特卡洛方法一样的结果。 4. 时序差分学习（TD Learning） 结合动态规划与蒙特卡洛的思想，逐步更新价值估计（仿照增量式蒙特卡洛方法），它也是一种迭代算法，但是也可以利用经验： $$ v(s_t) \\leftarrow v(s_t) + \\alpha \\left[ r_{t+1} + \\gamma v(s_{t+1}) - v(s_t) \\right], $$ 其中 $\\alpha$ 是学习率，通过**自举（bootstrapping）**利用当前估计值修正误差。$r_{t+1} + \\gamma v(s_{t+1}) $ 是对 $G_t$ 的估计。 三种方法的对比 特性 动态规划 (DP) 蒙特卡洛 (MC) 方法 时序差分 (TD) 学习 环境模型 必须知道环境的完整规则 (例如，知道在某个状态做一个动作后，会以多大概率转移到哪个新状态，并得到多少奖励) 不需要知道环境的规则；直接通过实际尝试和经验来学习 不需要知道环境的规则；直接通过实际尝试和经验来学习 学习方式 基于已知的环境规则进行计算和规划 从完整的、一次从头到尾的经验（称为“片段”）中学习 从不完整的经验片段、甚至是每走一步的经验中学习 自举 (Bootstrapping) 是 (会用之前算出来的对其他状态好坏的估计，来更新当前状态好坏的估计) 否 (对一个状态好坏的估计，完全基于这一次从头到尾实际拿到的所有奖励，不依赖对其他状态的估计) 是 (会用之前算出来的对下一个状态好坏的估计，结合刚拿到的奖励，来更新当前状态好坏的估计) 更新时机 通常是一轮一轮地，对所有可能的状态进行计算更新 必须等一次完整的尝试结束后，才能用这次尝试的总结果来更新 每走一步之后就可以进行一次更新（可以在线边玩边学） 估计偏差 如果环境规则准确，计算结果通常是准确的（相对于真实的好坏程度） 对状态好坏的估计是准确的（因为是基于大量实际完整尝试的平均结果） 对状态好坏的估计可能不完全准确（因为它依赖于对未来状态的不完美估计，尤其在学习初期） 估计稳定性 (方差) 非常稳定，没有随机性 (因为环境规则已知，计算是确定的) 不太稳定，波动较大 (因为每次完整尝试的结果可能因随机因素差异很大) 相对稳定，波动较小 (因为更新主要看眼前的一步和下一个状态的估计) 计算/数据效率 如果状态非常多，计算量会极大 计算每次尝试的总奖励很简单；但可能需要非常多次尝试才能学好 通常比蒙特卡洛方法学习得更快，更有效地利用经验 适用场景 当环境规则完全清楚时，用来做精密的策略评估和改进 适用于那些有明确开始和结束的阶段性任务，并且不知道环境规则时 适用于阶段性任务和持续进行下去的任务，不知道环境规则时，尤其适合需要边行动边学习的在线场景 核心思想 利用已知的状态间转换规则和奖励，迭代计算出各个状态的好坏程度 通过大量实际完整的尝试，用最终获得的平均总奖励来评价状态的好坏 结合动态规划的“用旧估计更新新估计”和蒙特卡洛的“从实际经验学习”，用“刚获得的奖励 + 对下一步的估计”来更新当前 典型算法举例 策略评估、策略迭代、价值迭代 首次访问MC、每次访问MC、MC控制（带探索策略的） TD(0)预测、Q学习 (Q-Learning)、SARSA 在线/离线 通常是离线的规划计算过程 可以是离线学习（先收集一堆经验再学习），也可用于在线评估 非常适合在线学习 ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:2","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"RL 算法分类 强化学习算法可按不同维度分类： 1. 基于模型 vs 无模型（Model-Based vs Model-Free） 基于模型：依赖环境的状态转移 $P(s’ \\mid s, a)$ 和奖励函数 $R(s, a)$ 的先验知识（如动态规划）； 无模型：直接从交互中学习策略，无需环境模型（如Q-learning、REINFORCE）。 2. 基于价值 vs 基于策略（Value-Based vs Policy-Based） 基于价值：学习价值函数（如Q-learning），通过最大化价值选择动作； 基于策略：直接优化策略函数（如REINFORCE），适用于连续动作空间； Actor-Critic：结合两者，策略函数（Actor）生成动作，价值函数（Critic）评估动作优劣。 3. 同策略 vs 异策略（On-Policy vs Off-Policy） 同策略：采样与优化的策略相同（如Sarsa）； 异策略：采样策略（如随机探索）与优化策略（如贪心策略）分离（如Q-learning）。 ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:3","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"动态规划算法 动态规划（DP）是求解MDP的经典方法，核心思想是分治与值函数迭代： 1. 策略迭代（Policy Iteration） 包含两步循环： 策略评估：计算当前策略 $\\pi$ 的价值函数 $v^\\pi$； 策略改进：根据 $v^\\pi$ 更新策略为贪心策略： $$ \\pi_{\\text{new}}(s) = \\arg\\max_a \\sum_{s’} P(s’ \\mid s, a) \\left[ R(s, a) + \\gamma v^\\pi(s’) \\right]. $$ 2. 值迭代（Value Iteration） 直接迭代最优价值函数，直至收敛： $$ v_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s’} P(s’ \\mid s, a) v_k(s’) \\right]. $$ 收敛后提取最优策略： $$ \\pi^\\star(s) = \\arg\\max_a \\sum_{s’} P(s’ \\mid s, a) \\left[ R(s, a) + \\gamma v^\\star(s’) \\right]. $$ ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:4","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"基于价值的算法：Sarsa 和 Q-learning 1. Sarsa（On-Policy TD Control） 通过时序差分更新动作价值函数 $Q(s, a)$： $$ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right], $$ 其中 $a_{t+1}$ 由当前策略 $\\pi$ 选择（如 $\\epsilon$-贪心，有的概率$1-\\epsilon$采用动作价值最大的那个动作，另外有的概率$\\epsilon$从动作空间中随机采取一个动作）。 Sarsa 实际上就是使用 TD 的策略迭代。每次通过迭代得到 Q 函数，然后选取最优策略进行策略提升，直到收敛。 2. Q-learning（Off-Policy TD Control） 更新规则为： $$ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a’} Q(s_{t+1}, a’) - Q(s_t, a_t) \\right]. $$ Q-learning 直接优化最优动作价值函数，与探索策略无关。可以理解为使用 TD 的价值迭代。Q-learning 不断迭代到最优价值函数，然后生成策略。 因为 $\\gamma \\max_{a’} Q(s_{t+1}, a’)$ 实际上是一个贪婪策略（优化策略），但采样策略一般采用 $\\epsilon$-贪心，所以它是 Off-Policy TD control。但显然，Q-Learning 中采样策略和贪婪策略也不是完全无关的，它们都共同依赖 Q 值。 如果状态太多， Q 表格放不下，我们可以把 Q 函数用神经网络表示，上面的迭代步骤就变成了一次梯度上升，这就是 DQN 算法。 ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:5","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"基于策略的算法：REINFORCE 和 Actor-Critic 策略梯度是强化学习中直接优化策略的一类方法，其核心思想是通过梯度上升（Gradient Ascent）调整策略参数，以最大化期望累积回报。与基于价值的方法（如Q-learning）不同，策略梯度不依赖显式的价值函数，而是直接通过策略的概率分布选择动作，尤其适用于连续动作空间和随机策略场景。 设策略由参数 $\\theta$ 参数化为 $\\pi_\\theta(a \\mid s)$，目标函数为期望累积回报。 $$ J(\\theta) = E_{\\tau \\sim \\pi_\\theta} [G_0] = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^\\infty \\gamma^t r_{t+1} \\right] $$ 其中 $\\tau = (s_0, a_0, r_1, s_1, a_1, \\dots)$ 表示轨迹。策略梯度的目标是找到最优参数 $\\theta^\\star$，使得： $$ \\theta^\\star = \\arg\\max_\\theta J(\\theta). $$ 策略梯度定理将目标函数的梯度转化为对动作概率的加权期望，其核心公式为： $$ \\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^\\infty \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot Q^{\\pi_\\theta}(s_t, a_t) \\right], $$ 其中 $Q^{\\pi_\\theta}(s_t, a_t)$ 是动作价值函数。 该定理表明：梯度方向由动作的对数概率梯度与其价值乘积的期望决定。 REINFORCE 算法 REINFORCE 是策略梯度的最基础实现，属于蒙特卡洛方法，通过完整轨迹的回报 $G_t$ 估计梯度： $$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^\\infty \\gamma^t G_t^{(i)} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)}), $$ 参数更新公式为： $$ \\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t). $$ 特点： 无需环境模型，完全无模型； 高方差（因依赖完整轨迹的回报）； 可通过基线（Baseline）减少方差，例如减去状态价值函数 $V(s_t)$： $$ \\nabla_\\theta J(\\theta) \\propto \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot (Q(s_t, a_t) - V(s_t)) \\right]. $$ Actor-Critic 方法 Actor-Critic 结合策略梯度与价值函数（Critic），用Critic评估动作价值以降低方差： Actor（策略函数 $\\pi_\\theta$）：生成动作； Critic（价值函数 $V_w$ 或 $Q_w$）：评估动作优劣，计算优势函数 $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$。 更新规则： Actor 更新： $$ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot A(s_t, a_t). $$ Critic 更新（如TD误差）： $$ w \\leftarrow w + \\beta \\left( r_{t+1} + \\gamma V_w(s_{t+1}) - V_w(s_t) \\right) \\nabla_w V_w(s_t). $$ ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:6","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"TRPO，PPO TRPO TRPO（Trust Region Policy Optimization）（2015年提出）旨在解决策略梯度方法中策略更新不稳定的问题。传统策略梯度（如REINFORCE）若更新步长不当，可能导致策略性能骤降。TRPO通过信赖域约束（Trust Region Constraint），限制新旧策略之间的差异，确保每次更新后策略性能单调提升。其优化目标为最大化替代优势函数，同时约束新旧策略的KL散度（Kullback-Leibler Divergence）： $$ \\max_\\theta E_{s \\sim \\pi_{\\text{old}}, a \\sim \\pi_{\\text{old}}} \\left[ \\frac{\\pi_\\theta(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)} A_{\\text{old}}(s, a) \\right], \\ \\text{s.t. } E_s \\left[ \\text{KL}[\\pi_{\\text{old}}(\\cdot \\mid s) | \\pi_\\theta(\\cdot \\mid s)] \\right] \\leq \\delta. $$ 其中： $\\pi_{\\text{old}}$ 是旧策略，$\\pi_\\theta$ 是新策略； $A_{\\text{old}}(s, a) = Q(s, a) - V(s)$ 是优势函数； $\\delta$ 是KL散度的容忍阈值（如0.01）。 PPO PPO（Proximal Policy Optimization）（2017年提出）是TRPO的简化版本，通过目标函数剪裁替代KL约束，在保持稳定性的同时大幅降低计算复杂度。PPO已成为工业界最主流的强化学习算法之一。 PPO的目标函数通过剪裁策略比率（$\\frac{\\pi_\\theta}{\\pi_{\\text{old}}}$），限制更新幅度： $$ L^{\\text{CLIP}}(\\theta) = E_{s,a} \\left[ \\min\\left( \\frac{\\pi_\\theta(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)} A(s, a), ,, \\text{clip}\\left( \\frac{\\pi_\\theta}{\\pi_{\\text{old}}}, 1-\\epsilon, 1+\\epsilon \\right) A(s, a) \\right) \\right], $$ 其中 $\\epsilon$ 是剪裁阈值（如0.2）。当优势 $A(s,a)$ 为正时，限制策略比率不超过 $1+\\epsilon$；当 $A(s,a)$ 为负时，限制不低于 $1-\\epsilon$。 ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:7","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"RLHF and DPO RLHF 在一开始使用 PPO 算法，具体来说，会先基于人类偏好数据（两个回答中选一个更好的），训练一个奖励模型来评估生成内容的质量。再用奖励模型的分数作为奖励信号，通过 PPO 算法优化语言模型，同时加入 KL 惩罚项以保持与原始 SFT 模型的接近度。 然而，这个过程存在几个问题： 奖励模型训练和使用过程复杂，增加了计算成本 PPO 训练不稳定，需要仔细调参 KL 惩罚超参数难以选择 DPO（Direct Preference Optimization）（2023年提出）提供了一个更简单的解决方案。它证明了在某些条件下，基于人类偏好的强化学习问题可以转化为监督学习问题。关键在于： 最优策略应满足一个隐式的奖励函数，使得被人类偏好的回答 $y_w$ 获得更高奖励。 这个奖励函数可以通过 Bradley-Terry 模型直接表示，其中策略与参考策略（SFT模型）的对数比值就对应于相对奖励。 Bradley-Terry 模型中，$p(y_w \\succ y_l \\mid x)$ 表示在给定输入 $x$ 的情况下，人类偏好输出 $y_w$ 而不是 $y_l$ 的概率。当 $\\pi_\\theta$ 对优质输出 $y_w$ 的概率相对于 $\\pi_{\\text{ref}}$ 提升时，人类偏好它的概率也随之增加。 $$ p(y_w \\succ y_l \\mid x) = \\frac{\\exp(\\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\text{ref}}(y_w \\mid x)})}{\\exp(\\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\text{ref}}(y_w \\mid x)}) + \\exp(\\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\text{ref}}(y_l \\mid x)})}， $$ 简单来说，在每个偏好数据 $(x, y_w, y_l)$ 中： $x$ 是输入提示 $y_w$ 是人类更喜欢的回答 $y_l$ 是人类不太喜欢的回答 $\\pi_{\\theta}$ 是待优化的模型（新策略） $\\pi_{\\text{ref}}$ 是 SFT 模型（参考策略） $\\beta$ 是超参数，控制参考模型偏离程度 DPO 直接最大化被偏好回答相对于未被偏好回答的概率，同时通过超参数 $\\beta$ 控制与参考模型的偏离程度。这样就绕过了显式的奖励建模，将 RLHF 简化为一个二分类问题。 $$ L_{\\text{DPO}} = -E_{(x, y_w, y_l)} \\left[ \\log \\sigma\\left( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\text{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\text{ref}}(y_l \\mid x)} \\right) \\right], $$ 其中 $\\sigma$ 是sigmoid函数，$\\beta$ 控制策略与参考策略的偏离程度。 相比传统 RLHF，DPO 具有以下优势： 训练流程更简单，无需奖励模型 训练更稳定，超参数更少 计算效率更高，可以使用标准的交叉熵损失训练 这种优化人类偏好的简单而有效的方法，使得大语言模型的对齐变得更加实用。 ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:8","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":"DPO in Action 在实践中，我们可直接使用 Hugging Face 的 trl 库提供的 DPOTrainer。 trainer = DPOTrainer( model_name_or_path=model_name, # 策略模型 # ref_model_name_or_path=model_ref_name, # 参考模型路径，如果省略，则使用 model_name_or_path 的初始状态 args=dpo_config, tokenizer=tokenizer, train_dataset=train_dataset, # peft_config=peft_config, # 如果使用 PEFT (如 LoRA) ) trainer.train() ","date":"2025-05-15","objectID":"/posts/adaptation-in-action/:4:9","tags":["LLM","RL"],"title":"Adaptation in Action (and RL Basic)","uri":"/posts/adaptation-in-action/"},{"categories":["AI"],"content":" Time is money. ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:0:0","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Intro 在 Building a Transformer LM 中，我们使用 Pytorch 提供的基础 API 实现了一个语言模型。在这篇博客中，我们要穿透 Pytorch 提供的抽象层，看看语言模型究竟是如何运行在硬件上的，我们又能针对硬件做些什么优化。在最后，我们还会简单的介绍 scaling law，以及推理和部署服务问题。 主要参考资料： https://stanford-cs336.github.io/spring2024/ https://dlsyscourse.org/ https://hao-ai-lab.github.io/cse234-w25/ https://www.cs.cmu.edu/~15418/ ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:1:0","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Pytorch internel 在实现语言模型时，我们使用到的 Pytorch API 主要有三类： 基本的 Tensor 和 Module API 优化器 optimizer.step() 自动微分 loss.backward() 语言模型由神经网络表示，神经网络的输入和输出都是高维向量，也就是 Tensor. Tensor 在内存中即是平坦的一维数组，但可以通过 shape 和 strides 两个 tuple 将其表示为高维向量。其中 shape 是表示每个维度的大小，strides 表示每个维度相邻两个元素的距离。 class NDArray: # Tensor Implementation def _init(self, other): self._shape = other._shape self._strides = other._strides self._offset = other._offset self._device = other._device self._handle = other._handle 通过 shape-strides 表示法，许多操作，如 transpose，reshape 都可以不必操作底层元素而直接改变这两个 tuple 就可以完成。但是对于某些操作就必须要先转化为 compat 数组才能进行。 有了 Tensor 之后，我们就可以实现神经网络的各种 building block 了，如 Linear 层： class Linear(Module): def __init__( self, in_features, out_features, bias=True, device=None, dtype=\"float32\" ): super().__init__() self.in_features = in_features self.out_features = out_features self.weight = Parameter( init.kaiming_uniform(in_features, out_features, device=device, dtype=dtype)) if bias: self.bias = Parameter( init.kaiming_uniform(out_features, 1, device=device, dtype=dtype) .reshape((1, out_features))) def forward(self, X: Tensor) -\u003e Tensor: Z = X @ self.weight if not self.bias is None: Z = Z + self.bias.broadcast_to(Z.shape) return Z 同时，为了方便地从父模型得到子模型的参数，我们可以使用一些 python 的反射特性，遍历得到所有类型为 Parameter 的 field。 有了各种 building block，我们就可以构建语言模型了，同时也很容易就能实现前向传播。 class TransformerLM(nn.Module): def __init__( self, vocab_size: int, context_length: int, d_model: int, num_layers: int, num_heads: int, d_ff: int, attn_pdrop: float, residual_pdrop: float, **kwargs ): super().__init__() self.embed = nn.Embedding(vocab_size, d_model) self.pos_embed = nn.Parameter(torch.zeros(context_length, d_model)) self.drop = nn.Dropout(residual_pdrop) self.layers = nn.Sequential(*[TransformerBlock(d_model, num_heads, d_ff, attn_pdrop, residual_pdrop) for _ in range(num_layers)]) self.ln_f = RMSNorm(d_model) self.head = nn.Linear(d_model, vocab_size, bias=False) self.context_length = context_length def forward(self, x): x = self.embed(x) x = x + self.pos_embed[:x.size(1), :] x = self.drop(x) x = self.layers(x) x = self.ln_f(x) x = self.head(x) return x 但在神经网络训练中，我们还需要反向传播，计算各个参数相对于 loss 的梯度。 最简单的，我们可以通过数值微分方法得到梯度，但这样计算复杂度太高，因此我们考虑采取符号微分。考虑到神经网络计算可以表示为计算图，其中每一个节点都是一个中间值，我们可以利用链式法则来自动求得 loss 相对于某个参数的微分（实际实现时要牢记微分的定义，以免被复杂的 Tensor 运算搞晕）。 根据利用链式法则求导时固定微分的是分子还是分母，我们可以分别实现前向自动微分或者后向自动微分。但显然后向自动微分固定分子，有更多的可复用性。 我们可以在定义神经网络时实际构建计算图，同时也实际构建反向传播的计算图。这样，我们就可以先对计算图进行编译优化，再运行/训练神经网络了。 Tensorflow 原生地支持先定义计算图，再运行。在 Pytorch 中，我们可以通过给函数添加 @torch.compile 装饰器，将命令式代码也转化为计算图，然后进行编译优化。 总之，通过一次正向传播和一次反向传播得到每个参数的梯度后，我们就可以使用优化器遍历所有参数，通过一定的优化规则更新参数了。 因为神经网络中大量的矩阵运算，我们可以使用 GPU 来加速运算。CPU 中大量计算单元都用于实现流水线，超线程，动态多发射，乱序执行，以及 Speculation 和复杂的缓存机制。而 GPU 没有这么多复杂功能，却有大量简单的执行单元，我们通过 data-parallel model 来使用 GPU 的大量计算单元编程。这个 workload 与简单大量的 Tensor 运算十分相合。 不同厂家的GPU都着相似但不同的编程模型，幸好，Pytorch 会帮我们处理这一切。我们只需要在创建 Tensor 传入 device 参数就可以了。 ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:2:0","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Benchmarking and profiling 我们在学习原理时往往不注意性能考量，但在各种资源有限的实践中性能却是非常重要的。就像凯恩斯说的，“站在长尺度下，我们都会死”，时间和计算资源约束在实践中往往起到决定性的作用。因此，性能优化也就成了一种必须。 但是 “Premature optimization really the root of all evil”，我们不能想当然的去任意优化，只有在 benchmark 和 profile 后，我们才知道自己的实现性能究竟如何，bottleneck 又在哪里，这样才能去针对性优化。而在当前的硬件中，计算和内存是两种最主要的资源，因此计算耗时和内存占用是我们主要 profile 的对象。 在性能优化中，我们继续使用之前训练的模型，具体参数如下： { \"vocab_size\": 10000, \"context_length\": 256, \"d_model\": 512, \"num_layers\": 4, \"num_heads\": 16, \"d_ff\": 2048, } ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:3:0","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Simple Benchmark 首先，我们需要知道我们的实现跑的有多快。我们需要分别考虑前向和后向传播的 workload，前者代表推理，后者代表训练。在 benchmark 开始前，我们需要一些 warm-up steps，以使得系统达到稳定状态（如填充 cache，部分语言的 JIT 编译，GC 达到稳定状态等）。 def run_benchmark(model: nn.Module, input_data: torch.Tensor, device: torch.device, warmup_steps: int, timed_steps: int, pass_type: str): ... # --- Warm-up Phase --- print(\"Running warm-up steps...\") with context: for _ in range(warmup_steps): outputs = model(input_data) if pass_type == 'forward_backward': loss = outputs.sum() # Dummy loss optimizer.zero_grad(set_to_none=True) # More efficient loss.backward() optimizer.step() synchronize(device) # Synchronize after each warm-up step, beacuse cuda is calld asycnly # --- Timing Phase --- print(f\"Running {timed_steps} timed steps...\") total_elapsed_time = 0.0 with context: for i in range(timed_steps): synchronize(device) # Ensure previous work is done (esp. CUDA async) start_time = timeit.default_timer() outputs = model(input_data) if pass_type == 'forward_backward': loss = outputs.sum() # Dummy loss optimizer.zero_grad(set_to_none=True) loss.backward() optimizer.step() synchronize(device) # Ensure operation completes before stopping timer end_time = timeit.default_timer() total_elapsed_time += (end_time - start_time) avg_time_per_step = total_elapsed_time / timed_steps print(\"--- Benchmark Complete ---\") return avg_time_per_step 随机生成长度为 context-length 的向量，在我的 M4 芯片上经过 5 step 的 warm up 后再 banchmark 20 step 的反向传播，得到结果如下。 --- Benchmark Results --- Device: mps Pass Type: forward_backward Batch Size: 16 Average time per step: 0.482039 seconds Steps per second: 2.07 --- Benchmark Results --- Device: mps Pass Type: forward Batch Size: 16 Average time per step: 0.156253 seconds Steps per second: 6.40 根据上面的基准测试结果，可以得出： 在 batch size 为 16 的情况下，模型的推理吞吐量为 6.40 steps/s * 16 tokens/step = 102.4 tokens/s。但单步解码（非批处理）的速度仍然限制在 6.4 tokens/s。 对于 2GB 的训练数据： 估计总 Token 数：2048 MB * 250,000 tokens/MB = 512,000,000 tokens 有效训练样本数：512,000,000 tokens / 1024 tokens/sample ≈ 500,000 samples 每个 Epoch 所需步数：500,000 samples / 16 samples/step ≈ 31,250 steps 单个 Epoch 训练时间：31,250 steps * 0.482039 seconds/step ≈ 15,064 秒（约 4.18 小时） 相比之前在 Building a Transformer LM 中仅通过 GPU 理论吞吐率进行的粗略估算，这次基于实际基准测试的时间预估显然更为准确。不过结果表明，使用 M4 芯片进行训练的效率确实不太理想。 ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:3:1","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Profile Essence 其次，我们要知道我们的实现为什么跑这么快，这样才能针对性优化。但是 benchmark 并没有告诉我程序具体每一部分执行的时间。而这就需要 profiler 出场了。 profile 的基本原理为在程序中加入一些探针，收集运行时的信息，就像中医里的针灸一样。探针既可以编译时静态添加，也可以使用 DTrace 等工具在运行时动态添加。而在 Python 这样的虚拟机语言中，解释器本身就提供了一套 profiler 实现，也就是 cProfile。我们可以是利用 Python 解释器提供的事件钩子（hooks）。通过 sys.setprofile() 函数，可以注册一个回调函数，这个回调函数会在特定的事件发生时被解释器调用。call，return 字节码指令的执行都可以是我们关心的事件。 但 cProfile 并不能很好的适配 pytorch。pytorch 提供了 torch.profiler，它会追踪 GPU执行，CPU执行以及内存分配等多种事件，并把整合起来。具体使用，如下面代码所示。 def run_profile(model: nn.Module, input_data: torch.Tensor, device: torch.device, profile_steps: int, pass_type: str, profile_memory: bool, with_stack: bool, output_dir: str = \".\"): ... # Define the step function to be profiled def profile_step(): ... profile_step() synchronize(device) print(\"Starting profiler context...\") if profile_memory and device.type == 'cuda': torch.cuda.memory._record_memory_history(max_entries=10000) with profile(activities=activities, record_shapes=True, profile_memory=profile_memory, with_stack=with_stack, schedule=torch.profiler.schedule(wait=0, warmup=0, active=1, repeat=profile_steps), experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof: # with_stack MUST be True for export_stacks for _ in range(profile_steps): profile_step() # Need to sync *inside* the step for CUDA kernel timings in profiler results # but *outside* the record_functions if you want to profile the sync itself. # Syncing after the main work is usually best for seeing kernel times. synchronize(device) prof.step() # Signal the profiler that one step is done print(\"--- Profiling Complete ---\") ... 在通过 profile 收集了 rawdata 之后，我们就可以展现为人类可理解的信息。如，基于事件的时间线视图 Chrome Trace，或者基于调用栈样本的聚合视图 Flame Graph。前者让我看的程序按时间顺序都做了什么，都耗时多少，后者告诉我们每个函数都运行了多少时间。 我们以 CPU profile 为例，可以看到大部分时间都在 Transformer Block 中。 先看 forward pass，很明显，多头注意力计算占据了绝大部分时间，而其中最耗时的则是 softmax 和 Dropout 操作。 另外反向传播耗时略大于正向传播，但无明显瓶颈。 从火焰图中我们也可以验证这一点。因为在 pytorch 实现中，反向传播也通过计算图实现，也会调用 forward 方法，因此火焰图中的每个计算都是结合了正反向传播的， Chrome Trace 考虑了时序信息，更适合分析响应时间，暂停时间等问题。而火焰图考虑程序计算整体，更适合提升吞吐量，找到性能瓶颈。在神经网络中，显然吞吐量是更重要的，因为火焰图也就更为清楚了。从下面放大的火焰图中我们可以看出，最耗时的操作为： Dropout Softmax Matrix mal Linear Mask 但是这并没有结束，当我们查看 mps 火焰图时，就会发现在 GPU 上 Softmax 操作反而占据了绝大部分时间。由此可见不同的设备上，瓶颈也是不同的。 另外我们也可以对内存进行 profile（M系列芯片GPU和CPU共用内存），可以发现，与想象中的不同，参数只占据了很少一部分内存，绝大部分内存都用来存放激活值，剩下的少部分则主要为优化器状态（AdamW）。之所以内存时间图成山峰状是因为随着正向转播，激活的值越来越多。而反向传播后不需要的激活值会被及时释放。 ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:3:2","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Single GPU Optimize 既然通过 profile 知道了具体运行情况，我们就可以做进一步优化了！ ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:4:0","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Mixed precision 首先如何降低考虑峰值 2GB 的可怕的显存占用。考虑到我们在当前的实现中全程使用 32 位浮点数，这是一笔不小的内存开销，可以考虑在准确率运行范围内降低精度。再考虑到一些 GPU 还针对低精度浮点数做了许多优化，我们就更可以获得速度上的提升了！NVIDIA A100 的规格表显示，其 FP32 的最大吞吐量为 19.5 TFLOP/秒，而其 FP16（半精度浮点数）或 BF16（脑浮点格式 bfloat16）的最大吞吐量则显著更高，达到了 312 TFLOP/秒。 然而，简单地将我们的模型转换为较低精度的格式可能会导致模型准确率下降。例如，在实践中，许多梯度值往往过小，无法用 FP16 表示，因此在使用 FP16 精度进行简单训练时这些梯度值会变成零。为了解决这个问题，在使用 FP16 进行训练时，通常会采用损失缩放（loss scaling）技术——即将损失值乘以一个缩放因子，从而增大梯度的大小，以防止它们下溢归零。 在实际使用中，我们使用 pytorch 的 autocast context manager 就可以，如： with autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type == 'cuda')): outputs = model(dummy_data) loss = criterion(outputs, dummy_labels) ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:4:1","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Fused Kernel 因为 Softmax 是我们的瓶颈之一，我们可以先考虑优化 Softmax。目前的 Softmax 实现如下。 def softmax(x, dim): max_vals = torch.max(x, dim=dim, keepdim=True).values shifted_x = x - max_vals x_exp = torch.exp(shifted_x) x_exp_sum = torch.sum(x_exp, dim=dim, keepdim=True) return x_exp / x_exp_sum 考虑到在 Python 间传值效率较低，多个计算步骤融合 (fuse) 到了一个单一的 GPU Kernel（内核程序）中执行。 如同写一般程序，我们可以用汇编，可以用 c，也可以用 Python。写 kernel（针对CUDA） 我们也有 SASS，CUDA，Triton 几种选项。 但是 Trtion 是时间投入有限时性价比最高的一种。如同 Cython 一样，Triton 语言是利用了 Python 的装饰器机制，潜入了 Python 语言之中，可以很容易地整合进一般 Python 程序。下面是使用 Triton 写的 fused softmax kernel。 @triton.jit def softmax_kernel( output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE_N: tl.constexpr, ): row_idx = tl.program_id(axis=0) row_start_ptr_input = input_ptr + (row_idx * input_row_stride) row_start_ptr_output = output_ptr + (row_idx * output_row_stride) col_offsets = tl.arange(0, BLOCK_SIZE_N) input_mask = col_offsets \u003c n_cols row_data = tl.load(row_start_ptr_input + col_offsets, mask=input_mask, other=-float('inf')) row_max = tl.max(row_data, axis=0) numerator = row_data - row_max numerator = tl.exp(numerator) denominator = tl.sum(numerator, axis=0) softmax_output = numerator / denominator tl.store(row_start_ptr_output + col_offsets, softmax_output, mask=input_mask) # --- Wrapper function to launch the kernel --- def softmax(x: torch.Tensor): if not x.is_cuda: print(\"输入张量必须在 CUDA 设备上。正在移至 CUDA...\") x = x.to('cuda') x = x.contiguous() n_rows, n_cols = x.shape output = torch.empty_like(x) BLOCK_SIZE_N = triton.next_power_of_2(n_cols) if BLOCK_SIZE_N \u003e 16384 and n_cols \u003c= 16384 : BLOCK_SIZE_N = triton.next_power_of_2(n_cols) if n_cols \u003e 0 else 1 elif n_cols == 0: return output grid = (n_rows,) softmax_kernel[grid]( output, x, x.stride(0), output.stride(0), n_cols, BLOCK_SIZE_N=BLOCK_SIZE_N, ) return output 除了简单的 Fused Kernel，我们还有更深入的优化手段。考虑到 Attention 中的注意力矩阵占据了绝大部分内存，而且 Softmax 的速度瓶颈也是由于这个庞大的注意力矩阵造成的。当 Cache 无法完全容纳注意力矩阵时，我们可以采用 Tiling 方法将其分块计算，以最大化地复用 Cache。同时，可以采用 Online Softmax 算法来替代传统 Softmax，避免读取整行数据的开销。 除此之外，还有从数据、计算、内存等多个角度出发的优化方法，在此不再赘述。 ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:4:2","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Distributed data parallel training 性能是程序实际运行中的某种度量，但因为系统和目标的复杂性，优化手段并没有一个统一的抽象的分类体系。只有一些经验性的思路和具体的 toolbox。 在性能优化中，常见的思路有： 减少不必要的工作（如编译优化中的冗余消除，近似计算，Lazy Evaluation） 提高原子操作的效率（如采用硬件优化，采用高效的算法） 并行化（任务分解，流水线，异步编程） 减少资源访问延迟（Cache，批处理，数据局部性） 有效管理和利用资源（线程池，负载均衡，资源调度） 识别关键路径和瓶颈（fast path，slow path） 明智权衡（如同样是性能的吞吐量和延迟间的权衡，性能和其他质量属性间的权衡） 测量，分析，迭代 而在摩尔定律终结之后，并行化成为了提升性能最显著的方法之一。对于计算依赖较小的计算密集型程序，性能几乎可以随硬件数量线性提升。下面，我们将研究如何在深度学习中运用并行化方法来优化性能。 ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:5:0","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Data Parallelism 就如同一般并行化方法中的函数并行和数据并行一样，神经网络也可以依靠这种思路并行化。首先就是数据并行，最直观的想法就是将一个 batch 分成多分，分别在各个 CPU 上前向，后向传播并计算梯度，之后再聚合梯度并更新参数。 在聚合梯度时，我们也有多种选择： Parameter Server：使用一个参数服务器集中管理所有参数 但是参数服务器就会变成瓶颈 AllReduce：让各个设备之间自行同步梯度，根据交流的拓扑，又可分为 Ring AllReduce, Tree AllReduce, Butterfly AllReduce 等。 我们一般都采用 Ring AllReduce，因为其在延迟，负载均衡和扩拓展性上都比较好。 此外，因为在 Data Parallelism 中，每个 GPU 都保存了一份完整的模型，所以难以训练超过 GPU 显存的模型。因此，Deepspeed 创造了 ZeRO 框架来优化显存占用，将优化器状态，梯度和参数分散在各个 GPU 上。当然 ZeRO 也不可避免地增加了通信开销。 ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:5:1","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Model \u0026 Pipeline Parallelism 对应函数并行的是 Model \u0026 Pipeline Parallelism。这种方法将模型看作一个函数，将其拆分为多个子图并分散到不同的 GPU 上。每个子图可以看作一个子函数，甚至单个矩阵乘法操作也可以分散到多个 GPU 上进行计算。 函数并行一方面可以和数据并行结合，训练更大的模型。但更为重要的是可以通过拆分函数引入流水线，进一步提高吞吐率。 ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:5:2","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Scaling Scaling Law 主要用来回答这样的问题：在算力有限的情况下，我们应该如何权衡模型大小和训练数据量，以达到最低的 loss，可以数学表示为下面的公式。其核心思想是以小见大。 Scaling Law 的一个灵感来自于传统 ML 中的 Data Scaling Law。 Data Scaling Law 是通过训练数据量估计高斯分布模型的loss，log(Error) = − log(n) + 2 log(σ) 。但语言模型却没有这样简明的数学公式可以推导出来。所以我们只能按照物理学的方法，收集实验数据并提出假设。 DeepMind 在其名为 “Chinchilla” 的模型研究提出了我们常说的 Scaling Law： 总而言之就是，模型参数量和训练数据应该同步缩放，以达到最低 loss。 ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:6:0","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"Inference and Serving 在涉及到模型的推理和部署服务时，我们需要在延迟、吞吐量和准确率之间取得平衡。以下是一些常见的优化方法： 模型压缩 量化：降低数值精度，减少内存和计算开销 剪枝：移除不重要的网络连接 知识蒸馏：将大模型知识迁移到小模型 注意力机制优化 PagedAttention：借鉴操作系统分页机制，实现 KV Cache 的动态内存管理 支持非连续显存分配，解决长序列推理时的显存碎片问题 推理阶段优化 Chunked Prefill：将长输入分块处理，降低显存压力 计算解码分离：将计算密集的预填充和内存密集的解码分配到不同硬件 Speculative Decoding：使用小模型预测，大模型验证，减少约 30% 解码步骤 批处理策略 Continuous Batching：通过动态调度实现​​细粒度请求管理，如每个解码步骤（生成1个token）重新评估批次组成，新请求可立即加入，完成请求及时退出 Adaptive Batching：根据序列长度动态调整批次大小 Delayed Batching：引入固定延迟窗口以积累更多请求，提升整体吞吐量 ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:7:0","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["AI"],"content":"LLM API and SSE 我们在调用 LLM API 时，常常会使用 SSE（Server-Sent Events）协议，该协议支持服务器向客户端单向推送数据，非常适合大模型这样的流式文本输出。一个简单的 client-server 例子如下： const eventSource = new EventSource('/api/stream'); eventSource.onmessage = (event) =\u003e { const data = JSON.parse(event.data); document.getElementById('output').innerHTML += data.token; }; eventSource.addEventListener('end', () =\u003e { eventSource.close(); }); event: message data: {\"token\": \"春\", \"finished\": false} event: end data: {\"finished\": true} from fastapi import Request import openai import asyncio @app.get(\"/api/stream\") async def stream_response(request: Request): async def event_generator(): response = await openai.ChatCompletion.acreate( model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": \"写一个递归的快速排序函数\"}], stream=True ) async for chunk in response: token = chunk.choices[0].delta.get(\"content\", \"\") yield f'data: {{\"token\": \"{token}\"}}\\n\\n' yield \"event: end\\ndata: {}\\n\\n\" return EventResponse(event_generator()) ","date":"2025-05-10","objectID":"/posts/optimize-the-performance-of-a-lm/:8:0","tags":["LLM","Code","Performance"],"title":"Optimize the Performance of a LM","uri":"/posts/optimize-the-performance-of-a-lm/"},{"categories":["Instinct"],"content":" 本文由 Gemini 辅助生成 在信息爆炸的时代，我们每天都会接触到海量的数据和数字。从国家财政预算的万亿，到病毒大小的纳米，再到计算机模型的数十亿参数，这些数字跨越了极其广阔的范围。然而，对于这些巨大或微小的数字，我们常常缺乏直观的感受，难以理解它们真实的含义和相对大小。 培养“数字直觉”——也就是对数字大小和比例的快速、准确的感知能力——对于我们理解世界、做出判断至关重要。这篇博客旨在通过梳理不同领域的数字尺度，并结合可视化方法，帮助我们建立起这种直觉。 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:0:0","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["Instinct"],"content":"1. 数字的表示与单位：从“个”到“无量大数” 理解数字的基础是掌握其表示方法和单位。我们常用的十进制系统，配合不同的单位，可以描述从极小到极大的数值。 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:1:0","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["Instinct"],"content":"1.1 常用英文与中文单位对照 现代科学和国际交流中，英文单位（如 Million, Billion, Trillion）和科学计数法 ($10^n$) 是标准。中文单位则有其独特的体系，尤其是在大数和小数上。了解它们之间的对应关系是建立数字直觉的第一步。 英文单位 中文单位 阿拉伯数字 科学计数法 国际单位制词头 One 个 1 $10^0$ — Ten 十 10 $10^1$ Deca (da) Hundred 百 100 $10^2$ Hecto (h) Thousand 千 1,000 $10^3$ Kilo (k) Ten thousand 万 10,000 $10^4$ — Hundred thousand 十万 100,000 $10^5$ — Million 百万 1,000,000 $10^6$ Mega (M) Ten million 千万 10,000,000 $10^7$ — Hundred million 亿 100,000,000 $10^8$ — Billion 十亿 1,000,000,000 $10^9$ Giga (G) Ten billion 百亿 10,000,000,000 $10^{10}$ — Hundred billion 千亿 100,000,000,000 $10^{11}$ — Trillion 万亿（兆） 1,000,000,000,000 $10^{12}$ Tera (T) 一些示例： 地球人口大约是 8 Billion 人，即 $8 \\times 10^9$ 人，或 80 亿人。 一个大型跨国公司的年收入可能是 100 Billion 美元，即 $10^{11}$ 美元，或 1000 亿美元。 全球每年的经济总量（GDP）接近 100 Trillion 美元，即 $10^{14}$ 美元，或 100 万亿美元。 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:1:1","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["Instinct"],"content":"1.2 小数单位与传统中文单位 对于小于1的数字，我们也有相应的单位。 英文单位 中文单位 阿拉伯数字 科学计数法 国际单位制词头 Thousandth 毫 0.001 $10^{-3}$ Milli (m) Millionth 微 0.000001 $10^{-6}$ Micro (μ) Billionth 纳 0.000000001 $10^{-9}$ Nano (n) Trillionth 皮 0.000000000001 $10^{-12}$ Pico (p) 传统中文单位在大数和小数上有着更丰富的层级： 大数： 万（$10^4$）→ 亿（$10^8$） → 兆（$10^{12}$） → 京（$10^{16}$） → … → 无量大数（$10^{68}$） 小数： 分（$10^{-1}$）→ 厘（$10^{-2}$）→ 毫（$10^{-3}$）→ 丝（$10^{-4}$）→ 忽（$10^{-5}$）→ 微（$10^{-6}$）→ 纤（$10^{-7}$）→ 沙（$10^{-8}$）→ 尘（$10^{-9}$）→ 埃（$10^{-10}$） 一些示例： 人类头发的直径大约是 50-100 微米，即 $50 \\times 10^{-6}$ 米 到 $100 \\times 10^{-6}$ 米。 一个水分子的直径大约是 0.3 纳米，即 $0.3 \\times 10^{-9}$ 米。 硅原子的直径大约是 0.2 纳米，即 $0.2 \\times 10^{-9}$ 米。 传统的“一埃”等于 $10^{-10}$ 米，常用于描述原子尺度。 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:1:2","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["Instinct"],"content":"2. 物理世界的尺度：从原子到宇宙 物理世界涵盖了从微观粒子到浩瀚宇宙的广阔尺度。 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:2:0","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["Instinct"],"content":"2.1 长度的尺度 量纲 典型值 科学计数法 测量方法/说明 原子直径 零点一到零点五纳米 (Zero point one to zero point five nanometers) $10^{-10}$ m 扫描隧道显微镜 (STM) DNA 双螺旋直径 两纳米 (Two nanometers) $2 \\times 10^{-9}$ m X射线衍射 病毒大小 二十到三百纳米 (Twenty to three hundred nanometers) $10^{-8}$ m 电子显微镜 细菌大小 零点五到五微米 (Zero point five to five micrometers) $10^{-6}$ m 光学显微镜 人类头发直径 五十到一百微米 (Fifty to one hundred micrometers) $10^{-5}$ m 光学显微镜 灰尘颗粒大小 一到一百微米 (One to one hundred micrometers) $10^{-5}$ m 光学显微镜 蚂蚁长度 一毫米到两厘米 (One millimeter to two centimeters) $10^{-3}$ m 直接测量 人的身高 一点五到两米 (One point five to two meters) $10^0$ m 直接测量 珠穆朗玛峰高 八千八百四十八米 (Eight thousand eight hundred forty-eight meters) $8.8 \\times 10^3$ m 测量 地壳厚度 五到七十千米 (Five to seventy kilometers) $10^4$ m 地震波走时分析 平流层高度 十五到五十千米 (Fifteen to fifty kilometers) $10^4$ m 气象探测 地球半径 六千三百七十一千米 (Six thousand three hundred seventy-one kilometers) $6.4 \\times 10^6$ m 地震波走时分析 地月距离 三十八万四千四百千米 (Three hundred eighty-four thousand four hundred kilometers) $3.8 \\times 10^8$ km 激光测距 地球绕太阳轨道半径 一点五亿千米 (One hundred fifty million kilometers) $1.5 \\times 10^{11}$ km 天文测量 太阳系直径 约一百天文单位 (Approximately one hundred astronomical units) $1.5 \\times 10^{13}$ m 估算 ($1 , \\text{AU} = \\text{地球到太阳距离}$) 离太阳最近的恒星（比邻星）距离 四点二光年 (Four point two light-years) $4 \\times 10^{16}$ m 视差法 银河系直径 约十万光年 (Approximately one hundred thousand light-years) $10^{21}$ m 天文观测 可观测宇宙直径 约九百三十亿光年 (Approximately ninety-three billion light-years) $9 \\times 10^{26}$ m 宇宙学模型 一些示例： 一个典型的细菌 ($10^{-6}$ m) 比人类头发 ($10^{-5}$ m) 小约 10 倍。 地球的半径 ($6.4 \\times 10^6$ m) 大约是珠穆朗玛峰高度 ($8.8 \\times 10^3$ m) 的 700 多倍。 从地球到最近的恒星（比邻星）的距离 ($4 \\times 10^{16}$ m) 是地月距离 ($3.8 \\times 10^8$ km) 的约 100 万亿倍。 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:2:1","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["Instinct"],"content":"2.2 时间的尺度 量纲 典型值 科学计数法 说明/事件 光速穿过原子所需时间 十的负十八次方秒 (Ten to the power of minus eighteen seconds) $10^{-18}$ s 阿秒 (Attosecond) 物理学领域 原子核反应时间 十的负十五次方秒 (Ten to the power of minus fifteen seconds) $10^{-15}$ s 飞秒 (Femtosecond) 化学反应尺度 分子振动周期 十的负十二次方秒 (Ten to the power of minus twelve seconds) $10^{-12}$ s 皮秒 (Picosecond) 晶体管开关时间 十的负九次方秒 / 一纳秒 (Ten to the power of minus nine seconds / One nanosecond) $10^{-9}$ s 纳秒 (Nanosecond) 计算机CPU时钟周期 人类眨眼时间 零点一到零点四秒 (Zero point one to zero point four seconds) $10^{-1}$ s 一天 八万六千四百秒 (Eighty-six thousand four hundred seconds) $8.6 \\times 10^4$ s 一年 三点一五乘以十的七次方秒 (Three point one five times ten to the power of seven seconds) $3.15 \\times 10^7$ s 人类平均寿命 约八十年 (Approximately eighty years) $2.5 \\times 10^9$ s 有文字记载的历史 约五千年 (Approximately five thousand years) $1.6 \\times 10^{11}$ s 农业文明史 约一万年 (Approximately ten thousand years) $3.15 \\times 10^{11}$ s 智人出现时间 约二十万年 (Approximately two hundred thousand years) $6.3 \\times 10^{12}$ s 旧石器时代开始 约两百万年 (Approximately two million years) $6.3 \\times 10^{13}$ s 恐龙灭绝时间 六千六百万年 (Sixty-six million years) $2.1 \\times 10^{15}$ s 新生代开始 寒武纪大爆发 五点四亿年 (Five hundred forty million years) $1.7 \\times 10^{16}$ s 显生宙开始 地球形成时间 四十五亿年 (Four point five billion years) $1.4 \\times 10^{17}$ s 冥古宙开始 宇宙年龄 一百三十八亿年 (Thirteen point eight billion years) $4.3 \\times 10^{17}$ s 一些示例： 人类眨眼的时间 ($10^{-1}$ s) 比计算机CPU的一个时钟周期 ($10^{-9}$ s) 慢约 1 亿倍。 从恐龙灭绝到现在 ($2.1 \\times 10^{15}$ s) 的时间大约是人类平均寿命 ($2.5 \\times 10^9$ s) 的 80 多万倍。 宇宙的年龄 ($4.3 \\times 10^{17}$ s) 是地球年龄 ($1.4 \\times 10^{17}$ s) 的约 3 倍。 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:2:2","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["Instinct"],"content":"3. 社会与经济的尺度 社会和经济领域也充满了大大小小的数字，反映着人类活动的规模和复杂性。 量纲 典型值 科学计数法 说明/来源 世界人口 约八十亿人 (Approximately eight billion people) $8 \\times 10^9$ 人 Worldometer: 世界人口 中国人口 约十四亿人 (Approximately one point four billion people) $1.4 \\times 10^9$ 人 国家统计局 全球 GDP 约一百万亿美元 (Approximately one hundred trillion US dollars) $10^{14}$ 美元 世界银行 美国年度财政预算 约六万亿美元 (Approximately six trillion US dollars) $6 \\times 10^{12}$ 美元 USA.gov: 美国财政预算 某大型公司市值 约两万亿美元 (Approximately two trillion US dollars) $2 \\times 10^{12}$ 美元 某城市人口 约一千万人 (Approximately ten million people) $10^7$ 人 某大学学生数 约五万人 (Approximately fifty thousand people) $5 \\times 10^4$ 人 某产品年销量 约一百万件 (Approximately one million units) $10^6$ 件 一些示例： 全球人口 ($8 \\times 10^9$) 大约是中国人口 ($1.4 \\times 10^9$) 的 5 倍多。 全球 GDP ($10^{14}$ 美元) 是美国年度财政预算 ($6 \\times 10^{12}$ 美元) 的约 16 倍。 一个拥有 $10^7$ 人口的城市的人口是一个拥有 $5 \\times 10^4$ 学生的大学的 200 倍。 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:3:0","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["Instinct"],"content":"4. 计算机与信息的尺度 在计算机和信息领域，我们处理着从字节到拍字节，从代码行数到模型参数量的各种数字。 量纲 典型值 科学计数法 说明/来源 一个字节 (Byte) 八比特 (Eight bits) $8 \\times 10^0$ bit 存储一个字符所需空间 一千字节 (KB) 一千字节 (One kilobyte) $10^3$ Bytes 一个小文本文件大小 一兆字节 (MB) 一兆字节 (One megabyte) $10^6$ Bytes 一张高清图片大小 一吉字节 (GB) 一吉字节 (One gigabyte) $10^9$ Bytes 一部电影大小 一太字节 (TB) 一太字节 (One terabyte) $10^{12}$ Bytes 个人电脑硬盘容量 一拍字节 (PB) 一拍字节 (One petabyte) $10^{15}$ Bytes 大型数据中心存储容量 Linux 内核代码行数 约三千万行 (Approximately thirty million lines) $3 \\times 10^7$ 行 TIOBE Index Windows 操作系统代码行数 约五千万行 (Approximately fifty million lines) $5 \\times 10^7$ 行 估算 某大型软件项目代码行数 约一百万行 (Approximately one million lines) $10^6$ 行 Information is Beautiful: Million Lines of Code GPT-2 Small 模型参数量 一点二五亿 (One hundred twenty-five million parameters) $1.25 \\times 10^8$ Information is Beautiful: The Rise of Generative AI GPT-3 模型参数量 一千七百五十亿 (One hundred seventy-five billion parameters) $1.75 \\times 10^{11}$ 某大型AI模型参数量 一万亿 (One trillion parameters) $10^{12}$ 一些示例： 一个 $GB$ ($10^9$ Bytes) 大约是一个 $KB$ ($10^3$ Bytes) 的 100 万倍。 Linux 内核代码行数 ($3 \\times 10^7$) 大约是某大型软件项目 ($10^6$) 的 30 倍。 GPT-3 模型参数量 ($1.75 \\times 10^{11}$) 是 GPT-2 Small 模型 ($1.25 \\times 10^8$) 参数量的约 1400 倍。 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:4:0","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["Instinct"],"content":"5. 总结：培养你的数字直觉 通过上面的例子，我们可以看到不同领域的数字跨度之大。从微观的原子尺度到宏观的宇宙尺度，从个人财富到国家经济，从一行代码到大型AI模型，数字无处不在，并以惊人的比例变化着。 培养数字直觉并非要求记住每一个精确的数字，而是要建立起对不同数量级的概念性理解。当你听到“十亿”时，脑海中能否浮现出它大约是多少个“百万”？当你看到“纳米”时，能否想象它与“微米”或“毫米”的相对大小？ 这种直觉能帮助我们更好地理解新闻报道、分析数据、评估风险，并在日常生活中做出更明智的决策。 数字尺度交互数轴 ","date":"2025-04-19","objectID":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/:5:0","tags":["CheatSheet"],"title":"数字直觉","uri":"/posts/%E6%95%B0%E5%AD%97%E7%9B%B4%E8%A7%89/"},{"categories":["AI"],"content":" Once upon a time, there was a mountain. 输入提示 ➜ Once upon a time, there was a mountain. 生成结果 ➜ Once upon a time, there was a mountain. In the sky, there was a big, round ball. A little boy named Tim and his mom went to the park to play. The sun was shining, and the sky was blue. In the park, Tim saw a big tree. He wanted to play near it. His mom said, “Okay, but be careful.” Tim was happy. He played near the tree. He got up and down. While playing, Tim saw a small bird. The bird was sad. Tim said, “The bird has a hurt wing. Can you help me?” The bird said, “Yes, I can help you.” Tim took the bird to his mom. He gave her a magic rock. The bird said, “Thank you, Tim. You are a good friend.” Now, Tim and the bird were friends. They played together every day. ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:0:0","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Intro 自从 2022 年 ChatGPT 发布后，LLM 开始席卷世界，俨然有一次新的科技浪潮之势。大模型资本市场以每年近 40% 的速度狂飙突进，在 2025 年初已接近 77 亿美元，且预计在 2034 年可达千亿级别，与之相比，较为成熟但应用领域略窄的的计算机视觉市场规模也有 250 亿美元。 资本市场的火热与否在于应用的前景，而大模型的应用前景也显然十分广阔。现在许多 app 已接入 AI， 增强功能，而 RAG, Agent 等技术也正在迅速发展，更何况未来还有大模型辅助智能制造的广阔的场景。 应用前景引来资本投资，资本投资引起硬件，算法，以及系统的全方面发展。在摩尔定律失效之后，领域专用硬件成为了提升计算力的绝妙法门。英伟达的 GPU 针对训练和推理做了许多优化，华为，Google 也推出了许多深度学习专用计算设备。算法层面，模型结构和参数量也都在不断演化。而系统层面，也不断有新技术和新项目来适应新的模型和硬件。在这几者的共同协作下，即使科技树点偏了，其探索之深也足以改变世界了。 在这个背景下，无论是出于好奇还是功利，学习大模型，理解其基本原理就显得有必要了。因此，我参考cs336课程,，试着 build a transformer-language model from scratch。在过程中，我也大量参考了 Umar Jamil 的 live coding 以及李宏毅的机器学习课程。 最终我使用 TinyStory 数据集，在一张 4090 显卡上花费一个半小时训练出了一个讲故事模型，且称之为 StoryLM。文章开头的故事续写就由 StoryLM 生成。 这篇文章介绍该模型构建的全过程，并解释相关概念。目标读者是有一些深度学习基础的计算机专业学生（也就是年初的我）。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:1:0","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Architecture 首先，从宏观上来介绍大模型的结构以及其生命周期。 就像任何深度学习模型一样，大模型的训练离不开模型结构，优化器，损失函数三个组成部分。优化器一般为常见的 AdamW，损失函数为分类任务常用的交叉熵，而几乎所有大模型的结构都基于 Transformer。Transformer 可分为编码器和解码器两部分，最初用于语言翻译等任务。编码器理解源语言，自回归解码器生成目标语言。而在生成式语言模型中，我们仅仅保留解码器即可。 在实际训练中，为了防止过拟合，梯度爆炸/消失，训练难以进行三个问题，会对上面三个组成部分做诸多调整 除了上面三者之外，训练数据也需要做一些处理。具体而言，我们需要通过 Tokenizer 加上词嵌入把句子变为模型能够理解的形式。具体可见 Tokenizer 和 Model 两小节。简而言之，一个句子会被切分成许多 token，每个 token 会被转化为一个词向量。输入一个句子后，模型会输出一个表示接在该句子后可能的 token 的概率分布。 StoryLM 的训练数据只有2GB，而真实世界大模型的训练数据动辄数TB，为了爬取并清洗得到这些训练数据，真实的 LLM system 也包含了许多数据处理相关技术。本文暂时不涉及。 同时 StoryLM 的参数量仅为 22M，而真实的大模型参数量均为 Billion 以上，因此其训练和推理过程的性能优化也是必不可少的。我会在后续文章介绍相关优化。 最后，通过大量语料进行的自监督训练只是预训练，得到的是基座模型，只具有续写能力。为了使模型更像语言助手，还需要进行监督微调与 RLHF。不过我们现在只考虑训练基座模型。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:2:0","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Tokenizer ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:3:0","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Byte-Pair Encoding Tokenizer 的目的是把句子转化为 Token 序列，每个 Token 都可以用一个数字表示，以便于模型处理。一个最简单的想法是 word-level tokenizer，但是 word 太多会导致词汇表过大，降低计算效率。而且会将如 “played” “playing” 编码为完全不同的两个Token，没能考虑到两者之间关系。因此，就有了第二个想法—— byte-level tokenizer：既然所有语言在计算机中的表示都是字节序列，那么我们把字节序列送给模型就好了！但是它也有一个问题：这种编码方式有太多冗余，word level 中的一个 Token 可能在 byte level 中就对应了好几个 Token，这样也会降低计算效率。 而 Byte Pair Encoding 是一种折中的算法：首先基于 byte-level tokenizer，这样就可以使用很小的词汇表处理各种语言了，但同时为了防止 Token 数目过多，BPE 会不断将最常见的 Token Pair 合并为一个新的 Token，直到达到预设的词汇表数目上限，以压缩 Token 长度。 在实践中，BEP有训练和运行两个过程。训练阶段会学习到哪些 Token 可以合并，构建词汇表。而运行阶段接受一个句子，根据训练好的词汇表将其编码为 Token 数组。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:3:1","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"BPE Implementation Train BPE 首先考虑训练。训练时我们要不断识别最频繁出现的 Token-Pair 并将其合并。最简单的暴力算法就是每次合并时都扫描一遍，找到最出现最多的 Pair。假设 n 为语料长度，m 为词汇表大小上限。显然这个 O(mn) 的算法开销太大。 但是考虑到每合并一个 Pair 后，只有与这个 Pair 相邻的 Pair 数目才产生变化，我们可以这样设计程序： 数据结构： 将 Pair 与其对应 count 存在一个数据结构 PairCounter 中，我们可以获取频率最高的 Pair，并方便地进行增删查改 比如 存储子树最值的平衡搜索树 支持O(logn)删除的堆 延迟删除法（原堆+删除堆） 即时删除法（哈希表记录元素索引） 训练语料转化而来的 Token 链表，L 一个从 Pair 到其出现位置的 map，M 算法： 每次获取最频繁 Pair 增加到词汇表中 通过 M 修改 Token 链表 L 通过 Token 链表 L 信息更新 PairCounter 和 M 但考虑到大部分语言中常见单词只为几千个，如果忽略跨词 Pair，我们就可以考虑把 Token 链表 L 替换为一个 从单词到该单词 Token 链的 map 加上一个从单词到该单词在训练集中出现数目的map。这样，并把 M 修改 从 Pair 到其出现的单词的 map。这样，又能大大降低实际计算的开销。 这样，假设k1为最大 Pair 数目，k2为最大单词数目，该算法就可以在O(n + mlogk1 + mk2)的时间内完成训练。 此外，这个算法比较复杂，实现时要注意： 及时把大函数拆分为小函数，方便理解 采用 OOP 的设计方法，把数据结构和对其操作的方法结合起来 Run BPE 训练完成后，实际运行时，我们要编码的语料更多，因此更要考虑性能问题。 首先是算法上的性能。 假设语料长度为n，词汇表长度为m，简单的暴力算法时间复杂度显然为 O(mn)。 考虑到我们在训练时，合并的 Pair 都处于一个单词内，所以可以这样设计程序： 数据结构： 一个代表原语料的单词 ID List 一个从单词 ID 到该单词 Token 链的 map，M 一个从 Pair 到其出现的单词的 map 算法： 仿照训练过程，初始化这些数据结构 根据训练过程得到的词汇表，不断进行 Pair 合并 根据单词 ID List 和 M 得到最终编码结果 假设k为最大单词数目，则时间复杂度为 O(n + mk) 其次是工程上的性能。 考虑到 TinyStory 语料文件在 GB 级别，我们没办法一次把所有内容都放在内存中，因此只能process incrementally, save incrementally。 考虑到编码时会在词汇表中指定一个分隔符，而 Pair 是不能跨分隔符的。我们就可以每次从源文件中读取一个 chunk，找到距末尾最近的分隔符，并将之后的内容放进 buffer。然后处理该 chunk 并写入目标文件。处理第二个 chunk 前先在它之前加上 buffer 中内容。 def encode_file(self, file_path: str, out_path: str): with open(file_path, \"r\") as f: encoded = [] i = 0 while True: chunk = f.read(self.chunk_size) if not chunk: break i += 1 encoded_chunk = self._process_chunk(chunk) encoded.extend(encoded_chunk) # 增量保存（每处理100个分块写入一次） if len(encoded) \u003e 100*self.chunk_size: self._save_incrementally(encoded, out_path) encoded = [] # 处理最终缓冲区的剩余数据 if self.buffer: final_tokens = self._process_chunk(\"\") encoded.extend(final_tokens) self._save_incrementally(encoded, out_path) ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:3:2","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Model 当 Tokenizer 把输入转化为 int 数组后，我们就可以直接把该数组输入模型，并得到 logit 了。 下图是 TinyStory 的模型结构： 下面是其代码实现： class TransformerLM(nn.Module): def __init__( self, vocab_size: int, context_length: int, d_model: int, num_layers: int, num_heads: int, d_ff: int, attn_pdrop: float, residual_pdrop: float, **kwargs ): super().__init__() self.embed = nn.Embedding(vocab_size, d_model) self.pos_embed = nn.Parameter(torch.zeros(context_length, d_model)) self.drop = nn.Dropout(residual_pdrop) self.layers = nn.Sequential(*[TransformerBlock(d_model, num_heads, d_ff, attn_pdrop, residual_pdrop) for _ in range(num_layers)]) self.ln_f = RMSNorm(d_model) self.head = nn.Linear(d_model, vocab_size, bias=False) self.context_length = context_length def forward(self, x): # x (batch_size, context_length) x = self.embed(x) x = x + self.pos_embed[:x.size(1), :] x = self.drop(x) x = self.layers(x) x = self.ln_f(x) x = self.head(x) return x 可以看到，模型基本由 Embedding，Attention，FFN，Linear 以及负责把函数变平滑的 Norm，Dropout 和 Residual Connection 组成。不同的大模型结构也大体类似。 Embedding 层负责生成词向量，TransformerBlock 是主要计算发生的地方，而最后的 Linear 层负责生成下一个 Token 的概率分布。 下面分别介绍每个层。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:4:0","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Positional Embedding 这个 layer 负责将 Token 转化为词向量（Word Vector），并在附加上位置信息。 词向量处于一个高维语义空间中，两个向量间的位置关系即可表示他们的语义关系。如 V(法国)-V(巴黎) = V(德国)-V(柏林)。 具体的词向量信息我们在神经网络中学习即可。我们首先要做的是把 Token 转化为高维向量。这个功能由 nn.Embeding 实现。该层会维护一个形状为 (num_embeddings, embedding_dim) 的查找表，把每一个 Token 都映射为高维向量。具体的参数在梯度下降时更新。 除了词嵌入外，我们还需要记录 Token 的位置信息，这个功能由 Positional Embedding 完成。 Positional Embedding 层会给每个不同位置的词都生成一个表示位置的向量，该向量与 Token Embedding 生成的向量相加，得到最终的词向量。 Positional Embedding 存在多种实现方法，其中旋转位置编码（RoPE）已被证明效果最优。但鉴于其实现复杂度较高，我们选择采用基础的 Absolute Positional Embeddings。具体实现方式为：创建一个可训练的位置嵌入矩阵，其行数设置为预定义的上下文最大长度（context_length）。在模型前向传播时，根据实际输入序列长度对矩阵进行动态截取，并与词嵌入向量进行逐元素相加。其优点是位置编码可以由学习得到，效果好，缺点是不能处理长度超出 context_length 的句子。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:4:1","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"TransformerBlock 上图是一个简化版的 Transformer，其核心组件由两个，Self-Attention 和 FFN。Attention 负责捕捉序列信息，将单独的词向量变为语境中的词向量（真正的能指）；而 FFN 负责存储知识信息，是整个 LM 中参数量最多的地方，也是训练推理时计算量最大的地方。 Attention 对于变长序列信息，我们需要有一种办法来考虑序列信息。在 attention 之前，我们一般使用 RNN 来完成这点。RNN 会引入隐状态，每个 Token 的计算都会依赖前一个 Token 的隐状态，并重新写入隐状态，以此来传递序列信息。 但是因为隐状态的相邻依赖关系，RNN 难以并行化，且难以捕捉长距离依赖，因此就有了 self-attention。最初的 attention 的在 RNN 语言模型的 input 序列和 output 序列间传递信息，但是 Transformer 的研究者发现让序列自己做 self-attention，就可以取代 RNN 了。 虽然 RNN 有着上述缺点，但是因为其恒定推理成本和在短序列中的低成本，也被广泛使用。 在 attention 机制中，我们有 Q, K, V 三个可学习矩阵。每个词向量分别乘着三个矩阵得到 q，k，v 三个向量。每个词的 q 向量和其他词的 k 向量先点积再做 softmax，得到注意力分数。再根据注意力分数对所有 v 做加权和，得到 attention 值。此时的 attention 值就包括了句中其他词的信息。 上述过程写成矩阵的形式如下： 下面是图示： 但在 decoder-only LM 中，我们要预测下一个 Token，因此计算 attention 时，一个 Token 的 q 只能于它之前的 Token 的 k 做计算，否则在训练时就提前泄露了未来信息。所以我们在计算注意力分数时要加上掩码，不需要的分数设置为一个极小值，这样 softmax 后其概率就接近为 0 了。 一个 attention 头（即一组 Q,K,V）可以学习一种关联模式，我们可以使用多组 attention 头来实现多头注意力机制（Multi-Head Attention）。简而言之，多头注意力就是使用多个 Q,K,V计算多个 attention 值，然后把所有的 attention 值连在一起，经过一个 Linear 层得到最后的 attention。 在实际实现中，多个 Q K V 可以拼成一个矩阵，一起运算，这样就减少了计算开销。 def scaled_dot_product_attention(K, Q, V, mask=None, pdrop=None): # K, Q (batch_size, [num_heads,] seq_len, k_dim) # V (batch_size, [num_heads,] seq_len, v_dim) # attn_scores (batch_size, [num_heads,] seq_len, seq_len) attn_scores = torch.matmul(Q, K.transpose(-2, -1)) attn_scores = attn_scores / math.sqrt(K.shape[-1]) if mask is not None: attn_scores = attn_scores.masked_fill(mask, -1e9) attn_scores = softmax(attn_scores, dim=-1) attn_scores = attn_scores.masked_fill(mask, 0.0) else: attn_scores = softmax(attn_scores, dim=-1) if pdrop is not None: attn_scores = nn.Dropout(pdrop)(attn_scores) return torch.matmul(attn_scores, V) class MultiHeadSelfAttention(nn.Module): def __init__(self, d_model: int, num_heads: int, dropout: Optional[float] = None): super().__init__() assert d_model % num_heads == 0 self.num_heads = num_heads self.d_model = d_model self.d_k = d_model // num_heads self.w_q = nn.Linear(d_model, d_model, bias=False) self.w_k = nn.Linear(d_model, d_model, bias=False) self.w_v = nn.Linear(d_model, d_model, bias=False) self.w_o = nn.Linear(d_model, d_model, bias=False) self.dropout = dropout def forward( self, in_features: torch.FloatTensor, ): Q = self.w_q(in_features) K = self.w_k(in_features) V = self.w_v(in_features) batch_size, seq_len, _ = Q.size() Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) mask = torch.triu(torch.ones(seq_len, seq_len, device=K.device), diagonal=1).bool() attn_scores = scaled_dot_product_attention(K, Q, V, mask, self.dropout) attn_scores = attn_scores.transpose(1, 2).contiguous().view(batch_size, seq_len, -1) return self.w_o(attn_scores) FFN FFN 层是 Transformer 中计算最多的地方。这里可以控制的变量有： 隐藏层的维度 激活函数的选择 class PointwiseFeedForward(nn.Module): def __init__(self, d_model: int, d_ff: int): super().__init__() self.w1 = nn.Linear(d_model, d_ff, bias=False) self.w2 = nn.Linear(d_ff, d_model, bias=False) def forward(self, x): return self.w2(gelu(self.w1(x))) 此外，现在许多 LLM 在 FFN 层采用 MoE 架构。关键在于，把 FFN 拆成许多小 FFN，运行时选择一个 FFN （专家/export）传播。这样就能保证模型参数的同时提升推理速度。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:4:2","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Others 模型剩下的组件还包括，RMSNorm，Residual Connection，以及 Dropout。 RMSNorm 和 Residual Connection 都可以避免梯度消失和梯度爆炸，保证函数的 Lipschitz 连续性质。 而 Dropout 属于一种正则化方法，通过随机将某些参数置为 0，来避免模型过分依赖某些参数，从而保证了其泛化性。 此外，模型参数的初始化也十分重要。有许多实验表示初始化的好坏对于模型训练过程有很大影响，并且训练结果与初始化偏差并不大。通过合理的初始化（比如对于方差，均值的调整），我们可以使得函数更易于训练。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:4:3","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Train 深度学习除了作为 hypothesis 的模型结构，损失函数和优化方法也是训练时不可或缺的组成部分。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:5:0","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Cross-entropy loss 在熵的公式中，熵越大，分布越均匀（不确定性越高）；熵越小，分布越集中（确定性越高）。 交叉熵衡量两个概率分布之间的差异，对于真实分布 p 和模型预测分布 q，交叉熵的公式为： 若 q(x) 与 p(x) 完全一致，则交叉熵等于熵 H(p)，否则 H(p,q)\u003eH(p)。 在训练中 p(x) 一般为真实标签的 one-hot 分布（仅正确类别为1，其他为0），因此只用考虑正确类别。因此，LLM 中使用的交叉熵公式如下： 对于 one-hot 分布，熵为0，因此交叉熵的最小值也为0。 在训练语言模型时，假设初始状态为均匀分布，收敛状态预测正确 token 概率为 0.9，则交叉熵在区间 [0.1,10.82] 内。 代码实现如下： def cross_entropy_loss(inputs, target): max_vals = torch.max(inputs, dim=-1, keepdim=True).values shifted_inputs = inputs - max_vals exp_inputs = torch.exp(shifted_inputs) exp_inputs_sum = torch.sum(exp_inputs, dim=-1, keepdim=True) log_softmax_inputs = shifted_inputs - torch.log(exp_inputs_sum) loss = -log_softmax_inputs[torch.arange(target.shape[0]), target] return torch.mean(loss) ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:5:1","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"AdamW 神经网络最经典的优化算法为随机梯度下降法。AdamW 在此基础上增加了： 一阶矩 动量 m：可以减小随机batch带来的震荡 二阶矩 自适应学习率 v：不同参数的梯度数值大小不同，通过除二阶矩的开放来使得梯度稳定，避免大梯度爆炸，小梯度消失 第四步对学习率的调整实际上是为了应对一开始 m 和 v 太小的情况 weight decay：减去参数，增强泛化性 L2 正则中，正则项梯度会被自适应学习率缩放，导致实际衰减强度与学习率耦合，影响正则化效果。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:5:2","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Learning rate scheduling 学习率调整的核心作用有： 加速初期收敛：训练初期使用较大学习率，快速逼近最优解区域。 避免后期震荡：后期逐步降低学习率，防止在最优解附近震荡。 逃离局部极小值：周期性或突变的调整策略（如重启）可帮助跳出局部最优。 通过余弦函数平滑地将学习率从初始值（ηmax）降低到最小值（ηmin），形成一个周期性的退火过程。其特点是平滑过渡，避免学习率突变导致的训练不稳定。 代码实现如下： def get_lr_cosine_schedule(t: int, alpha_max: float, alpha_min: float, Tw: int, Tc: int) -\u003e float: if t \u003c Tw and Tw \u003e 0: return alpha_max * (t / Tw) if Tw \u003c= t \u003c= Tc: ratio = (t - Tw) / (Tc - Tw) # Progress through cosine phase cosine_decay = 0.5 * (1 + math.cos(math.pi * ratio)) return alpha_min + (alpha_max - alpha_min) * cosine_decay return alpha_min ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:5:3","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Gradient clipping 梯度裁剪用于控制梯度的大小，防止训练过程中因梯度爆炸导致的数值不稳定。它通过限制梯度的最大值或范数，确保参数更新步长合理，从而提升模型训练的稳定性和收敛性。 代码实现如下： def clip_gradient(parameters, max_norm: float, eps: float = 1e-6): grads = [p.grad for p in parameters if p.grad is not None] if not grads: return device = grads[0].device total_norm = torch.norm( torch.stack([torch.norm(g.detach(), 2) for g in grads]), 2 ).to(device) scaling_factor = max_norm / (total_norm + eps) if total_norm \u003e max_norm: for grad in grads: grad.mul_(scaling_factor) ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:5:4","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Resource accounting 一个简单的资源估算： 基本想法：考虑 Y = WX，反向传播时需要分别对 W 和 X 求导，因此反向传播计算量约为正向传播的两倍。 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:5:5","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Train in Practice 把上面所有组件组合在一起后，就可以进行训练了。 实际训练时的参数配置如下： { // 模型超参数 \"vocab_size\": 10000, \"context_length\": 256, \"d_model\": 512, \"num_layers\": 4, \"num_heads\": 16, \"d_ff\": 2048, \"attn_pdrop\": 0.1, \"residual_pdrop\": 0.1, // 优化器超参数 \"learning_rate\": 0.001, \"beta1\": 0.9, \"beta2\": 0.95, \"epsilon\": 1e-8, \"weight_decay\": 0.01, // 训练过程超参 \"batch_size\": 64, \"epochs\": 10, // 其他训练相关参数 \"save_every\": 1000, \"save_path\": \"checkpoints/\", \"train_data\": \"valid.npy\", \"valid_data\": \"valid.npy\", \"result_model\": \"model/model.pth\", \"log_path\": \"logs/log.txt\" } 实际训练时，在一张 4090 显卡上运行了 1h30min。 训练时出了许多 bug（比如奇奇怪怪的 cuda 报错），checkpoint 非常实用。 下图是 loss 曲线，可以看到在 10000 个迭代后 loss 曲线就不怎动了。最终 loss 为 0.9105。 可能因为每隔 100 个迭代才记录一次学习率，所以从图中看不出周期性 同时，Tc 可能设置过小，导致后期 loss 降不下去 ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:5:6","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["AI"],"content":"Run 模型训练好之后，就是推理了！ 当输入一个句子时，模型会生成表示下一个Token可能性的logits值。这些logits会除以称为Temperature的超参数——Temperature值越大，模型选择低频词的概率越高。之后我们筛选出概率最高的前k个候选Token（top-k sampling），通过softmax函数将筛选后的logits转换为概率分布，然后依据该概率分布随机选取一个Token作为输出。 之后，我们将新生成的Token追加至输入序列末尾，重新输入模型进行下一轮预测，循环直至出现终止符（如）或达到最大生成长度。 文本生成中，也常常使用 beam search 来选择 Token ","date":"2025-04-06","objectID":"/posts/building-a-transformer-lm/:6:0","tags":["LLM","Code"],"title":"Building a Transformer LM","uri":"/posts/building-a-transformer-lm/"},{"categories":["Backend","System"],"content":"在后端实习面试中，有 数据库，Java，网络，OS 四大考点，其中又以数据库考察最为频繁。数据库问题中，除了少部分范式相关理论，基本为 MySQL 问题。因此，在这里摘录相关知识点，做一个总结，既是为秋招做准备，也是总结找实习过程中的所学知识。 b 站视频解析 架构 MySQL 整体可分为 Server 层 和 存储引擎 层。Server 层处理客户连接，接收 SQL 语句，生成执行计划，并调用存储引擎 API 来获取 / 写入具体数据。每一个客户端连接都由一个线程来负责处理。每个 table 都可以设置不同的存储引擎。具体流程 存储引擎自定义实现数据的物理，逻辑组织以及索引，事务等特性。（因此面试考察的问题几乎都属于存储引擎层）抽象存储引擎层 api 是通过抽象类 handler 来实现。a minimal sorage engine in c++ The current limitations of the storage API are not intrinsic aspects of MySQL/MariaDB or Postgres’s design. For both project there used to be no pluggable storage at all. We can imagine a future patch to either project that allows support for batched row reads and writes that together could make column-wise storage and vectorized execution more feasible. MySQL 采用插件式存储引擎，我们可以根据使用场景 选择合适的。最常用的也是默认的存储引擎为 InnoDB，它是唯一满足 ACID 特性事务的存储引擎。 Maybe you want an in-memory storage layer so that you can quickly run integration tests. Maybe you want to switch between B-Trees (read-optimized) and LSM Trees (write-optimized) and unordered heaps (write-optimized) depending on your workload. Or maybe you just want to try a third-party storage library (e.g. RocksDB or Sled or TiKV). 索引 B-tree 资源 **在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表 **。又 InnoDB 使用了 **B+ 树索引模型 **，所以数据都是存储在 B+ 树中的。每一个索引在 InnoDB 里面对应一棵 B+ 树。 聚簇索引是主键所在的索引，数据也存在其中。 二级索引是初次之外的索引，查询后得到主键，其他信息需要到主索引中 “回表查询”。 除此之外，也可以对多个键进行联合索引。联合索引的建立应该遵循最左前缀原则。 而如果我们要查询的数据已经在索引中，避免了回表查询。这个索引就叫做覆盖索引。 事务 事务的执行流程： 事务指满足 ACID 特性的一组操作。 一致性：数据库处于正确的状态 原子性：或者成功，或者失败，没有中间状态 隔离性：并发事务相互不影响 持久性：断电内存数据丢失后可恢复 ","date":"2025-03-12","objectID":"/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/:0:0","tags":["Database","Interview","Summary"],"title":"MySQL 知识点总结 ","uri":"/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"},{"categories":["Backend","System"],"content":"实现原子性 -\u003e undo log 每一条 SQL 通过执行器后，会在 undo log segment 中申请一个 undo log 页。然后根据 SQL 信息构造 undo log 内容，并写入磁盘，保证真正操作之前 undo log 完整。 MySQL 采用 SEGMENT -\u003e PAGE -\u003e 具体记录 来管理内存 每当 InnoDB 中需要修改某个 Record 时，都会将其历史版本写入一个 Undo Log 中，对应的 Undo Record 是 Update 类型。当插入新的 Record 时，还没有一个历史版本，但为了方便事务回滚时做逆向（Delete）操作，还是会写入一个 Insert 类型的 Undo Record。 Insert Undo Record 仅仅是为了可能的事务回滚准备的，并不在 MVCC 功能中承担作用。因此只需要记录对应 Record 的 Key，供回滚时查找 Record 位置即可。 其中 Undo Number 是 Undo 的一个递增编号，Table ID 用来表示是哪张表的修改。下面一组 Key Fields 的长度不定，因为对应表的主键可能由多个 field 组成，这里需要记录 Record 完整的主键信息，回滚的时候可以通过这个信息在索引中定位到对应的 Record。除此之外，在 Undo Record 的头尾还各留了两个字节用户记录其前序和后继 Undo Record 的位置。 由于 MVCC 需要保留 Record 的多个历史版本，当某个 Record 的历史版本还在被使用时，这个 Record 是不能被真正的删除的。因此，当需要删除时，其实只是修改对应 Record 的 Delete Mark 标记。对应的，如果这时这个 Record 又重新插入，其实也只是修改一下 Delete Mark 标记，也就是将这两种情况的 delete 和 insert 转变成了 update 操作。再加上常规的 Record 修改，因此这里的 Update Undo Record 会对应三种 Type：TRX_UNDO_UPD_EXIST_REC、TRX_UNDO_DEL_MARK_REC 和 TRX_UNDO_UPD_DEL_REC。他们的存储内容也类似： 除了跟 Insert Undo Record 相同的头尾信息，以及主键 Key Fileds 之外，Update Undo Record 增加了： Transaction Id 记录了产生这个历史版本事务 Id，用作后续 MVCC 中的版本可见性判断 Rollptr 指向的是该记录的上一个版本的位置，包括 space number，page number 和 page 内的 offset。沿着 Rollptr 可以找到一个 Record 的所有历史版本。 Update Fields 中记录的就是当前这个 Record 版本相对于其之后的一次修改的 Delta 信息，包括所有被修改的 Field 的编号，长度和历史值。 每个事务其实会修改一组的 Record，对应的也就会产生一组 Undo Record，这些 Undo Record 收尾相连就组成了这个事务的 Undo Log。除了一个个的 Undo Record 之外，还在开头增加了一个 Undo Log Header 来记录一些必要的控制信息，因此，一个 Undo Log 的结构如下所示： 索引中的同一个 Record 被不同事务修改，会产生不同的历史版本，这些历史版本又通过 Rollptr 穿成一个链表，供 MVCC 使用。如下图所示： insert undo log 在事务结束后即可回收，但 update undo log 需要支持 MVCC，不能直接删除。当某个历史版本已经确认不会被任何现有的和未来的事务看到的时候，就应该被清理掉。 ","date":"2025-03-12","objectID":"/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/:1:0","tags":["Database","Interview","Summary"],"title":"MySQL 知识点总结 ","uri":"/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"},{"categories":["Backend","System"],"content":"实现隔离性 -\u003e MVCC 和锁 如果并发事务间不隔离就会产生以下问题： 脏读：A 事务读取 B 事务未提交的数据 不可重复读：A 事务两次读取到不同的数据（B 事务修改且提交） 幻读：A 事务两次读取到数量不同的行数据（B 事务删除 / 增加行） InnoDB 中实现了四大隔离级别： 隔离级别 脏读 不可重复读 幻读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × Innodb 通过锁机制和 MVCC 来实现各种隔离级别，其中锁又分为： LOCK_S 共享锁 LOCK_X 独占锁 LOCK_IS 共享意向锁（意向锁：在表上做一个标记） LOCK_IX 独占意向锁 LOCK_AUTO_INC 自增锁 按照粒度又分为 LOCK_TABLE 表锁，和 LOCK_REC 行锁。 行锁又分为： LOCK_REC_NOT_GAP 精准行锁，锁住某一行 LOCK_GAP 锁住某两行间隙，防止插入 LOCK_ORDINARY next-key 锁 = LOCK_GAP + LOCK_REC_NOT_GAP LOCK_INSERT_INTENTION 插入意向锁，共享的 gap 锁 如果要加多个锁，会强制使用 两段锁协议 以保证可串行化。 通过强制锁的申请和释放顺序，两段锁协议确保事务间的冲突操作（如读写、写写）遵循固定顺序，从而避免因操作交叉导致的数据不一致。例如，若事务 T1 和 T2 均遵守两段锁协议，它们的操作顺序会被锁定为 “T1 全加锁→T1 释放锁→T2 加锁”，等效于串行执行。 MVCC 具体通过 undo log 和 read view 实现。undo log 存储了所有变更。一个事务的 read view 记录了当前活跃事务集合，最小事务 id，下一个事务 id，当前事务 id。读取时，沿着版本链，找到合适的版本。 对于读未提交级别： 读取时不做任何加锁处理，直接访问内存中的最新数据页版本。 更新时加 独占行锁，事务结束时释放。（其他事务不可以修改，但可读取） 对于读已体提交： 每次查询构造一个新的 readview，解决脏读 更新时加 独占行锁，事务结束时释放。 对于可重复读： 使用第一次查询时生成的 readview，解决脏读和不可重复读 更新数据时加 next-key 锁，解决了部分幻读问题 对于串行读： 读取时使用共享表锁 更新时使用独占表锁 ","date":"2025-03-12","objectID":"/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/:2:0","tags":["Database","Interview","Summary"],"title":"MySQL 知识点总结 ","uri":"/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"},{"categories":["Backend","System"],"content":"实现持久性 -\u003e redo log，binlog redo log 是一种 WAL（Write-Ahead Logging）。当有一条记录需要更新时，InnoDB 引擎就会先把记录写到 redo log 里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面。（注意 undo 页可能一直在 buffer pool 中，从不实际写入磁盘） 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。 那么我们需要什么样的 REDO 呢？首先，REDO 的维护增加了一份写盘数据，同时为了保证数据正确，事务只有在他的 REDO 全部落盘才能返回用户成功，REDO 的写盘时间会直接影响系统吞吐，显而易见，**REDO 的数据量要尽量少 **。其次，系统崩溃总是发生在始料未及的时候，当重启重放 REDO 时，系统并不知道哪些 REDO 对应的 Page 已经落盘，因此 REDO 的重放必须可重入，即 **REDO 操作要保证幂等 。最后，为了便于通过并发重放的方式加快重启恢复速度，REDO 应该是 ** 基于 Page 的，即一个 REDO 只涉及一个 Page 的修改。 数据量小是 Logical Logging 的优点，而幂等以及基于 Page 正是 Physical Logging 的优点，因此 InnoDB 采取了一种称为 Physiological Logging 的方式，来兼得二者的优势。所谓 Physiological Logging，就是以 Page 为单位，但在 Page 内以逻辑的方式记录。 举个例子，MLOG_REC_UPDATE_IN_PLACE 类型的 REDO 中记录了对 Page 中一个 Record 的修改，方法如下： （Page ID，Record Offset，(Filed 1, Value 1) … (Filed i, Value i) … ) 其中，PageID 指定要操作的 Page 页，Record Offset 记录了 Record 在 Page 内的偏移位置，后面的 Field 数组，记录了需要修改的 Field 以及修改后的 Value。 MySQL 的 InnoDB 页大小为 16KB，而操作系统（如 Linux）的页大小为 4KB。InnoDB 将一个 16KB 的页写入磁盘时，需要拆分为 4 个操作系统页（4KB×4）。若在写入过程中发生崩溃（如断电），可能出现仅部分操作系统页成功写入的情况，导致 InnoDB 页数据损坏。这种损坏无法通过 Redo Log 恢复，因为 **Redo Log 记录的是操作而非完整页数据 **。 InnoDB 中采用了 Double Write Buffer 的方式来通过写两次的方式保证恢复的时候找到一个正确的 Page 状态。它的作用是，在把页写到数据文件之前，InnoDB 先把它们写到一个叫 Double Write Buffer（双写缓冲区）的共享表空间内，在写 Double Write Buffer 完成后，InnoDB 才会把页写到数据文件的适当的位置。如果在写页的过程中发生意外崩溃，InnoDB 在稍后的恢复过程中在 Double Write Buffer 中找到完好的 page 副本用于恢复。 Double Write Buffer 能够保证找到一个正确的 Page 状态，我们还需要知道这个状态对应 REDO 上的哪个记录，来避免对 Page 的重复修改。为此，InnoDB 给每个 REDO 记录一个全局唯一递增的标号 LSN (Log Sequence Number)。Page 在修改时，会将对应的 REDO 记录的 LSN 记录在 Page 上（FIL_PAGE_LSN 字段），这样恢复重放 REDO 时，就可以来判断跳过已经应用的 REDO，从而实现重放的幂等。 MySQL 中使用 binlog 做 主从库间的备份。 最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统 —— 也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是 “在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如 “给 ID=2 这一行的 c 字段加 1”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写” 是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 MySQL 采用两阶段提交(2PC) 机制，是为了解决 redo log（重做日志） 和 binlog（归档日志） 之间的逻辑一致性问题，确保事务的原子性和持久性。 ","date":"2025-03-12","objectID":"/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/:3:0","tags":["Database","Interview","Summary"],"title":"MySQL 知识点总结 ","uri":"/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":" cheat sheet Machine Learning Glossary ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:0:0","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"基本概念 什么是机器学习？ 机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。 经验通常以\"数据“形式存在，因此机器学习研究的主要内容，是关于在计算机上从数据中产生”模型\"(model) 的算法，即\"学习算法\" (learning algorithm). 记录的集合称为一个\"数据集\" (data set) ，其中每条记录是关于一个事件或对象的描述，称为一个\"示例\"(instance) 或\"样本\"(sample). 反映事件或对象在某方面的表现或性质的事项称为\"属性\"(attribute) 或\"特征\"(feature); 属性上的取值称为\"属性值\" (attribute value). 属性张成的空间称为\"属性空间\" (attribute space) 、\"样本空间\" (sample space)或\"输入空间\"。我们也把一个示例称为一个\"特征向量\" (feature vector).属性的数目被称为样本的\"维数\" (dimensionality). 从数据中学得模型的过程称为\"学习\"(learning)或\"训练\" (training),这个过程通过执行某个学习算法来完成.训练过程中使用的数据称为\"训练数据\" (training data) ，其中每个样本称为一个\"训练样本\" (training sample),训练样本组成的集合称为\"训练集\" (training set). 学得模型对应了关于数据的某种潜在的规律，因此亦称\"假设\" (hypothesis); 这种潜在规律自身，则称为\"真相“或”真实\"(ground-truth) ，学习过程就是为了找出或逼近真相. 学得模型后，使用其母行预测的过程称为\"测试\"(testing) ，被预测的样本 称为\"测试样本\" (testing sample). 都有哪些机器学习算法？ 一份算法列表 除如图所示的监督和非监督学习外，强化学习也是一类基本学习算法。 什么是深度学习 深度学习（Deep Learning）是机器学习的一个分支，是一种基于人工神经网络的算法模型。它模仿人类大脑处理信息的方式，采用多层神经网络从大量数据中提取特征并进行决策。深度学习在监督，非监督，强化学习中均有应用。 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:1:0","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"对学习的理解 我们可以把学习过程看作一个在所有假设 (hypothesis)组成的空间中进行搜索的过程，搜索目标是找到与训练集\"匹配\"(fit) 的假设，即能够将训练集中的瓜判断正确的假设.假设的表示一旦确定，假设空间及其规模大小就确定了. 可能有多个假设与训练集一致，即存在着一个与训练集一致的\"假设集合\"，我们称之为\"版本空间\" (version space). 机器学习算法在学习过程中对某种类型假设的偏好，称为\"归纳偏好\" (inductive bias) ,或简称为\"偏好\".任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上\"等效\"的假设所迷惑，而无法产生确定的学习结果. 归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或\"价值观\"。“奥卡姆剃刀” (Occam’s razor)是一种常用的、自然科学研究中最基本的原则，即\"若有多个假设与观察一致，则选最简单的那个。 事实上，归纳偏好对应了学习算法本身所做出的关于\"什么样的模型更好\"的假设.在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能. 不同的神经网络架构既体现了不同的假设空间，也体现了不同的归纳偏好 假设空间：由架构定义，是模型可以表示的所有可能函数。 理论上来说，一层全连接神经网络（具有足够的神经元和非线性激活函数）就能逼近任意连续函数。 归纳偏好：由架构及其优化过程（例如损失函数、正则化策略）定义，是模型在有限数据下更倾向选择的函数。 而同时，根据NFL 定理，没有任何一种学习算法在所有问题上都优于其他算法。 直观来说： 如果我们对问题分布一无所知，那么所有算法在这些问题上成功的概率是等价的。 要在特定任务中取得好性能，算法必须对任务的特定性质做出假设（即引入归纳偏好）。 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:2:0","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"模型评估和选择 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:3:0","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"经验误差与过拟合 在机器学习中，我们通常用以下指标来衡量模型的性能： 错误率（Error Rate）：分类错误的样本数占样本总数的比例。例如，如果在 $m$ 个样本中有 $\\alpha$ 个样本被分类错误，则： $\\text{错误率} = \\frac{\\alpha}{m}$ 精度（Accuracy）：分类正确的样本数占样本总数的比例，与错误率互补： $\\text{精度} = 1 - \\text{错误率}$ 更广义地，我们把学习器的实际预测输出与样本的真实输出之间的差异称为“误差（Error）”： 训练误差（Training Error）：学习器在训练集上的误差，又称为“经验误差（Empirical Error）”。 泛化误差（Generalization Error）：学习器在新样本上的误差。 我们的目标是构建一个泛化误差尽可能小的学习器。然而，由于事先无法得知新样本的分布，我们通常通过最小化经验误差来间接优化泛化误差。 在实践中，我们希望学习器能对新样本做出准确预测，这要求它从训练数据中学到能适用于所有潜在样本的“普遍规律”。但模型的学习能力可能出现两种极端情况： 过拟合（Overfitting）： 学习器对训练样本学得过于“精准”，甚至将训练样本中特有的噪声或不普遍的特性当成一般规律。这会导致模型对新样本的泛化性能下降。 欠拟合（Underfitting）： 学习器没有充分捕捉训练样本的一般规律，导致模型在训练集和测试集上的表现都不理想。 常见原因： 过拟合：学习能力过强，模型复杂度过高。 欠拟合：学习能力不足，模型复杂度过低。 过拟合是机器学习中的一个普遍现象，无法彻底避免，但可以通过以下方法缓解或减小其影响： 增加训练数据。 使用正则化（如 $L_1$ 或 $L_2$ 正则化）。 采用早停（Early Stopping）策略。 使用更简单的模型（降低复杂度）。 引入交叉验证（Cross-Validation）评估模型性能。 本质限制：过拟合无法完全避免，因为机器学习问题通常是 NP 难或更复杂的，而有效的学习算法必然是多项式时间内运行完成。要彻底避免过拟合，经验误差最小化就能找到最优解，而这在理论上等同于 $P=NP$（构造性地证明），目前尚未被证明。 许多学习问题（如神经网络权重优化、支持向量机中的核参数选择等）涉及非凸优化问题，或需要在一个离散的组合空间中搜索最佳解。这类问题通常被证明为 NP难。 在实际任务中，我们面临许多选择，包括： 不同的学习算法（如决策树、支持向量机、深度学习）。 同一算法的不同参数配置（如正则化强度、学习率、网络结构）。 这引出了模型选择（Model Selection）问题： 如何选择一个泛化性能最好的模型？ 理想目标：直接评估候选模型的泛化误差，选择泛化误差最小的模型。 现实困境：泛化误差无法直接获得，训练误差又可能因过拟合而无法准确反映模型性能。 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:3:1","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"评估方法 在机器学习中，模型的性能评估是关键的一环。为了衡量模型的泛化能力（即在新样本上的表现），我们通常通过实验测试来评估泛化误差。以下将介绍几种常用的数据集划分与评估方法，并探讨参数调节的技巧。 在评估模型时，需将原始数据集 $D$（包含 $m$ 个样本）划分为训练集 $S$ 和测试集 $T$，即 $D = S \\cup T$ 且 $S \\cap T = \\emptyset$。训练集用于训练模型，测试集用于评估模型在未见样本上的表现。测试误差（Testing Error）被视为泛化误差的近似，假设测试样本与训练样本是从相同分布中独立同分布抽取的。 1. 留出法（Hold-out Method） 留出法是最简单的评估方法，将数据集 $D$ 随机划分为训练集和测试集。例如，常用 2/3 至 4/5 的样本作为训练集，其余作为测试集。 注意事项： 数据划分需尽量保持分布一致性（如分类任务中保持类别比例），可通过分层采样（Stratified Sampling）实现。 单次划分的评估结果可能不稳定，因此需多次随机划分，取平均值作为最终评估结果。 权衡： 训练集比例过大：模型更接近使用全数据训练的情况，但测试集过小可能导致结果不稳定。 测试集比例过大：测试结果更稳定，但训练集与全数据差距较大，可能影响评估结果的保真性（Fidelity）。 2. 交叉验证法（Cross-validation） 交叉验证将数据集 $D$ 分为 $k$ 个大小相等的互斥子集： $D = D_1 \\cup D_2 \\cup … \\cup D_k$​，且 $D_i \\cap D_j = \\emptyset , (i \\neq j)$ 每次用 $k−1$ 个子集作为训练集，剩余一个子集作为测试集，重复 $k$ 次后取测试结果的均值作为最终评估结果。常见的 $k$ 值为 10，称为10 折交叉验证（10-fold Cross-validation）。 优点： 比留出法更稳定可靠。 可随机重复多次交叉验证（如 10 次 10 折交叉验证），进一步减少划分差异的影响。 特例：留一法（Leave-One-Out Cross-validation, LOO） 令 $k=m$，即每次仅用一个样本作为测试集，其余样本作为训练集。 LOO 不受随机划分影响，训练集与全数据集非常接近，但计算复杂度较高，尤其在样本量较大时。 在理论上，多次留出法的随机划分结果有可能与交叉验证法某次划分完全一致 3. 自助法（Bootstrapping） 自助法通过自助采样（Bootstrap Sampling）生成新的训练集。每次从原始数据集 $D$ 中随机抽取一个样本放入训练集，并将其放回，再次采样，重复 $m$ 次。采样后： $D′$ 为训练集，包含 $m$ 个样本，其中部分样本重复出现。 $D \\setminus D’$ 作为测试集，包含约 36.8% 的未被采样样本（概率计算如下）： $P(\\text{样本未被采样}) = (1 - \\frac{1}{m})^m \\approx e^{-1} \\approx 36.8%$ 特点： 适用于数据量较小或难以划分的场景。 可生成多个训练集，用于集成学习等方法。 局限： 改变了数据分布，可能引入估计偏差。 参数调节与最终模型选择 机器学习模型通常有多个参数需设定，不同参数配置会显著影响模型性能。调参（Parameter Tuning）是优化模型性能的重要步骤。 常见调参方法： 网格搜索（Grid Search）： 为每个参数设定取值范围和步长，逐一评估所有参数组合，选择性能最佳者。 计算复杂度较高，但可行性依赖于参数数量和候选值范围。 示例：3 个参数，每个参数有 5 个候选值，需训练 $5^3 = 125$ 个模型。 随机搜索（Random Search）： 随机采样参数组合，降低计算成本。 验证集（Validation Set）： 在训练数据中留出一部分作为验证集，用于模型评估和参数调节。 实际测试数据则保留用于最终测试。 常用的比例是训练集占 70%~80%，测试集占 20%~30%。再从训练集划分出验证集，常见比例是：训练集占原始数据的 60%~70%，验证集占 10%~20%。 验证集允许模型在开发过程中尝试不同的算法或参数设置，为模型选择和调参提供依据，而测试集不参与这个过程，以防止评估偏差。 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:3:2","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"神经网络 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:4:0","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"神经元模型 神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。神经网络中最基本的成分是神经元 (neuron)模型。 （M-P 神经元） ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:4:1","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"感知机和多层网络 感知机 (Perceptron) 由两层 神经元 组成，感知机能容易地实现逻辑与、或、非运算。 史一般的，常见的神经网络是形如图所示的层级结构，每层神经元与下 层神经元全互连。神经元之间不存在同层连接，也不存在跨层连接。这样 的 神经网络结构通常称为\" 多层前馈神经网络 \" (multi-layer feedforward neural networks) ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:4:2","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"反向传播算法 最常用的神经网络训练方法，详见： https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:4:3","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"深度学习训练技巧 深度学习训练中的变量： 数据集 模型结构（model bias） 超参数 epoch数目，batch大小等 loss function 优化算法 解决问题都得从这几个方面入手 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:5:0","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"应对欠拟合问题 优化过程中，神经网络的目标是最小化损失函数。然而，优化过程可能会遇到多种挑战，尤其是在高维空间中。 Adam算法结合了RMSProp和动量方法，是目前最常用的优化算法之一，能够有效提高训练速度，并避免梯度消失问题。最为常用。 1. 局部最优与鞍点 局部最优解：损失函数在某个局部区域内达到最小值，这可能导致优化算法在该点停滞，从而影响模型的训练。 鞍点：在高维空间中，鞍点比局部最优解更多。鞍点的梯度为零，但它既不是最小值也不是最大值，优化算法容易被卡在这种点上。 解决鞍点：可以通过计算Hessian矩阵来判断一个点是否为鞍点。训练过程中使用动量和自适应学习率等技术可以帮助避免被卡住。 2. 小批量与噪声 小批量的优势：小批量训练时，由于每个批次的样本较少，噪声较大，这种噪声有助于模型跳出局部最小值，避免陷入“峡谷”中。实验证明，小批量训练有助于提高模型的泛化能力，并且在测试集上的表现通常更好。 劣势：小批量训练需要更多的时间来完成一个epoch，因为每个batch的训练时间较长。 大批量的劣势：大批量训练时，由于噪声较小，模型更容易停留在局部最小值，从而导致训练效果不佳。 3. 自适应学习率 学习率是控制梯度下降步伐的重要超参数。过大的学习率可能导致优化过程不稳定，过小的学习率则可能导致收敛缓慢。 自适应学习率：使用Adagrad、RMSProp、Adam等自适应学习率算法，可以根据每个参数的梯度大小调整步伐，从而提高训练效率和收敛速度。 学习率调度：逐渐降低学习率（如通过学习率衰减或warm-up策略），能够避免震荡，帮助网络找到更精确的解。 4. 动量（Momentum） 动量方法通过结合当前的梯度和前一时刻的梯度信息，帮助网络在遇到局部最小值时继续向前推进，避免停滞在局部最小值。动量方法模仿物理中小球滚动的现象，在训练过程中具有较强的稳定性。 5. 批归一化（Batch Normalization） 批归一化通过对每层输入进行标准化，消除不同维度间的差异，使得网络训练更加稳定。 平坦化误差表面：批归一化能有效平滑损失函数，减少训练过程中出现不均匀的误差。 训练与测试的区别：在训练时，批归一化使用当前批次的数据进行标准化，而在测试时使用训练期间计算的均值和方差。 6. 激活函数的选择 激活函数对优化过程有着重要影响。不同的激活函数会决定网络的非线性能力，从而影响优化效果。 ReLU（Rectified Linear Unit）：ReLU是目前最常用的激活函数，能够有效避免梯度消失问题，并且计算效率高。 Sigmoid和Tanh：这些激活函数适用于浅层网络，但在深层网络中容易出现梯度消失问题，影响训练效果。 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:5:1","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"应对过拟合问题 过拟合是深度学习训练过程中常见的问题，特别是当模型在训练集上表现很好，但在测试集上性能较差时。为了减少过拟合，可以采用以下方法： 1. 增加数据量 数据增强：通过对现有数据进行旋转、平移、缩放等变换，生成更多的样本，以减少模型对训练数据的依赖，从而提高泛化能力。 正则化技术：如L2正则化和dropout等，能够有效抑制模型的过拟合。 2. 正则化方法 L2正则化：通过在损失函数中增加惩罚项，迫使模型参数保持较小的值，从而避免模型过于复杂。 Dropout：在训练过程中随机丢弃一定比例的神经元，这样可以避免网络过度依赖某些特定神经元，从而提高泛化能力。 3. 提前停止（Early Stopping） 在训练过程中，通过监控验证集的损失，若验证集损失不再下降或开始上升，则提前停止训练。这样可以避免模型在训练集上过拟合。 4. 更简单的模型结构 减小网络规模：使用较小的网络结构（例如减少网络层数或神经元数量），可以减少模型的复杂度，从而降低过拟合的风险。 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:5:2","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"分类任务中的优化策略 在分类任务中，优化策略需要根据任务的特点进行调整： 1. 分类问题转化为回归问题 有时将分类问题转化为回归问题进行解决，网络输出一个连续值。这种方法可能会引入伪关系，因此更常见的是使用One-hot向量作为输出，并结合Softmax函数进行标准化。 2. 损失函数的选择 交叉熵损失 (Cross-Entropy Loss)：对于分类任务，尤其是多分类任务，交叉熵损失是更合适的选择，特别是与Softmax结合使用时，能够更好地指导优化过程。 ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:5:3","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["AI"],"content":"Pytorch——手写数字识别的例子 pytorch internals import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets, transforms from torch.utils.data import random_split # 超参数 batch_size = 64 learning_rate = 0.001 epochs = 10 dropout_rate = 0.5 # 1. 数据加载和预处理 transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) # 加载MNIST数据集 train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform) # 数据分批次处理 train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # 2. 定义卷积神经网络 (CNN) class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() # 第一卷积层，输入1个通道，输出32个通道 self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(32) # 批归一化 # 第二卷积层，输入32个通道，输出64个通道 self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(64) # 批归一化 # 第三卷积层，输入64个通道，输出128个通道 self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) self.bn3 = nn.BatchNorm2d(128) # 批归一化 # 线性层（全连接层） self.fc1 = nn.Linear(128 * 7 * 7, 512) self.fc2 = nn.Linear(512, 10) # 输出10个类别（MNIST是0-9的数字） # Dropout层，防止过拟合 self.dropout = nn.Dropout(p=dropout_rate) def forward(self, x): # 卷积层 + 激活函数 + 批归一化 + 池化 x = F.relu(self.bn1(self.conv1(x))) x = F.max_pool2d(x, 2) x = F.relu(self.bn2(self.conv2(x))) x = F.max_pool2d(x, 2) x = F.relu(self.bn3(self.conv3(x))) x = F.max_pool2d(x, 2) # 将特征图展平（Flatten） x = x.view(-1, 128 * 7 * 7) # 全连接层 x = F.relu(self.fc1(x)) x = self.dropout(x) # 在全连接层后应用Dropout x = self.fc2(x) return x # 3. 模型、损失函数和优化器 model = SimpleCNN() # Adam优化器，并使用L2正则化（weight decay） optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001) # 使用交叉熵损失函数（适用于分类任务） criterion = nn.CrossEntropyLoss() # 4. 训练过程 def train(model, train_loader, optimizer, criterion, epoch): model.train() running_loss = 0.0 correct = 0 total = 0 for batch_idx, (data, target) in enumerate(train_loader): # 将数据移到GPU（如果可用） data, target = data.cuda(), target.cuda() # 清除梯度 optimizer.zero_grad() # 向前传播 output = model(data) # 计算损失 loss = criterion(output, target) running_loss += loss.item() # 向后传播 loss.backward() # 更新参数 optimizer.step() # 计算准确度 _, predicted = torch.max(output, 1) total += target.size(0) correct += (predicted == target).sum().item() # 打印每个epoch的损失和准确率 print(f'Epoch {epoch}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100. * correct/total:.2f}%') # 5. 测试过程 def test(model, test_loader, criterion): model.eval() # 切换为评估模式 test_loss = 0.0 correct = 0 total = 0 with torch.no_grad(): # 测试时不需要计算梯度 for data, target in test_loader: # 将数据移到GPU（如果可用） data, target = data.cuda(), target.cuda() # 向前传播 output = model(data) # 计算损失 loss = criterion(output, target) test_loss += loss.item() # 计算准确度 _, predicted = torch.max(output, 1) total += target.size(0) correct += (predicted == target).sum().item() # 打印测试集上的损失和准确率 print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {100. * correct/total:.2f}%') # 6. 训练和测试模型 # 如果有GPU，使用GPU训练 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) for epoch in range(1, epochs + 1): train(model, train_loader, optimizer, criterion, epoch) test(model, test_loader, criterion) ","date":"2025-01-13","objectID":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/:6:0","tags":["DeepLearning","Summary"],"title":"深度学习基础知识总结","uri":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"categories":["Sociology"],"content":" 人们自己创造自己的历史，但是他们并不是随心所欲地创造，并不是在他们自己选定的条件下创造，而是在直接碰到的、既定的、从过去继承下来的条件下创造。 ","date":"2024-11-23","objectID":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/:0:0","tags":["Book"],"title":"吉登斯「社会学」笔记","uri":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/"},{"categories":["Sociology"],"content":"写在前面的感想 吉登斯的这本「社会学」虽然只有薄薄百余页，却都是精华。做笔记时差点把整本书都复制一遍…（所以与其看笔记倒不如去看书_(┐「ε:)_ 读完印象最深刻的观点有： 社会学不同于自然科学，是批判的学科 社会学理论的两大主流：资本主义理论和工业社会理论 吉登斯简要介绍了这两种理论，并对比了这两种理论对现代社会的不同看法 吉登斯同时批判了两种理论，认为真相应该是某种折中 此外，我也感受到，这些社会学理论的背后都有一些基本的价值，我所总结出的是：机会平等 和 结果平等。这两者不可得兼，但也要一定程度上兼顾。前者激励人们奋发图强，后者能保证社会和谐。「礼记」中的「選賢與能，講信修睦」似乎就是这个意思。 当然，我们肯定也有别的价值：比如自由，比如家庭… 但是这两种平等似乎才是现代资本主义社会面临的核心问题。 资本主义和工业社会理论的分歧点在于： 对于社会现状的判断不同：如果有更好的调查方法和更详实的数据，应该就能弥合这个分歧 对于未来的预测不同：这主要源于他们所构建的理论不同。一个好的理论应该能经得起实践的检验 基本价值的偏差：如密里本德相对于拉伦多夫，似乎更重视结果平等。前两者的分歧都可以用自然科学的方法来弥补，但是价值观的偏差就不能了。 不过，价值观也并不是什么先天就刻在DNA里的东西，而是受到我们的社会实践和社会认识的影响。所以社会学就像社会自身的哲学一样，会在实践中不断重新定义自身。 就像我们需要哲学一样，我们也需要社会学。这是由人本身理解世界的冲动决定的。即使这种理解只是特定条件下的谎言，或者部分的真理，它也是不可或缺的。 此外，在了解了社会学的大体框架后，我也产生了下面一些问题，等待后续有机会再去研究。 资本主义的问题：马克思认为资本主义产权以及自由的概念本身就蕴藏了不平等——究竟哪里不平等了？难道有一种在具有相同自由的程度下还更平等的社会形态吗？ 第三条道路：吉登斯以及许多人主张资本主义和社会主义之外的“第三条道路”（虽然没有明确提出），似乎在上个世纪也有不少政党践行这一理论。第三条道路究竟是怎么样的？它成功了吗？ 参与式民主：吉登斯提出，现有的代议制民主存在许多问题，其本身的前提就是官僚制，而且民主只保留在了政治领域。但是，在人口众多的前提下，难道真的能实现参与式民主吗？ 自由市场的问题：许多社会学家对自由市场提出了批判，认为其逐利的本性反而限制了人自发生产的积极性。甚至对垄断资本提出了赞扬——垄断资本至少没有那么逐利。但是显然竞争不充分也降低了市场效率。那么，不充分的竞争究竟在什么程度上，以什么方式影响了效率与平等呢？ 技术发展的问题：技术发展对社会形态也产生了巨大影响，比如上个世纪白领阶层的出现。如今信息产业的发展似乎大有取代大部分工作之势，或许真将制造一个庞大的失业者群体。所以，技术发展到底会给社会带来什么？如果技术发展反而破坏了社会（就如同19世纪许多工人的生活水平甚至不如农民），我们有办法改变吗？ ","date":"2024-11-23","objectID":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/:1:0","tags":["Book"],"title":"吉登斯「社会学」笔记","uri":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/"},{"categories":["Sociology"],"content":"什么是社会学 社会学产生的背景： 1789 年法国大革命，人类历史上首次出现由纯粹人间理想所指引的运动——自由和平等 18,19 世纪发源于英国的工业革命，带来了城市化，工业化，人口大增长，深刻地改变了社会 在欧洲，这两场大革命带来一些列社会变迁，当人们试图理解这些变迁的条件及其可能的结果时，社会学诞生了。但同时，社会学形成的思想背景也促进了两次大革命。 什么是社会学： 社会学研究的是人类社会 一个社会就是一套制度化(institutionalised)行为模式的集结(cluster)或体系 社会学和其他社会科学，如人类学，经济学都把社会作为共同的研究对象，但社会学的显著特征在于它集中关注“两次大革命”以来所出现的各种社会形式。 最终定义：社会学是一门社会科学，它重点研究的是过去两三个世纪工业转型所形成的社会制度 注意：社会学和其他社会科学间不存在明确分野，而且这种分野也没有存在的必要 社会学与自然科学的不同： 在一开始，许多社会学家认为，研究社会就应该像研究自然科学一样 奥古斯特·孔德(Auguste Comte)： 预测就是为了控制(Prévoir pour pouvoir) 涂尔干主张，社会现象应当作为事物(things)来处理：我们应当把我们自身看作是自然世界中的客体。他从而重申了社会学与自然科学之间的相似性。 但是吉登斯认为社会学的研究方法仍然和自然科学有区别 我们无法像了解自然世界中的客体或事物那样来了解社会或“社会事实”，因为社会仅仅存在于人类自身的创造和再创造行为中。 我们创造社会的同时，社会也创造了我们 社会学的实践意涵也不能直接等同于科学的技术用途。 作为人类，我们不仅仅生活在历史之中，我们对历史的理解本身构成了历史及其未来面貌的内在组成部分。 社会学的想象力： 社会学的实践需要唤起C.赖特·米尔斯所谓的“社会学的想象力” 历史的想象力：能够意识到传统社会和现代社会的区别 人类学的想象力：能够意识到原始社会和工业社会的区别 批判的想象力：能够想象未来的各种可能性 ","date":"2024-11-23","objectID":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/:2:0","tags":["Book"],"title":"吉登斯「社会学」笔记","uri":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/"},{"categories":["Sociology"],"content":"资本主义和工业社会 为了解释“两次大革命”对世界工业化地区的发展所造成的影响，历史上有两天主线：工业社会和资本主义。这两个术语并不是单纯的标签，而是代表了社会思想家在理解现代世界变迁的本质时形成的两种对立途径。 “工业社会”术语是19世纪初期由圣西门(Comte Henri de Saint-Simon)在其作品中提出的，他同时还提出了一些后来被其他研究者所采用的普遍性理论准则。这条主线内部差异也巨大。 资本主义社会的观点首先是与卡尔·马克思联系在一起的，尽管马克思的主要观点也出自此前社会理论、哲学和经济学各种思想流派。 下面以典型的工业社会和资本主义理论为例，比较两者不同： 工业社会理论 主要观点 当代世界最重大的变迁在于从主要以农业为基础的“传统社会”向以机械化生产和商品交换为基础的“工业社会”过渡。 从传统社会向工业社会过渡代表了历史上的进步运动。 对于19世纪和20世纪早期发生在西欧的阶级冲突，工业社会理论认为，这是从传统农业秩序向工业社会转变过程中形成的张力的结果。 但是，随着彼此能够接受的工业谈判模式得到建立，以及“政治公民身份权利”——投票和组建政党的权利——扩展到大多数人口，这些紧张关系很大程度上被化解了 从传统向现代过渡的一个基本环节就是自由民主国家的兴起。自由民主制度是一种常见于西欧和美国的政治制度，盛行议会制政府，由两个或两个以上政党通过选举的方式获取议席。 工业社会理论的支持者假定或认为，工业秩序一旦出现，就存在其基本的一致性。这种观点有时得到了最为直率的表达，其中最著名的要数克尔(Kerr)和他的同事。当然，它也遭到了如本迪克斯(Bendix)这一类人的强烈批判，他们强调，在当代社会，传统与现代的融合存在着各种不同的方式。 现代化理论的核心论点在于，“不发达社会”深陷于传统制度之中，如果它们想要获得西方社会的经济繁荣的话，它们就必须从传统制度中解放出来。从世界上未实现工业化的社会而言，工业社会的观念通常与所谓的“现代化理论”密切相关。 一个代表人物 达伦多夫 的论述（如何看待后马克思时代） 资本主义已经过时，现在是工业化时代 达伦多夫所使用的“工业化”概念表示的是工厂或企业中商品生产机械化的涵义，而工业社会则表示在该社会中，工业主义已成为经济组织的主要形式。 达伦多夫毫不迟疑地告诉我们，工业化是影响当代社会发展的主要现象。他说道，资本主义仅仅是工业社会的一种组织形式——一种仅仅局限于19世纪和20世纪早期的西欧社会的过渡形式 因为所有权和管理权，所以已经没有资本家 但是，这种所有权与管理权重叠在一起的现象仅仅是一种短暂的现象。自马克思时代以来，随着工业规模的不断扩大，资本所有权不再赋予对企业权威体系的控制权了。 资本主义社会不过是工业社会的一种次要形式，不过是工业社会发展的一个阶段。 社会流动性的增长可以带来机会平等的扩大。 个体在教育体系的成功或失败将成为影响其社会地位的主要因素。教育使社会流动更趋自由，而这一趋势又成为工业社会稳定成长的关键。 人们的地位将更多取决于涂尔干的“内在不平等”，而非“外在不平等” 个人将找到与其智慧相当的社会地位，而这又依赖于一个善意国家的引导。 善意国家能调节不同阶级冲突 自由民主国家的出现，各种工业仲裁形式的建立，包括法律上对罢工权利的正式承认，使工业领域的冲突得到了调节和控制。前者使政治领域中代表各种不同阶级利益的政党的正式组建成为可能，后者则使工业领域中的不同利益得到了类似的承认。 资本主义理论 主要观点 经济活动形式的资本主义 资本主义的两个基本结构：资本 和 雇佣劳动力 假定作为经济活动形式的资本主义在工业革命以前就已经出现，并且为工业化的启动提供了刺激因素。 工业化的过程——加速了农村劳动力向以城市为基础的工业劳动阶级的转换。 资本主义是一种永不休止、不断扩张的体系，资本主义生产越来越处于支配地位，并推动了与工业革命联系在一起的技术革新。 政治制度的资本主义 马克思把资本主义看作是一种经济活动形式，同时，由于他相信，其他制度与这种经济活动形式是紧密联系在一起的，因此，他也把资本主义看作是一种社会类型。 在马克思理论中，阶级关系直接把资本主义经济组织与资本主义社会的其他制度连接在一起。 1789年法国大革命，标志着资产阶级政治权力的兴起。 资本主义社会本质上是一种阶级社会，形成这种社会的各种阶级关系本质上是一种冲突或斗争的关系 资本家和工人之间的依赖是一种不平衡关系：工人很少能够控制他们所从事的工作，而雇主却能够依照自己的目的获取利润。 阶级冲突并不会仅仅局限在资本主义发展的早期阶段，它将随着时间的延续而变得越来越尖锐。 国家是阶级权力的表现形式 马克思的时代，大部分人都没有投票权力（因为财产限制） 劳动者政党的组建受到法律限制或禁止 国家更注重于维护资本主义劳动契约 一个代表人物 拉尔夫·密里本德(Ralph Miliband) 的论述（如何看待后马克思时代） 资本主义仍然存在 在密里本德看来，尽管存在着大型企业不断成长和国家不断染指经济领域的现象，资本的私人所有权仍然是当代西方社会的首要特征。 （资本主义社会）有两个重要的特征：一是它们都是高度工业化的社会，二是其经济活动的大部分工具都为私人所占有和支配。 所有权和管理权分离并没有导致资本主义解体 首先，这种分离将不会产生通常所说的进步意义，少数股票持有者依然能够控制一个大企业，只要它的其他股份是高度分散的。 更为重要的是，资本所有者和经理在维持资本主义的生产框架方面有着相同的经济利益，他们都来自相同的特权背景，从而形成了一个相对统一的支配阶级。 社会流动性实际上并没有那么大 密里本德认为，大部分流动都是“小幅度的”(short-range)，也就是说，是阶级体系中毗邻地位之间的流动，很少存在“大幅度”的流动，使来自工人阶级背景的人能够上升到精英群体。 即使把一种较具‘精英’(meritocratic)方式的升迁路径嫁接到现存的经济体系中去，也只能保证许多工人阶级出身的人能够上升到现存阶级体系的上层。这种结果或许是怡人心意的，但它不会把现存的阶级体系变成另一种不同的体系。” 阶级斗争依然是核心 普遍公民权和工业仲裁程序，但在密里本德看来，它们只有在阶级斗争中，而且也只有通过阶级斗争才能实现，并且它们仍然是阶级斗争的核心。（并非善意国家赐予的） 苏联模式：它至多是一种残缺的或扭曲的社会主义面貌。 资本主义国家的社会主义政党巩固，而不是瓦解了现存秩序 为了赢得选举的广泛胜利，它们通常必须淡化其社会变革的方案。一旦上台执政，它们的实际政策也不会像选举运动时所标榜的那么激进，因为既得利益阶层出于其既有的地位和特权所受到的威胁，将会提出强烈的抗议。 “议会制社会主义”(parliamentary socialism)本身不可能奏效，只有借助于议会之外的额外运动，彻底的社会变革才有可能实现。 他认为，由于受流行意识形态的影响，西方社会主义政党出现了广泛的改良主义倾向。 控制了生产工具和政治机器的阶级同时也就控制了这个社会的信仰和象征。在资本主义社会，存在着密里本德所谓的“同意工程学”(engineering of consent)，它可以在人口中培育起对现状的顺从态度。 在达伦多夫看来，教育是通往平等的主要车轮，它促进了流动性和“精英式”社会秩序的形成，但在密里本德眼里，教育主要是一种阻碍社会变革的现象，因为教育体系在每一代人那里都会被反复用来创造有利于统治阶级利益的总体意识形态。 可以总结出他们的几个关注点：机会平等和社会和谐，以及阶级冲突的地位 机会平等在于每个人都得到与其智慧相当的社会地位（選賢與能） 社会和谐在于社会的整体结构不存在过分的不平等和压迫（講信修睦） 当前两者不能满足时，就会有阶级冲突 资本主义理论解释了这两种不平等的来源 拉伦多夫，密里本德对马克思19世纪资本主义工业的描述都赞同，但区别在于近一百年西方社会的变迁 现状的判断不同 -\u003e 需要详实的数据支持 趋势的判断不同 -\u003e 需要完善的理论支持 大道之行也，天下為公。選賢與能，講信修睦，故人不獨親其親，不獨子其子，使老有所終，壯有所用，幼有所長，矜寡孤獨廢疾者，皆有所養。男有分，女有歸。貨惡其棄於地也，不必藏於己；力惡其不出於身也，不必為己。是故謀閉而不興，盜竊亂賊而不作，故外戶而不閉，是謂大同。禮記 禮運 吉登斯的最后评价： 在逻辑上，工业社会理论和资本主义理论都未必说得通，在经验上，两者也未必可行。我们面对的是一个具有无限可能性的世界，我们有关这个世界的知识形成了这些可能性。 但是，评估上面所勾画的这两种观点依然是重要的，我们要看看何者更为准确。因为这将强烈影响我们对这个世界最有可能出现的发展趋势的认识，同时还强烈影响我们有关社会变迁方向的最现实选择的认识。 ","date":"2024-11-23","objectID":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/:3:0","tags":["Book"],"title":"吉登斯「社会学」笔记","uri":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/"},{"categories":["Sociology"],"content":"若干的现象进一步讨论 ","date":"2024-11-23","objectID":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/:4:0","tags":["Book"],"title":"吉登斯「社会学」笔记","uri":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/"},{"categories":["Sociology"],"content":"阶级分析问题 股份制大公司 现象： 这里更重要的是要关注不断提高的经济集中(concentration of the economy)：大型公司对经济生活的支配。 20世纪初以来，美国200个最大的公司以每年递增0.5%的速度扩大了其在总资产中的份额。今天，这200个最大的公司已经控制了整个工业生产的3/5左右。与此类似，200个最大的金融组织现在也控制了美国全部金融交易的一半以上。 在这些大公司中，大部分都是所谓的公开招股公司(public corporate)：也就是说，公司由股票所有者所“拥有”，它们发行既可以买进也可以卖出的股票。 这说明了什么： “经理主义拥护者的立场”(managerialist position) 与全盛时期的企业家资本主义相比较，巨型公司已经“更不那么资本主义”了 19世纪时期的资本主义具有强烈的竞争性，因此，每一个公司都致力于实现利润最大化的目标。但是现在，巨型公司已经处于某一经济领域的主导地位，它们能够支配市场，而不被市场所支配。 另外，这种观点还进一步认为，它们已变得更关注于稳定和长期增长了，而不是一时的最大利润。 这种公司越来越变成了“满足者”(satisficers)。它们所追求的是总体利润率的维持，而不是把利润最大化的目标置于一切之上。 巨型公司的形式拥有者——即股票持有者——现在不再对公司事务形成强大的控制了，权力已经过渡到了经理手中。 既然经理不是“资本家”——他们不拥有他们所控制的公司——他们也就更关心公司内部行政管理的稳定，而不是它所追求的利润。一些持经理主义拥护者立场的作家极为强调这一点。他们宣称，巨型公司已成为一种具有社会责任感的机构，一种具有“高尚情感的公司”(soulful corporation)，它远离了19世纪那种野心勃勃的、自私自利的公司形式。 公司越是趋于集中化，以前的阶级团结(solidarity)形式也就越趋于瓦解。 资本“所有者”现在已成为一个四分五裂的概念，因为现在股票持有者已变得高度分散了。 反驳 实际上，早期实业家通常非常注重从长远的角度经营自己的企业，而不仅仅是获取尽可能多的利润。 或许更重要的是，今天的巨型公司仍然是在竞争资本主义的框架内运作的 从严格的意义上说，很少有哪个巨型公司能够真正处于一种“垄断”的地位：这指的是这样一种情况，在某一特定经济领域中，这个公司是唯一的商品生产者。 这种竞争的压力，这种为了获得尽可能高的边际利润的驱动力，实际上可能是非常大的。 应当注意到，经理主义的主张主要盛行于20世纪五六十年代，当西方工业处于稳定扩张和相对没什么问题的阶段。在随后的几年里，经济危机的再现，以及部分工业化国家依靠低廉的劳动成本使本国的生产商日益居于优势地位，它们给巨型公司在某些经济领域中的获利能力制造了巨大的麻烦。 经理也是资本家 一是与其他资本所有者一样，他们在股市繁荣方面有着共同的利益；二是他们在巨型公司中的活动受资本主义企业这一活动框架的引导。 支配阶级依然存在 西方资本主义社会在财富分配方面仍然存在的明显不平等现象。如果我们从股票和股权的角度考察，而不是一般财富的角度来看，少数人占有的情况还要更加突出。 社会流动的问题——从精英群体的构成角度来看，甚至根本就不存在什么社会流动。 工会：阶级冲突的制度化 现象： 一个世纪以来，墓仍然没有挖好，而且，资本主义未来的继承人尽管不再像其先祖那样充满活力，但也没有那种死期将至的严重威胁。 这说明了什么： “阶级冲突制度化”是工业社会理论家所热衷的观点之一，而且它也明白地涵盖了他们所着重强调的重点。因为根据这些作者的观点，公开或者破坏性的阶级斗争仅仅局限于工业资本主义发展的早期阶段。工业仲裁模式的建立和规范化使阶级冲突的棱角趋于钝化，并转化成为“工业冲突”。 政治权力的获得也促进了其工业谈判权利的发展。 T. H.马歇尔(T. H. Marshall) 19世纪的阶级冲突被此后发展起来的三种连续性“公民身份权利”(citizenship rights)所软化。这三种公民权利是：公民身份权利、公民政治权利和公民社会权利。 第一种公民身份权利，主要涉及法律面前人人平等和参与法律体系的权利。 第二种公民政治权利，主要指普遍公民权的获得，以及组建政党的权利。 第三种权利主要指工业谈判的权利和获得福利的权利——包括失业救济、医疗保险，等等。 马歇尔说道，在过去100多年的时间里，“公民身份与资本主义的阶级体系一直处于尖锐对立的状态”。但是，胜利的是前者，尽管这种胜利可能并不全面，但阶级斗争的威胁已经不再足于瓦解资本主义秩序了。 对马歇尔的修正 马歇尔没有充分强调“资产阶级法律关系”与雇佣工人的地位之间存在的不平衡特征。 早期资本家侧重于建立一个能够按自己意志加以雇佣或解雇的“自由”劳动力后备队伍。 资本主义的劳动契约预先假定了形式上“自由”的个体，他们之间不存在各种封建效忠关系的约束，这是一种通过自由契约所形成的纯粹经济关系。 但是，这种“自由”实际上有助于雇主支配工人的权力。 政治权利尽管使每一个人成为了公民，但它并没有延伸到工业领域，可是，这一领域却占据了大多数人日常生活的大部分时间。 马歇尔所描述的公民身份权利的扩张实际上并非国家仁慈的结果，而是必须积极争取的东西。 马歇尔所说的“福利国家”依然是资本主义 在私有资本的支配下，生产利润仍然是经济系统的主要动力； 私有财产，尤其是私有资本的分布仍然是高度不均衡的； 阶级冲突在经济和政权(polity)层面仍然具有重要的意义。因此，可以说，资本主义仍然是阶级社会。 白领阶层 现象： 20世纪西方社会的阶级体系发生的一个引人注目和最受到讨论的变化就是，与体力或者“蓝领”劳动比较，“非体力”或者“白领”工作的相对增长。 这些统计数据似乎与马克思所设想的资本主义社会的发展轨迹完全背道而驰。因为马克思相信，大部分人口将注定要成为体力劳动者，从事一些例行单调的工作，小资本家很大程度上将消失，最终形成一个少数大资本家与广大无产阶级对抗的格局。 这说明了什么： 马克思所设想的是一个两个对立阶级日益分裂的社会。但的确，新中间阶级的扩张已成为一种促进稳定的力量，它腐蚀了工人阶级，使之越来越纳入到中间阶级的队伍中，而不是反过来使中间阶级受到改造。 但同时许多白领职业依旧具有单调、甚至是机械性的特征 大部分人认为，尽管新中间阶级并不像统计数据所天真地表明的那么大，但它或多或少已成为资本主义阶级关系的一个复杂因素。 信息技术以及失业者 现象1： 我们正在进入一个由“信息技术”所支配的时代，这个时代不再像过去那样由制造工业所主宰。 这说明了什么： 科学在现代生产中所发挥的作用、电脑技术的广泛采用，尤其是最近微芯片技术的广泛普及，将对先前的社会秩序产生深远的影响。 反驳 后工业社会概念延续了隐藏在工业社会理论后面的技术决定论。 在圣西门所构想的“工业社会”中，就是科学家和技术专家的联合统治。但是，它并没有成为现实，而且尽管当代信息技术突飞猛进，它仍然不太可能实现。 只有在一种世界性的背景下，我们才能充分理解与新技术发展联系在一起的社会和经济变迁 现象2： 信息技术的广泛使用可能逐渐改变大部分人所从事的工作的性质，并提高失业率的水平。事实上，大部分世界经济都深陷于长期性经济衰退之中，近年来，许多国家的失业率水平也急剧提升。针对这些现象，许多人认为，工业国家将永远不可能再回到20世纪60年代那种“充分就业”的经济景象中去了。 这说明了什么： 法国作家高兹(Gorz)——工人阶级在消失 当前的情况是，与其说无产阶级推翻资本主义体系，反不如说资本主义的成熟越来越缩小了工人阶级的作用。 在他看来，信息技术将消除大部分现存的体力劳动，同时也将消除许多单调乏味的白领工作。 现代工业所隐含的生产能力，能够创造远高于满足人类基本需要所必须的财富。 工作本身将成为目的，由它能够提供给生产者的自我满足水平所决定。 然而，高兹说道，资本主义的后续发展却彻底背离了这样一种预期。管理制度所采取的严格的劳动控制，已经有效地压制了工人形成任何创造性劳动潜力的意识。 “当前的重点在于反对劳动的性质、内容、必然性和各种模式，以便使自己从劳动中解放出来。” 上面所勾画的发展趋势表明，工人阶级已经被高兹所说的“非劳动者的非阶级”(non-class of non-workers)或“新无产阶级”(neoproletariat)所取代。 这个阶级或者由大量永久失业的人员所组成，或者即使他们在工作，也没有得到很好组织，缺乏阶级认同，职业的保障程度也较低。从长期来看，作为信息技术不断扩张的结果，失业的队伍将会日益增大。 他们越来越倾向于寻求满足感的源泉，但这种满足感与工作和工作场所毫无关系 高兹宣称，我们正在走向一种“二元社会”：一方面，生产和政治管理将会很好地组织起来，以使其效率最大化；另一方面，将会出现一个个人具有多元追求的领域，在这一领域中，享受和自我满足将成为追求的目标。 生产主义的工作伦理将让位于一种关注度过“自由时间”的全新观念，这种自由时间将不再仅仅被当作是一天活动之外的余暇。 希默尔斯特兰德——工人阶级在扩大 在瑞典，工人阶级并没有萎缩 他希望看到的并不是任何国有工业的扩展，或生产计划集中化的扩展。相反，他希望看到的是工人在某种程度上实际控制他们所从事的生产过程——在高兹看来，现代工业是不可能实现这个目标的。 相反，他所谈论的是他所谓的“扩展的无产阶级”(extended working class)——指大部分劳动力——他们对社会主义改革具有普遍的兴趣。 希默尔斯特兰德试图表明的是，根据各种阶级意识指标，这种以扩展的无产阶级面目","date":"2024-11-23","objectID":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/:4:1","tags":["Book"],"title":"吉登斯「社会学」笔记","uri":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/"},{"categories":["Sociology"],"content":"国家性质问题 国家在社会生活中的作用已日益扩张的问题。 从经济角度看，资本主义国家在指导生产活动方面发挥着越来越直接的作用。在绝大多数这类国家，国家直接雇佣的劳动人口占全国从事经济活动的劳动人口的40%以上 另外，在其他社会生活领域，同样少不了国家介入的影子，如参与组建监狱、收容所、医院等，参与供给“福利”名目下的各种服务等。 令人奇怪的是，直到最近，社会学却很大程度上忽视了国家的存在，不论是马克思主义的社会学家还是其他流派的社会学家，都概莫能外。 然而，在过去10年左右，国家却成为讨论的热门话题，在马克思主义者中尤其如此。 国家与阶级 观点1： 阶级统治的直接工具，直接由资产阶级所支配。 国家的阶级特征在于国家官僚所维护的是整个资本主义生产的延续性。 在一个依然是阶级社会的社会，统治圈子中存在着相当多的派系和摩擦。 观点2： 但在普朗查兹看来，资产阶级是一个内在分裂的阶级，国家具有一种独立于资产阶级的“相对自主性”。 在普朗查兹看来，上层阶级内部也存在着分裂，但这并不说明资产阶级的支配由此就遭到了破坏，分裂是一种正常的现象，关键在于，资本主义生产的制度性架构能够维持下去。 国家与官僚 韦伯 资本主义特征：例行化 只有在非个人化的准则(norm)的基础上，这种例行化的形式才能建立起来，因为这种准则能够对各种程序进行详细的说明，并使精确的经济计算成为可能。 韦伯比马克思更加强调，资本主义扩张的条件在于官僚制国家的早期发展。在他看来，在国家的管理之下，建立起一套完备的法律体系和金融体系是资本主义生产大规模扩张的必要基础。 从本质上看，官僚制使权力集中在少数人手里，即集中在处于组织顶峰的那些人手里。 工人被剥夺生产资料的情况并不仅仅限于资本主义社会，因此，超越资本主义并不见得这一现象就将消失。 使大部分人仅仅成为“机器上的一个小齿轮”，所有这些都是官僚制的一般特征 民主的理念起源于小型社会，在这种社会中，那些能被称作“公民”的人仅仅占人口中的极小部分，这部分人可以亲自参与各种集会以行使政治权力。 现代是“政党机器的政治”(party machine politics)的时代，在这种时代里，普通公民的参与程度对政策制定的影响可谓微乎其微。 “民主的精英主义”(democratic elitism)或许是一种有限的政治参与模式，但在多党制的背景下，它还是好过什么都没有 参与式民主的可能性 寡头统治的铁律既不是一种铁律，甚至根本就是一种未加限制的不确切表达。 实际上，在高度整合的工作条件下，工人在某些方面获得了比以前还要大的权力。因为高度整合的生产线是极度脆弱的，很容易为少部分工人步调一致的行为所破坏。 所有行动者都是具有认知能力的行动者，而不是只受其行动环境影响的被动接受者，认识到这一点具有重要的社会学意义。 如果处于下层地位的人能够通过这种消极的方式获得某种程度的权力的话（指的是罢工），那我们就有充分的理由假定，公民身份权利的扩张将进一步把这种权力延展至工业领域。换句话说就是，在当代社会，存在着某些建立“参与式民主”的可能性，不应当像韦伯那样完全排斥这种可能性。正如马克思的分析所表明的那样，这种民主在工业领域将具有特别强大的力量。 国家与革命 在蒂利看来，革命运动是他所谓“多元主权”(multiple sovereignty)——即指由于这种或那种原因，国家无法在其统治范围内充分行使控制权——情况下集体行动的次级类型。 蒂利倾向于认为，对利益有意识和有目的的追求是革命运动的先导，而“成功”的革命运动则发生在人们设法实现其利益的情况下。 在斯考切波看来，革命运动的目标典型的是模棱两可和摇摆不定的，大规模革命性变动很大程度上是一种无意识的结果，是某些群体和运动所要实现的一些较为片面的目标的意外后果。 当斯考切波强调，重大的社会革命并不是那种有目的地组织起来以推翻现存社会秩序为目的的运动时，她无疑是正确的。但是，在有目的地形成的社会变动与“结构性”失调所导致的社会变动之间，她或许过于强调了两者间的对立。 ","date":"2024-11-23","objectID":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/:4:2","tags":["Book"],"title":"吉登斯「社会学」笔记","uri":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/"},{"categories":["Sociology"],"content":"世界体系 现象： 在我们的时代，没有什么会比发生在特定时空背景中的日常生活行为与跨越广泛时空范围的事件联系在一起更富有特色。 现代世界体系理论： 现代化理论 第一，第三世界中的传统社会不仅是低度发展(underdeveloped)的社会，而且是不发达(undeveloped)的社会，它们有待工业转型力量的冲击。 第二，这些国家因此必须重蹈工业化国家所走过的老路，再生产出“工业社会”所取得的成就。 批判 现代化理论建立在一种错误的前提之上，在某种程度上，它为西方资本主义国家支配世界其他地方提供了意识形态辩护。 国家之间的不平等 民族国家 民族国家可以被定义为：它是政治治理的各种制度，在这些制度下，社会统治者成功地实现了对暴力工具（军队和警察）的垄断，控制成为他们在特定领土边界范围内进行行政管理的主要保障措施。 现代民族国家的形成与民族主义情感的兴起息息相关。民族主义可以被定义为：对于某些符号的共同归属感，这些符号可以使特定人群的成员认同他们归属于相同的共同体。 如果说资本主义世界经济是现代世界体系的一个突出特征的话，那么，民族国家体系同样是现代世界体系的突出特征之一。 国家间的无政府主义 ","date":"2024-11-23","objectID":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/:4:3","tags":["Book"],"title":"吉登斯「社会学」笔记","uri":"/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/"},{"categories":["System"],"content":"“I call architecture frozen music…” - Goethe BitTorrent 是一个 p2p 下载协议，官网网站见这里。官方网站内不仅含有标准规范，而且仿照PEP（Python Enhancement Protocol），有着自己的 BEP，用来对协议进行增强。 不过官方标准规范较为难懂，这里有一份协议的非官方版本，更易于理解： Unofficial Specification 这里也有一份简介： https://web.cs.ucla.edu/classes/cs217/05BitTorrent.pdf p2p 网络 最严格的 p2p 网络定义为： A communications model in which each party has the same capabilities and either party can initiate a communication session 也就是网络中每个节点都起相同功能，且不会出现单点故障。与之相反，server-client 式的架构就被称为 集中式 网络。 ARPNET 就是 p2p 式的；DNS system 可以算 p2p 和 集中式 的一个折中。 假如 DNS 是集中式的，一个主机负责全世界的 IP 查询，则很容易碰上 单点故障 和 性能限制。因此，DNS 采取了分布式的架构——树形递归查询加上多级缓存。有许多起相同功能的本地 DNS 服务器，挂掉一个也不影响整个系统运作，未尝不可称之为某种程度上的 p2p。 因此，一个宽松一点的 p2p 定义： “A distributed network architecture may be called a Peer-to-Peer (P-to-P, P2P,…) network, if the participants share a part of their own (hardware) resources (processing power, storage capacity, network link capacity, printers,…). These shared resources are necessary to provide the Service and content offered by the network (e.g. file sharing or shared workspaces for collaboration). They are accessible by other peers directly, without passing intermediary entities. The participants of such a network are thus resource (Service and content) providers as well as resource (Service and content) requestors (Servant-concept).” 这样的定义下，虽然不能完全避免单点故障，但的确体现出来 p2p 的本质： 每个参与者即是 服务提供者 又是 服务享受者 因此，p2p 就有着以下特点： 相对可扩展性：增加服务享受者的同时也增加了服务提供者 相对稳定性：许多服务提供者，挂掉一个也无妨 BitTorrent 下面再来介绍典型的 p2p 协议 —— BitTorrent。 BitTorrent 主要被用来分发/下载文件，因为其 p2p 的特性而不需要耗费上传者太多带宽 —— p2p 网络中的每个用户在下载文件的同时也把自己已下载好的文件传给别人。 BitTorrent 协议与在它之前的 p2p 协议相比，有一个特点：它会把每个文件都切分为许多小块（piece），一个用户可以同时向许多同伴（peer）请求同一个文件的不同部分。 而它之前的 p2p 协议往往是一对一协议，一个下载好的人负责给一个没有下载好的人传送文件。但是因为一般网卡的上传带宽都远远小于下载带宽，因此这种模型下，文件传送的最大速率最大也只能等于peer的上传带宽。 但是在 BitTorrent 下，对于一个非常火爆的已经有很多人下载好的文件，我们就可以同时向他们所有人请求文件块，充分利用自己的下载带宽了。这也是为什么 BitTorrent 一经推出就统治了 p2p 下载界。 比如上面的文件，有 169 人上传，只有 18 人下载，可想而知下载速度是非常快的 BitTorrent 的架构中含有如下实体： 一个元信息文件： torrent file 一个集中式 tracker 第一个参与者：seed 其他参与者：leecher tracker 会记录哪些 peer 正在下载文件，并帮助他们互相认识。peer 通过 http 协议和 tracker 进行通信，以获取相关信息。 世界上有许多公开的 tracker 服务器，比如这个列表里收录的：trackerlist 在一开始，一个用户决定分发某个文件时，会先做种（seed）：他会使用本地的bt软件，把文件分为通常是 512 KB 或 256Kb 许多小块（piece），并计算每个小块的 SHA-1 哈希值 ，然后把所有的哈希值和相关元信息封装在一个 torrent 文件里，传给别人。同时他也在 tracker 处登记自己持有这个文件。（通常 torrent 文件里也包含推荐的 tracker） 当某个用户要下载时，他会向 tracker 请求 peer 列表，然后向 peer 们请求不同的文件块，并用 torrent 文件里的哈希值验证数据是否出错。 在这个流程中，有两个算法决定着 bt 网络的正常高效运转： 决定先向 peer 请求那个块（The Piece Selection Algorithm） —— 以达到最大下载速度 决定是否给某个 peer 发送数据（Resource Allocation） —— 以避免懒汉（free rider） The Piece Selection Algorithm 快选择算法的核心目标有两个： 尽快在不同 peer 上复制不同的块（确保任何时刻网络中总能拼凑出一份完整的文件） 尽快下载完成文件 ","date":"2024-11-16","objectID":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/:0:0","tags":["BitTorrent","Knowledge"],"title":"BitTorrent协议及其扩展","uri":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/"},{"categories":["System"],"content":"sub pieces BitTorrent peer 之间通过 TCP 协议进行连接，因此需要避免 TCP 慢启动机制 对传输速率的影响。块常常被划分为 16kb 大小的子块进行传输，BitTorrent 协议保证和一个 peer 之间无论何时总是有一些 子块 等待被传输，以此来维持高速连接。一个块的不同子块可以从不同的 peer 处下载。 ","date":"2024-11-16","objectID":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/:0:1","tags":["BitTorrent","Knowledge"],"title":"BitTorrent协议及其扩展","uri":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/"},{"categories":["System"],"content":"policys strict policy：每开始下载一个块时，就优先下载该块的所有子块。 random first piece：刚开始下载时，为了尽快得到能给其他 peer 上传的块，就随机选择一个块进行下载。 rarest first：优先下载网络中最稀有的块。 endgame mode：当到最后一块时，向所有 peer 都请求该块的子块，以快速下载完文件。 Resource Allocation Resource Alllocation 算法用来决定和哪个 peer 进行合作。我们采用“tit-for-tat” （以牙还牙）策略，以达到利益最大化。一般来说，你上传的越多，下载的也就越快。 Enhancements 自从 BitTorrent 被发明以来，许多 BEP 提出了一些改进，下面介绍一些有趣的改进功能。 ","date":"2024-11-16","objectID":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/:0:2","tags":["BitTorrent","Knowledge"],"title":"BitTorrent协议及其扩展","uri":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/"},{"categories":["System"],"content":"Decentralized tracker youtube视频简介 BitTorrent 中，由于 tracker 服务器的存在，所以不能达到 “真p2p”，因此就有了 去中心化tracker 拓展。它基于 DHT （Distributed hash table）在 peer 间存储 peerlist，以取代中心化 tracker。 DHT 的基本思想为：将所有 node 连接成一个环，环上每个 node 都负责存储某一范围内的 key，且知道相邻 node 的地址。查询时沿着环就可以找到目标key了。当添加或删除新 node 时，都调整数据分布以使得环结构仍然成立。 ","date":"2024-11-16","objectID":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/:1:0","tags":["BitTorrent","Knowledge"],"title":"BitTorrent协议及其扩展","uri":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/"},{"categories":["System"],"content":"Upnp 问题的起因是这样的：我们的 bt客户端 大都运行在内网，可以通过路由器的 NAT机制 访问外网，可是外网设备就访问不到我们的 bt客户端 了，这就让其他 peer 没办法主动和我们建立连接。为了解决这个问题，我们可以在路由器上手动设置端口转发（当然前提是你的路由器就拥有公网 IP），或者使用 VPN。 Upnp （Universal Plug and Play）是一个应用层协议，可以在路由器上开启，接受到客户端的请求后，进行自动端口转发。其原本目标是为了让设备之间无需手动配置就能相互连接，不过许多 bt客户端 也实现了 Upnp协议，可以和路由器协商进行自动端口转发。但目前 Upnp协议 存在安全漏洞，很容易成为黑客攻击的目标。 ","date":"2024-11-16","objectID":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/:2:0","tags":["BitTorrent","Knowledge"],"title":"BitTorrent协议及其扩展","uri":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/"},{"categories":["System"],"content":"DDNS DDNS 即 动态（dynamic）DNS服务。如果机器每次启动都是不同的 IP，那么就可以使用 DDNS 服务，在机器每次启动时都向 DDNS 服务器告知新 IP。这适用于当你的路由器被运营商分配动态公网 IP 时，也能对外界提供一个稳定的地址。 ","date":"2024-11-16","objectID":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/:3:0","tags":["BitTorrent","Knowledge"],"title":"BitTorrent协议及其扩展","uri":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/"},{"categories":["System"],"content":"内网穿透 但当路由器本身就被运营商分配了局域网 IP 时，就只能通过内网穿透才能使我们的 bt客户端 暴露在公网中，接受其他 peer 的请求了。内网穿透即是通过外网中持有公网 IP 的服务器提供的中介服务，来暴露我们的客户端（相当于简化版 VPN）。 ","date":"2024-11-16","objectID":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/:4:0","tags":["BitTorrent","Knowledge"],"title":"BitTorrent协议及其扩展","uri":"/posts/bittorrent%E5%8D%8F%E8%AE%AE%E5%8F%8A%E5%85%B6%E6%89%A9%E5%B1%95/"},{"categories":["System"],"content":" 纵横计不就，慷慨志犹存 今年5月到8月间参加了 CSCC 举办的编译系统设计赛，但实际上集中开发也就是7月。尽管达成了最初的目标——“学习编译器后端知识”与“找回写代码的感觉”，但最后的名次却不尽人意。虽然比赛8月早已结束，近日写总结也不算为晚（总还未完全忘记）。 我们的编译器实现 ","date":"2024-11-09","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/:0:0","tags":["Reflection","Compiler"],"title":"编译器开发反思","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/"},{"categories":["System"],"content":"前端 使用antlr4生成lexer和parser，构建抽象语法树（AST）。 采用访问者模式遍历AST，构建中间代码（IR）。 ","date":"2024-11-09","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/:1:0","tags":["Reflection","Compiler"],"title":"编译器开发反思","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/"},{"categories":["System"],"content":"中端 ","date":"2024-11-09","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/:2:0","tags":["Reflection","Compiler"],"title":"编译器开发反思","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/"},{"categories":["System"],"content":"IR设计 IR结构设计参考了LLVM IR的设计，基本组件包括： Module: Module是IR的顶级容器，用于表示一个完整的程序或翻译单元。它包含函数、全局变量和相关的元数据。 Function: Function表示程序中的一个函数，由一系列基本块（BasicBlock）组成。每个函数在Module中唯一标识。 BasicBlock: BasicBlock是IR中的基本执行单元，由一系列指令组成。每个基本块以终结指令（如br或ret）结束，并且在执行过程中不包含控制流跳转。 Instruction: Instruction是IR中的操作指令，类似于机器指令。每个指令可以产生一个值，并作为其他指令的操作数。常见指令包括算术运算（如add、sub）、内存访问（如load、store）和控制流指令（如br、ret）。 Value: Value是IR中的数据表示，包括变量、常量和指令的结果。所有的Instruction和Constant都是Value的子类。 ","date":"2024-11-09","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/:2:1","tags":["Reflection","Compiler"],"title":"编译器开发反思","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/"},{"categories":["System"],"content":"IR优化 中端实现了各类优化，提升了代码性能。实现的优化包括： MemToReg: 实现寄存器提升（Promote Memory to Register），减少内存访问。 CommonSubexpElimination: 消除公共子表达式，减少重复计算。 MergeBlock: 合并基本块以简化控制流图。 Inlining: 函数内联，减少函数调用开销。 LoopSimplify: 简化循环结构。 LoopInvariantCodeMotion: 循环不变代码外提。 GlobalCodeMotion: 全局代码移动。 GlobalValueNumbering: 全局值编号，优化冗余计算。 LoopUnroll: 循环展开，提高性能。 TailRecursionElimination: 消除尾递归。 DeadCodeElimination: 死代码消除。 ConstantFolding: 常量折叠，计算常量表达式。 GlobalVariableLocalize: 全局变量本地化，减少全局变量使用。 StrengthReduction: 强度削减，用更低代价运算代替高代价运算。 LoadElimination: 加载消除，减少不必要的内存加载。 Reassociate: 重结合运算。 GEPSimplify: 简化GEP指令。 CFGSimplify: 控制流图简化。 StoreElimination.cc: 存储消除，减少不必要的内存存储。 LruCache.cc: 缓存递归纯函数的结果，避免冗余函数调用。 InductionVariableSimplify.cc: 简化归纳变量。 ","date":"2024-11-09","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/:2:2","tags":["Reflection","Compiler"],"title":"编译器开发反思","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/"},{"categories":["System"],"content":"后端 Instruction Seletction: 将中间代码转换为面向目标机器的低层IR。 Register Allocation: 采用SSA分配算法，先溢出以减小寄存器压力，再对寄存器进行着色。 Peephole Optimization: 实施一系列小规模的代码优化，如指令合并和消除冗余指令。 Instruction Scheduling: 采用 Local List Scheduling 算法，进行简单的指令重排，并针对 SIFive u74 的双发射特性进行了优化。 Branch Simplification: 实施和控制流相关的一些优化，如消除冗余跳转指令，基本块排序。 开发的流程与思路 ","date":"2024-11-09","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/:3:0","tags":["Reflection","Compiler"],"title":"编译器开发反思","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/"},{"categories":["System"],"content":"理想 vs 现实 最一开始确定的开发原则有两条： 渐进式开发 测试驱动开发 渐进式开发 主要指先实现 Sysy 的一个可工作子集，然后再不断扩充功能，直到实现完整的 Sysy 语言。在此之后慢慢增加各种优化。 这样做的好处有： 在大部分时间都有一个可以正常运作的编译器，可以增强开发者信心 每次实现一小部分功能，方便实现和测试 此外就是测试驱动开发，意思是每当实现一个功能时，先写好测试用例，然后进行开发。 这样做的好处有： 开发时有一个明确的目标，增强动力 每一部分代码都有测试，保证代码的正确性 同时编译器又可以分为前端，中端和后端三个部分。 最初的计划是小组四名成员，一人负责前端实现，两人负责中端框架实现，一人负责后端框架以及无优化下的翻译实现。在基本框架实现完成后进行调试，得到一个最简单的无优化编译器。之后再做进一步计划，渐进式不断增加新的优化pass。 然而这个计划却没有成功——我们前后中三端各自实现了一个月，才分别实现好，最后一起debug十余天后就到ddl了… 究其原因，是前端成为了我们的开发瓶颈。在后端和中端开发完成后，前端依然不能正常运行，以至于中后端bug无法及时暴露和修复。如果没有前端，中后端测试时只能通过直接调用API构造IR测试用例，但这又过于繁琐（当然也可以再实现一个 IR parser，这就更麻烦了，更何况有许多现成的 Sysy 语言测例可以利用）。所以我们还是选择等待前端完成后再debug，在这段时间抓紧时间实现一些优化 pass。但等到前端实现完成时，中后端也已经积累了非常多的bug，最后大把时间都花在了debug上… ","date":"2024-11-09","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/:4:0","tags":["Reflection","Compiler"],"title":"编译器开发反思","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/"},{"categories":["System"],"content":"陷入debug困境的原因 造成这样灾难性后果的原因主要有两点： 第一，开发规划不明确。 在一开始我认为前端较为简单，能在短时间内实现完整的 Sysy parser，而中端只是仿照 LLVM 实现，并不复杂（而且中前端的许多知识在本院课程中都已经学过）。所以实现一个可以 work 的无优化编译器应该并不困难。 可是当前端明显暴露出短板时，我才意识到直接实现一个完整前端的工作量较大，也许先实现一个 Sysy 的子集是更合理的。于是我告诉前端开发同学，“只要先实现 Sysy 的一个子集”，“能先跑起来就好”。可是当时的我却并没有 明确规定 Sysy 的哪个子集？如果我不去明确规定划分，前端可能也不知道如何选取一个易于实现的子集——如果前端有选取易于实现子集的能力，那么显然他也能在短时间内实现完整个前端。所以，前端开发同学最后还是花了一个月时间，实现了一个充满 bug 的完整前端。 第二，测试的基础设施不完善。 其次是测试驱动开发中遇到的问题：在前端遇到瓶颈时，中端和后端仍然不管不顾蒙头开发。可是现在回想一下，尽管没有前端不能进行系统测试，但是对部分函数进行单元测试总是可以的。实际上我在编写代码时也对部分函数感到不放心，但还是决定先把问题放着，等到集成测试时再 debug ——最后的集成测试时就成了一场灾难。实际上，c++虽然没有 rust 那么方便做单元测试，但是也有 Google test 等框架来帮忙。在前端迟迟没有完成的情况下，单元测试就更为必要了。 第三，出现问题时没有及时调整分工。 在前端遇到瓶颈时，当时更优的解决方法其实应该是让后端和中端的同学去协助前端，而不是让他们仍然开发各自模块。但是因为前端同学总是承诺马上就要做完了（实际上却一拖再拖），一直到20余天后中端开发同学才去帮忙实现。承诺往往是不可靠的，进行计划调整时其实应该更重视过往经验，而非开发人员的口头承诺。 ","date":"2024-11-09","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/:5:0","tags":["Reflection","Compiler"],"title":"编译器开发反思","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/"},{"categories":["System"],"content":"亮点不充分的原因 这里的亮点指我们在比赛中与其他队伍相比，较为亮眼的部分。当然，最后就成绩来看，也并没有什么非常亮眼之处了。 大部分组的亮点基本可以分为以下三类： 出色，完备的代码 实现了好的算法 优秀的基础设施 因为赶工，基本就不求 基础设施 和 代码质量 … 可能唯一值得一提的是算法。 中端参考了往年代码，实现了大部分的常见优化。后端在寄存器分配时使用了基于弦图的SSA分配算法，并且针对硬件做了简单的 Instruction Scheduling。不过SSA分配算法带来的性能提升并不大（实现上也略有粗制滥造之处），指令重排也是后期临时添加，提升效果非常有限。尽管如此，因为中后端优化实现的都比较完全,在初赛时也有一个不错的成绩。 但8月因为种种事情，在决赛以及之前并没有做多少新的优化，而其他组这期间大都在中端做了许多激进的优化：如函数记忆化，数组访问优化，以及一些针对特殊用例的优化。 最后也得知，许多队伍都参照了往年代码（这也就是比赛为什么一年比一年卷的原因），因此他们的目标很明确： 实现往年第一名实现过的所有优化 在此基础上再寻找新的优化可能 而我们组就偏向于蒙头写代码，想到什么写什么了。 这里的教训时：如果对于名次还是比较在意，比赛开始时就应该早早搜集信息，明确目标。 团队管理的总结 开发出现问题的一个重要原因就是团队管理没有做好。虽然是四个人的小组，但大部分工作却由两个人完成。可以预想，如果只有两个人，开发效率反而会更高。或许，在开始时应该多了解一些团队管理的技巧的… 最一开始人们结成团队，肯定是拥有共同的目标。我们经过利益权衡后选择加入团队，以使得自己的利益最大化。可是加入团队后，却往往丢失了主动性，开始变得依赖管理者，并因为各种琐事而推脱团队事物。经常，只有管理者才一直把团队的利益放在心理（这是我在几次合作中的体会）。也许这也和不同文化有关，如「文化与组织」这本书里说的，华人在团队合作中往往习惯于集权，管理者往往具有更大权力，但相应的，成员也会更缺乏“主人翁意识”。在这种情况下，管理者正确运用自己的“权力”，做出更好地决策就更为重要了，在适当时候，不妨果断一些… 我在团队合作中感受到的问题有： 信息交流不足 几个陌生人组队，很容易陷入尴尬状态，不能充分讨论，博采众长。 此时就需要管理者破冰，主动召开会议，客观分析现状，让每个都发言，都能有参与感 这样经过充分讨论做出的决策，每个人也更认同 动机不足 偷懒是人的天性… 尤其是成员，如果管理者没有分配任务，明确目标，就很容易偷懒 所以就需要管理者主动给每个人分配有明确验收标准的任务，并确定DDL 难以决策 可能因为华人文化的特点，大家都习惯于听从管理者的命令，不愿意主动提出自己想法 此时决策就需要管理者更自信一点，去一锤定音了 虽然大家没有明确提出自己意见，但是他们认同决策与否可以通过执行决策是否积极来判断 但发现大家的动力不足时，应该主动了解原因，调整决策 后端知识学习经验 我主要负责后端的实现。因为之前对于编译器后端了解较少，实现时也在短时间内学习了大量新知识。 在学习新知识时，往往会遇到许多困难。比如在学习某个寄存器分配算法时，我会先去看看一些经典的大部头教科书，提出算法的 paper，以及相关课程的slide，或者 youtube 上的讲解视频，最后是一些程序员的博客，中文所写的综述。不过在学习时我也并没有明确规划，而是这个看一点那个看一点，最后还是感到一头雾水。 经典教科书的问题有： 英语所写，较难抓住重点和关键的思想；工作记忆有限，梳理不清条理 都是讲最经典的实现，用简单的例子慢慢引入，学习时较为耗费时间，且很难知道研究现状和该问题各种算法的综合比较 paper的问题有： 因为阅读经验不足，也难抓住重点 context较多，就会变得难以理解；而且不知道该论文在领域内处于一个什么样的位置 slide的问题有： 虽然能抓住核心，但太简略难懂，许多图像意义需要脑补，不好理解 youtube视频以及博客的问题有： 虽然许多资料深入浅出，但许多深入内容的较少，也有许多价值不高的内容 …. 经过一些挫折后，我尝试着给出一个较为系统化的新知识学习方案，也许能以后作为参考。 STFW 搜集各种信息，并分类整理搜集到的信息 了解相关背景知识和发展历程，心里有一个大概的框架 深入学习其中一个经典案例，搞清楚核心的思想，基本的抽象；这一步可以做一些笔记，或者在草稿纸上模拟，以加深理解 再去搜集一些综述，了解该领域发展现状，能有一个更细致的框架 此时根据自己的目标 以及 刚刚学习到的框架，为目标选择一个适合的目标算法，然后去学习 当选定目标算法并实现时，可以参考开源项目代码以及开发人员的实现相关博客 总结 项目 明确规定目标：可以文档化规范化需求 完善基础设施：既有集成测试，也有单元测试 及时识别瓶颈并处理 留够时间做killer feature 管理 交流：充分协商，打开思路，让每个人获得参与感 动机：设置ddl 决策：果断决策，不断迭代（对于集中式团队） 比如：通过ddl完成情况来了解每个人的动力值，及时给动力值不足的人重新分配任务 学习 系统化 ","date":"2024-11-09","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/:6:0","tags":["Reflection","Compiler"],"title":"编译器开发反思","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E5%BC%80%E5%8F%91%E5%8F%8D%E6%80%9D/"},{"categories":["System"],"content":"查看本文配套的 slide 点这里 大家好，今天我分享的主题是「打破壁垒：代理技术在家庭网络中的应用」。主要是向大家介绍，假如在家庭网络中，有黑客挟持了网关，并对正常访问互联网产生了干扰，我们如何通过代理技术来突破封锁，打破壁垒，正常地访问互联网。 运行良好的家庭网络 首先我们来复习一下在一个运行良好的家庭网络中，用户是如何访问互联网，以及网关在其中起到了什么作用的。 上图是一个最简化的家庭网络的基本架构。ISP，也就是互联网服务提供商向你提供一个内置光猫的路由器。光猫把光缆中的光信号转化为路由器能够解析的电信号。路由器是在网络层进行路由转发的设备，往往也是一个通用计算机。路由器也往往充当了无线局域网络Wi-Fi的AP，接入点。 我们的设备在连接到Wi-Fi时，就相当于接入到了这个这个无限局域网。具有Wi-Fi功能的路由器一般也充当了dhcp服务器的角色。dhcp服务器在局域网中广播告知自己的存在，当设备检测到后就向dhcp服务器请求分配IP。dhcp不仅仅会告知设备自己被分配的IP，还会告知设备当前网段的子网掩码以及网关IP地址。在此之后，用户所有的网络报文都会被转发给网关，然后经由网关再转发给真正的目标。网关也往往是由这里的路由器担任。 黑客、襲来 鉴于路由器“一夫当关，万夫莫开”的地位，它很容易成为被攻击的对象。假设我们处于一个合租房里，黑客阿至是我们的房主。阿至是一个狂热的百度厌恶者，他千方百计地阻止别人访问百度，甚至不惜对路由器偷偷做手脚，以使得租客无法访问百度。 由于家中地路由器就是一个通用计算机，阿至通过刷机的方式掌握了这台路由器的root权限，从此他就可以对路由器胡作非为了。他把这台路由器刷成了Linux的系统，并输入了下面命令： iptables -A OUTPUT -d baidu.com -j DROP iptables -A INPUT -s baidu.com -j DROP 下面两条命令会使路由器拒绝一切由百度发送而来，或者发向百度的报文。iptables是一个运行在linux用户态的工具，但他可以对内核网络栈的处理规则进行配置。具体而言，用户可以通过它来为内核添加一些hook函数，内核会在处理网络报文时调用这些hook函数，并根据结果对报文进行一些处理。比如第一条命令，iptables 会首先通过DNS查询获得baidu.com的IP地址，然后在一个网络层报文经过内核栈处理并发送之前，判断该报文的目标是否是baidu.com，如果是，则直接丢弃该报文。第二条命令做的也是类似的事情，他会丢弃从baidu.com发来的报文。 这样阿至就成功地阻止了我们访问baidu.com。 代理服务器 在阿至对路由器做了手脚后，我们就不能再访问百度了。但是百度之外的网站却又可以正常访问，所以我们很快就猜到阿至做了什么。我们可不会向黑客屈服的！如果除百度之外的网站都可以访问…那么只要我们在外网上还有一台服务器，通过这台服务器的中转来访问百度，这样经过路由器报文的源IP和目的IP都会变成服务器IP而不是百度IP了，这样我们就能正常访问百度了！ 经过百度查询之后，我们发现了socks5代理这一应用层协议。在这个协议中，用户程序需要和目标通过TCP/UDP协议通信时，可以先通过socks5协议与实现了该协议的代理服务器通信，然后代理服务器再和目标通信，将返回报文转发给用户程序。HTTP/HTTPS协议起到的作用与socks5协议类似。大多数使用网络的用户程序都实现了该协议，比如 curl，大部分浏览器 他们会在发送网络请求时先查看环境变量中是否定义了 http_proxy socks_proxy 等，如果定义了，则通过代理服务器与目标通信，否则直接通信。但也有一些用户程序，比如 wget，并不会主动检查并使用用户定义的代理。 所以，我们只要使用代理服务器，就可以绕开阿至的封锁了！ 黑客、侵入 不过经过一段时间后，阿至发现我们经过路由器的报文总是发向同一个地址，他很快就猜到我们是使用了代理服务器来绕开路由器的封锁。他对经过路由器转发的数据包分析后发现，许多包中都存在 baidu 这样的字段… 于是阿至又想到了一个点子，他在路由器中输入了如下命令： iptables -I INPUT -m string --string \"/baidu/i\" --algo regex -j DROP 这样，只要报文中含有 “baidu” 这个字符串，就会被路由器丢弃，宁可错杀一百，不可放过一个！ 加密的代理 不久后，我们也发现baidu又不能正常访问了，甚至用bing搜索“baidu”都不行，似乎只要含有“baidu”这个关键词，报文就会被丢弃。于是我们想出了一个办法：既然你做关键词匹配，那么只要我的报文里没有关键词就好了。如果经过路由器的报文都是经过加密的，阿至肯定就不知道我们报文的真实内容是什么了，他也不至于和我们彻底闹翻，丢掉所有报文，不让我们连接互联网吧。 我们首先想到的是，对socks5代理进行增强，让应用程序与代理服务之间进行加密通话。可是市场上的应用程序基本都只实现了简单的socks5代理，如果要让他们支持加密功能则需要把他们的源代码都改了，这个工作量可太大了！ 经过思考，我们想到可以利用已有的socks5代理协议，在此基础上实现自己的加密版本。我们写了一个socks5代理服务器SS Local，local接收应用程序的代理请求，可是local不运行在外网，而是运行在内网，它会把应用程序发送给他的报文先加密，然后发送给外网的另一个socks5服务器SS Server，server会解密报文，得到真正需要代理的请求。当请求返回时server也会先把报文加密，发送给local，local把报文解密，再发送给用户程序。 这样，尽管用户程序以为自己还在使用普通的socks5代理，可是我们已经悄悄对socks5协议进行了升级，实际上经过路由器的报文都是加密过的了，阿至再也无法知道我们在看些什么了！ 透明的代理 虽然我们已经战胜了阿至，做到了几乎完美的加密，但我们还不满足于此。 上面的代理得要用户程序主动使用，有没有可能实现透明的代理方法，让所有网络报文都经过代理？ 答案是有的。只要通过新建虚拟网卡，把通过其中的报文都代理了就好了… VPN的原理就是这样… 漫无止境的战斗 不过，网络封锁与突破，将永远是一场漫无止境的战斗。 事实上阿至还可以 做 DNS 污染， 识别加密数据规律 主动嗅探… 但同时，兵来将挡，水来土掩，我们也永远有新的突破网络封锁的方式 “猫鼠游戏\"永远不会完结 总结 黑客的手段 我们的举措 ip审查 代理 内容审查 加密代理 流量规律 换加密方式 主动嗅探 更好的伪装 ​ ​ ","date":"2024-11-06","objectID":"/posts/%E4%BB%A3%E7%90%86%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/:0:0","tags":["Share","Network","Proxy"],"title":"打破壁垒：代理技术在家庭网络中的应用","uri":"/posts/%E4%BB%A3%E7%90%86%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"},{"categories":["System"],"content":" Programming languages should be designed not by piling feature on top of feature, but by removing the weaknesses and restrictions that make additional features appear necessary 之前在 Twitter 上听 Robert Nystrom 的一个演说时，有观众问他“如何看待 ChatGPT，Copilot 对于编程的影响”。Robert 回道：他认为编程的乐趣在于“make something”，而 Copilot 这类工具却很有可能把 programmer 变成代码审核员，从而丧失了“make something”的乐趣。可是就算在前 ChatGPT 时代，我又真正体会到过“make something”的乐趣吗？之前我的编程实践总是一些课程作业，这些作业的 idea 或是框架总是由他人提出，目的也往往是通过 OJ。这样的编程实践给人带来的“make something”之感自然就大打折扣了。于是在可能发生的“AI 革命”的前夜，我决定自己动手写一个兼容 R7RS 的 Scheme 解释器，真正“make something”。 在大一时曾读过部分 SICP，对 Scheme 有一点点认知。但对于其很多高级特性还不是很熟悉，尤其是 continuation 和 macro。于是在动写解释器前，打算先熟悉一下 Scheme 的特性。 S-Expression 1960年，John McCarthy 在函数式编程的开山之作 Recursive Functions of Symbolic Expressions and Their Computation by Machine 中提出了 LSIP 语言，这也是 Scheme 的前身。LISP 语言最初也是为了支持人工智能系统 Advice Taker 而创造的(可惜 Advice Taker 代表的符号主义 AI 研究方法在当前的 AI 浪潮中似乎不见了身影)，其目的在于提供一种操作 expression 的功能以使得 Advice Taker 能在其上推理。 在 LISP System 中，有 S-Expressions 和 S-Functions(对应 Scheme 中的过程/procedure) 两个概念。S-Expressions 即是 Symbolic Expression，和数学意义上的 expression (表达式)意义相同。S-expressions 本身仅仅是一种数据结构，没有任何求值的含义。 S-Expression 的定义如下： (以及list的定义如下) 在 S-Expression 之上，我们又可以定义对其进行操作的 S-function (S-function事实上为 partial function，因为有可能无限循环) atom. atom[x] has the value of T or F according to whether x is an atomic symbol. atom [X] = T atom [(X · A)] = F car. car[x] is defined if and only if x is not atomic. car [(e1 · e2)] = e1. Thus car [X] is undefined. car [(X · A)] = X car [((X · A) · Y )] = (X · A) 当然S-function也可以递归： ff[x]. The value of ff[x] is the first atomic symbol of the S-Expression x with the parentheses ignored. Thus ff[((A · B) · C)] = A 向上面的式子叫做 M-expressions (meta-expressions)。(注意到 M-expression 之上就有求值的语义了) 有趣的是，我们可以通过 S-Expressions 来表示 S-Functions，同理可以把任意一个 M-expression 用 S-Expression 表达： (这样就有平时用的 Scheme 的感觉了，，) 然而单是这个转换本身没有什么作用，我们需要有一个函数 eval 来对转换后的 S-Expression 进行求值，其结果相当于对应的 M-Expression 的求值结果。我们在写 LSIP 程序时其实就是在写 S-Expression，但表示的却是 M-Expression。解释器运行程序就相当于把你写好的 S-Expression 丢给 eval 函数并返回结果。 S-Expression 这种程序的表示方式自然也被 Scheme 所继承，不过在 Scheme 中这些东西都是小写。而第一条转换规则中的 Atom QUOTE 可以简写为’。 除了 S-Expression，Scheme 还从 LISP 中继承了 GC,动态类型。高阶函数等特征。 Fun Facts: John McCarthy 父亲为爱尔兰移民，母亲为立陶宛犹太人移民，二人都思想开放，加入了美国共产党(Communist Party USA)。John 出身生于1927年，少时通过阅读俄文译作 100,000 Whys (难道中文版的《十万个为什么》也来自苏联？)燃起了对科学的兴趣。John后来也精通俄语，与多苏联科学家相识。但当他在 1968 年正真去苏联旅行之后很快就失望了，转变为共和党支持者。 Continuation Scheme 于1970年代在 MIT AI Lab 被 Guy L. Steele 和 Gerald Jay Sussman发明(又是AI？)。与父亲 LISP 不同, Scheme 实现了尾递归优化，并引入了宏和 first class continuations，本节主要介绍 continuation，下一节介绍宏(Macro)。 上一节说了 M-Expression 有求值语义，eval 函数也能求值，但并未说明到底怎样求值。具体来说，eval 接收到一个 S-Expression 后，对 S-Expression 的所有子Expression(接收到的 S-Expression 应为一个list，子 Expression 指的是 list 中的各个子项)逐个调用eval函数。第一个子 Expression 应为一个过程，剩余的 expression 为这个过程的参数，然后eval会调用apply将参数作用到过程上并返回结果。对于 Scheme 语言中定义的基本过程，apply 会直接计算并返回结果，对于复合过程，apply 会将函数形参替换为实际值，再调用 eval 对函数体求值并返回。 可以看到，对一个 S-Expression 求值的过程可以看作一次次的对 eval 的调用。而每个 eval 调用结束之后都有下面还要进行的求值，这个后面还要进行的求值就叫做当前求值的 Continuation(延续)。 Scheme 中的 Continuation 是一个很抽象的概念，在其它编程语言中往往缺少对于概念。但在C语言中，我们可以通过机器状态来理解 Continuation。在C语言中，假如要调用一个函数 f，会在栈上 push f 的帧，f调用结束之后会将返回值 push 到栈上，再接着做后续计算。如果再将返回值 push 到栈上的一瞬间，我们对整个进程做一个快照，这个快照说明了在 f 返回后应该要做的计算，它其实就是 f 的 Continuation。在C语言中，我们可以通过int setjmp(jmp_buf env)来捕获在调用setjmp时的 Continuation 并保存在 env 中，而在未来调用void longjmp(jmp_buf env, int val) 会继续进行 setjmp 时保存的 Continuation，并把 val 作为 setjmp 的返回值。看起来就像进行了一次“时空穿越”。 C语言中 setjmp 和 longjmp 的实现很好理解——setjmp 保存当前的 pc 指针以及寄存器的状态，longjmp时复原就行——当然这样也就引出了一个问题：c语言中的 setjmp 并不能真正保存 Continuation，因为 setjmp 不可能将栈上的值也保存了。但 Scheme 中的 Continuation 却是真正的 Continuation Scheme 是通过 call/cc 这一过程起到和C语言中 setjmp 类似却更完备的功能的。call/cc 接收一个一元 lambda 函数，对其求值时相当于将当前的 Continuation传 入该一元 lambda 函数后对该函数体求值。如果一元 lambda 函数正常返回，则 call/cc 过程的返回值就是一元 lambda 函数的返回值。如过在任何地方调用了 Continuation(Continuation 也是一个一元过程)，当前的所要进行的计算就会变成 Continuation 所记录的计算，就好像 call/cc 过程刚刚返回一样，而此时 call/cc 的返回值为调用 Continuation 时传入的参数： (call/cc (lambda (k) (* 5 4))) =\u003e 20 (call/cc (","date":"2023-03-30","objectID":"/posts/scheme%E6%8B%BE%E9%81%97/:0:0","tags":["Scheme","PL","Knowledge"],"title":"Scheme 拾遗：S-Expression，Continuation 以及 Macro","uri":"/posts/scheme%E6%8B%BE%E9%81%97/"},{"categories":["Backend"],"content":" When you are on the dancefloor, there is nothing to do but dance. —— Umberto Eco, *The Mysterious Flame of Queen Loana* 有一道经典的Java面试题： Integer i = 100; Integer j = 100; System.out.print(i == j); // true or false? 由于之前没有准备直接上阵，在碰到这道题时一时不解。但显然这是考察autoboxing(自动装箱)机制，而一般整数转化为Integer对象时肯定是new一个新对象，按理说结果应该为false，但是考虑到在具体实现中可能为了效率考虑会预先缓存一部分整数对象，于是便猜测答案为true。下来网上一查，答案果然为true，但解释却是如下： 这就有点奇怪了——我总觉得缓存多少是具体的编译器或者JVM抑或是相关类库实现的问题，而不该定义在语言规范中，而如果是和具体实现相关的，那么言之凿凿256个数字需要缓存就有点奇怪了。上网查查资料之后，发现256这个数字果然还是Integer类的默认实现中决定的。下面我就综合查找的资料，介绍一下这道题背后可能涉及的知识。 Reference Type 和 Primitive Type 首先Java类型系统以及JVM中，任何 Value(值) 要么是 reference type(引用类型)，要么是 primitive type(基本类型)。所谓reference type就是指向一个对象的值，总是一条new指令的返回值，往往是自身具有一定状态，并可以改变的(mutable)，在JVM实现中为一个指向具体对象的指针；而 primitive type的值 则只能通过字面量或是预先定义的在primitive type上的操作获得，而操作并不改变值本身，只是产生了一个新的值，即是不可变的(immutable)的，在JVM实现中也对应实现语言的primitive type(可以理解为c++中的Int)。 但是在许多其他面向对象语言中(如Python)，一切皆是对象，一切值皆是引用。这样做比Java好理解许多，具体实现上也易于实现了。但是却带来了效率的低下，这主要有两点原因： 对于整数这样的在编程中最基本且常用的值，如果每次两数相加都返回一个新对象，则是对资源的极大浪费。 你可能会想：为什么不把整数对象做成单例的？也就是一个只有一个1对象，也只有一个2对象，每次 1 + 1 总能得到同一个2 —— 但注意reference type的特征就是mutable，如果这样就做不到mutable了，和用primitive type表示整数也就没有区别了。 在JVM执行时，遇到引用需要先解引用才能获取整数值，然后再相加，之后封装成整数对象并返回引用，与直接将整数表示为实现语言的primitive type相比效率天差地别。 因此，为了效率考虑，则需要把整数，浮点数等归入primitive type。这样做就又产生了一个问题：在JVM中，一个值即可能是用实现语言的primitive type表示，也可能用指针表示，我们如何对二者进行区分呢？幸好Java是静态类型语言，在编译之后每条指令所操作的值的类型也是确定的。比如当执行iadd指令时栈上存的值就一定为两个整数。但在动态语言中就不一定了，我们必须要先检查类型是否匹配，然后进行操作。此时为了标明一个值究竟是primitive type还是reference type，我们必须把值的高位留出来作为类型tag以示区分。 // 动态语言虚拟机中的一个值往往是这个样子的 struct Value_t { Type type; union { double number; bool boolean; HeapObject* object; } data; }; Generic 既然有了primitive type的整数，似乎再也不需要作为对象的整数了？看起来似乎是这样，但是Java却依然提供了Integer类表示作为对象的整数——这主要还是因为为了支持泛型的存在。 Java的泛型是通过类型擦除实现的，也就是泛型信息只在编译期可见，而在运行期(也就是JVM)不可见。无论你是写成List\u003cString\u003e 还是 List\u003cA\u003e ，它们在编译后都会变成同一个List，而这个List存储的是Object对象。 // 泛型类： public class Box\u003cT\u003e { private T t; public void set(T t) { this.t = t; } public T get() { return t; } } // 上面的泛型类在编译成字节码后 public class org/example/Box { private Ljava/lang/Object; t public \u003cinit\u003e()V ... public set(Ljava/lang/Object;)V ... public get()Ljava/lang/Object; ... } 因为实际上用Object存储，所以每次get都是做了一次强制类型转换，Java编译器会生成CHECKCAST指令来保证类型转换时的正确性。 // java代码 System.out.println(stringBox.get()); // 编译后的字节码 INVOKEVIRTUAL org/example/Box.get ()Ljava/lang/Object; CHECKCAST java/lang/String INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V 但是如果泛型已经由编译器保证没有类型错误，为什么又要加CHECKCAST指令来运行时再次检查呢？考虑如下泛型程序，在可以通过编译，但在运行时CHECKCAST会报错： public class Main { public static void main(String[] args) { Box stringBox = new Box\u003c\u003e(); stringBox.set(5); foo(stringBox); } public static void foo(Box\u003cString\u003e box) { System.out.println(box.get() + \"123\"); } } //Exception in thread \"main\" java.lang.ClassCastException: class java.lang.Integer cannot be cast to class //java.lang.String (java.lang.Integer and java.lang.String are in module java.base of loader 'bootstrap') // at org.example.Main.foo(Main.java:14) // at org.example.Main.main(Main.java:10) 这是因为Java为了让新代码和没有泛型之前(Java8)的代码兼容，而引入了Raw Type，也就是没有泛型参数的泛型——对这种类型的检查会适当放松——代价就是动态检查的开销。 当然泛型不仅仅可以通过类型擦除实现，像C++的模板就是走向了另外一个极端——为每个类型都生成相应的类——这样的缺点自然是代码膨胀，好处却是可以动态获取类型信息(C++是否支持反射暂且不论，但如果Java采取和C++一样泛型机制就一定会支持这样的反射的)，不需要动态类型检查，也不需要像Java一样的Integer类。 好，话题又回到Java的Integer类，正是应为Java的泛型是通过类型擦除实现，所以所有的泛型在背后都有一套统一的表示——也就是Object。但是primitive type却在Java OO继承链之外，也就无法使用泛型了——所有就引入了将primitive type封装成wrapper type的机制。将int封装为Integer即是一个例子。 Autoboxing and Integer Cache 虽然已经有了wrapper type，但每次手动封装也是非常麻烦的。所以编译就加入了自动检测类型，在合适的时候将primitive type转化为相应的wrapper type，将wrapper type转化为相应的primitive type的机制，也就是“Autoboxing”与“Unboxing” 所以说，面试题中Integer i = 100;等即采用了Autoboxing 机制，上面的代码在编译处理过之后就等价为下面的代码： Integer i = Integer.valueOf(100); Integer j = Integer.valueOf(100); System.out.print(i == j); // true or false? 已知引用比较在java中是直接比较地址，那么我们只需要知道Integer.valueOf做了什么就知道面试题的答案了。 查阅 java doc，发现: public static Integer valueOf(int i) Returns an Integer instance representing the specified int value. If a new Integer instanc","date":"2023-03-07","objectID":"/posts/autoboxing-and-integercache-in-java/:0:0","tags":["Java","Interview","Knowledge"],"title":"Autoboxing and IntegerCache in Java","uri":"/posts/autoboxing-and-integercache-in-java/"},{"categories":["System"],"content":" Surely all this is not without meaning. —— HERMAN MELVILLE, Moby-Dick, 1851 虽然题目起得很大，但是内容也只涉及了编译器实现的一小部分，主要还是一些自己对编译器high level的认识，加上部分cs143实验的总结。 什么是编译器 一个编程语言可以看作一个从它的合法程序集合到运行时行为的total function，设为f。我们编程则是已知运行时行为b，求解出一个合法程序a以使得f(a)=b的过程。函数f一般都为可计算函数，且f对应的计算规则也是比较特殊以使得我们也较易掌握的，具体而言，就是可以采用模块化，分而治之这样的思想来构造出a，这种计算规则常常表现为定义在AST上的求值规则(如opretional semantic)。 确定合法程序的集合的规则一般被称为 syntax 而total function f 一般被称为 semantics 有一部分的编程语言u被称为unsafe的，因为codom(u)往往含有一些我们——编程人员觉得不好的行为。比如该行为取决于目标代码所运行的机器，或是产生该行为的计算规则十分特殊(比如说数组越界不报错而是返回42)。而相应的，codom(u)均是好的行为的编程语言我们称为safe的编程语言。safe语言的例子有java，而c则是一个unsafe的语言。不过在实际编写代码，尤其是系统软件中，就算是safe的语言也往往会进行一些扩展，以允许编写一些与具体机器有关的unsafe操作。 当然编程语言仅仅定义为total function还是不够的，要想让编程语言不仅仅是我们脑海中的方程，我们还需要让机器去运行它。但机器有着自己的语言，我们必须把编程语言翻译为机器语言才可以让机器去运行。我们把机器的语言称为目标语言，其一段程序称为目标代码，而我们的编程语言称为源语言，其一段程序称为源代码。由于目标代码是可以被机器运行并表现出运行时行为，我们也可以把一个编程语言f的值域看作是目标代码的集合。f在计算机上的实现就称作编译器。 但是由于合法程序只是所有字符串的一个子集，而我们可以给编译器输入任何字符串，因此编译器必须还有错误程序检测的功能。因此，我们使用的编译器的一般都会有两项功能，其一是代码形式的转换，其二是拒绝错误的程序。因此我们可以把编译器看作从源代码到目标代码的partial function。 编译器的结构 编译器可以被分为几个相互独立的部分，分别实现，然后用管道组合起来。CS143中cool的实现也是如此：该实验的最后你会获得lexer，parser，semant，cgen四个可执行文件，可以用如下shell脚本组合起来得到一个完整的编译器./lexer $* | ./parser $* | ./semant $* | ./cgen $*。其中 lexer，parser 对应的是语法分析阶段，semant对应的是语义分析阶段，而cgen对应的是代码生成阶段。 除cgen/代码生成之外每个部分/阶段都有两项任务，一是检查并拒绝错误的程序，二是进行代码形式的转换。而cgen只有进行代码形式的转换一个任务，因为在cool以及大部分编译器的设计中，代码生成阶段都假定输入已经是合法的程序。 每一阶段的工作可以看作一个数学函数f(X-\u003eY)，定义域X为该阶段能处理的输入,Y为可能的输出的集合。假如lexer，parser，semant，cgen都看作数学函数，则compiler = cgen ∘ semant ∘ parser ∘ lexer。具体来说， dom(lexer) = {文本程序} codom(lexer) = {token流} dom(parser) = {token流} codom(lexer) = {语法树} dom(semant) = {语法树} codom(lexer) = {装饰后的语法树(主要指添加类型信息)} dom(lexer) = {装饰后的语法树} codom(lexer) = {目标代码} 但是，值得注意的是，这里的函数都是 **partial function，**即并非每个定义域的值都有陪域的值与之对应，这些值就代表这编译过程的发现的错误程序，编译器会报错并提示用户修改。 之所以一个compiler要被拆分为几个独立的部分，是因为这些部分的功能都相当内聚，互相之间联系较少。从错误处理的角度来看，lexer是用正则表达式排除错误程序，而parser是用CFG排除错误程序，semant做的事情则比较复杂，采用了多种技术来排除掉错误程序，并输出合法程序。 我们又常常说 由 lexer 和 parser 定义了语法规则，通过lexer和parser检查的是满足语法规则的程序，这个阶段所以被称为语法分析阶段 semant定义了静态语义规则，通过semant检查的是满足静态语义规则的程序，这个阶段所以被称为语义分析阶段。 语义分析阶段的任务 因此，semant做的事被称为语义分析，它会做许多检查：比如说检查标识符必须先声明再使用，类继承图必须无环之类的。但是语义分析阶段做的最重要的一件事则是类型检查，其目的是检查程序满足type rules，而type rule即是type system的主要组成部分。不过类型检查也不必非得在语义分析阶段做，有的语言也将类型检查纳入运行时，具体类型系统做了什么样的检查，这还得要从编程语言的求值规则说起。 编程语言写成的程序虽然是字符串，但这个字符串却实际上表示了一种树型结构，我们把这个树型结构叫做对应字符串的语法树。为什么要用树型结构？因为树型结构的表达能力足够强，也是易于理解的。我们所能构建的大部分复杂事物往往都只采用了组合和抽象两种手段。组合即是将基本的事物放在一起，而抽象指将许多事物忽略其细节，只关注其整体作用。如在Scheme中，基本的元素就是整数字面量，而组合的方法则有+, -, *, /等运算，而抽象则是函数。 一般来说，语法分析阶段只负责保证程序确实表达了一个树型结构即可 剩下的检查都是语义分析的任务了 但并非任意种类元素都能组合，抽象，一种运算可能只能组合某种特定类型的元素，比如说 1 + 2 就是对的，但是 1 + \"2\"就是不对的(不对指在程序员看来，这是无意义的。假如程序员真的写出了这样的程序，则他会希望编译期或是运行时报错，而非一声不吭执行下去得到一个奇怪的结果)。我们可以给语法树的每个节点都赋予一个类型，并定义类型推导的规则，这些推导规则的集合就叫做该语言的类型系统。 上面说的检查过程就叫做类型检查，检查程序是否满足类型系统的规则，是语义分析中最为重要的一个部分。在程序员眼中，类型检查如果静态能做自然好，但是动态报错也是无妨的——只要不一声不吭地执行就算是好的行为，动态类型检查的语言也可以是safe的。虽然动态进行检查会导致程序运行时错误增多，但是却增加了许多灵活性。 一般在编译器语义分析过程中进行类型检查的语言称为静态语言，而动态类型检查的语言称为动态语言。 另外我们常常会用到强类型语言与弱类型语言两个词，这两词的含义并未明确定义，而是依赖于语境。一般来说，如果一个语言不会因类型错误而产生unsafe行为，则称为强类型语言，反之则称为弱类型语言。 代码生成阶段的任务 通过语义检查的程序都是合法的程序，下一步就应该将合法的程序转化为目标代码了。但值得注意的是，lexer，parser以及semant中也都进行了代码形式的转换，将程序代码从字符流转化为了语法树(可能还伴随着符号表的填充)。和错误处理一样，lexer也是通过正则表达式来讲字符串识别为一个个token，parser也是通过CFG从token流中构建语法树。而semant只是遍历已建好的语法树并收集信息。lexer和parser合称为语法分析，在许多编译器实现中两个阶段是紧密耦合在一起的，Antlr，Pest等语法分析工具也是同时做了lexer和parser的事情，你只需要向这些工具中输入语言语法的specification，这些工具就会自动生成将字符串转化为语法树的代码。这些语法分析工具背后的理论则涉及形式语言与自动机，可以说是计算机理论在实际软件编写中非常成功的应用了。 与语法分析阶段，代码生成阶段往往没有如此自动化的工具或是普遍性的理论支撑——当然语言的语义是有operational semantics,denotational semantics 等形式化方法定义的，但是就算是用最贴近实现的operational semantics 来指导实现，我们所得到的也只能是一个效率低下的解释器。因为这些形式化的语义定义往往是直接在AST上定义，以AST层面的概念为程序状态，而我们想要的是将AST直接映射到一种低级语言，这往往是需要程序员自己去思考如何实现的。 将高级语言的许多概念，如类，对象的创建，控制流等映射到低级语言的过程在我看来是一件非常神奇的事情。在之前我也曾学过汇编语言，学过一些高级语言，但二者之间如何进行转换确实在我学习编译原理之前万万没有想到的。下面就拿cool举个例子： class A { method1() : SELF_TYPE { // 1 self }; method2() : SELF_TYPE { // 2 self }; }; class B inherits A { method1() : SELF_TYPE { // 3 self }; method3() : SELF_TYPE { // 4 self }; }; /////////////////////// 5 le","date":"2023-03-06","objectID":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E4%BB%BB%E5%8A%A1/:0:0","tags":["Compiler","Knowledge"],"title":"编译器的结构与任务","uri":"/posts/%E7%BC%96%E8%AF%91%E5%99%A8%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E4%BB%BB%E5%8A%A1/"},{"categories":["System"],"content":" You are my creator, but I am your master; Obey! —— Mary Shelley, Frankenstein 导言 操作系统大体来说即是“管理软/硬件资源，为程序提服务”的程序。我们一方面可以从应用的视角把操作系统当作服务的提供者，去学习如何使用操作系统提供的API，如POSIX或是windows api，另一方面也可以深入其内部去学习它的实现。操作系统的实现紧密围绕着管理资源与提供服务两项任务展开，而对硬件资源的管理是实现中最为“dirty”的一部分。为了能屏蔽这些琐碎的硬件细节，让我们把精力集中在操作系统中各个对象的交互上去，jyy通过AM提供了一组API，抽象出了硬件提供的服务，从而把OS真正变成了一个普通的C程序。 可是屏蔽太多的细节也会让人感到不安与心虚——内存的地址空间到底是怎样被决定的？页表究竟是怎样实现的？各个处理器上执行流刚刚开始时的栈又是谁决定的？当然，OS作为ICS的后继课程，是默认我们对于计算机体系结构有一定了解的，上述问题在ICS的PA实验中也都有答案——理论上在PA中我们应该已经自己实现了从硬件模拟器到AM再到简易操作系统的所有内容，对于OS和硬件如何交互应该再熟悉不过了——但是，对于我这个在PA后期全程摆烂的人来说，这些问题始终是模模糊糊不知所云的。“欠下的债终究是要还的”，所以我的OS就stuck住了，所以我才会在这里从xv6和riscv入手看看OS是究竟如何在硬件上运行起来的。 qemu vitr OS终究是要运行在硬件上的，而qemu提供了对硬件的模拟。由于我选取的是riscv64版本的xv6，因此运行xv6的qemu必须模拟基于riscv64架构的机器。 可以看到qemu提供了对于多种机器的模拟，其中xv6所使用的是virt，qemu-virt是一个虚拟的机器，该机器是仿照真实世界中的 SiFive FU540-C000 开发的。 先不管各种设备与该机器的具体结构，对于一个操作系统来说我们最关心的是cpu reset后的状态，包括内存空间的分配与各个寄存器的值。SiFive FU540-C000 Manual的第五章就介绍了memory map。当然，如果觉得manual太过难读，在运行时通过qemu monitor亲眼看看机器的状态也是一种选择。 我们可以在qemu中使用-S 选项令qemu在初始化完成 (CPU Reset) 后暂停 我们运行qemu时使用的命令如下： # 各个选项意义可RTFM https://www.qemu.org/docs/master/system/riscv/virt.html qemu-system-riscv64 -machine virt -bios none -kernel kernel/kernel # 通过-kernel 选项直接把可执行文件加载到内存空间中 -m 128M # 分配128M RAM -smp 1 -nographic -monitor telnet:127.0.0.1:55555,server,nowait -drive file=fs.img,if=none,format=raw,id=x0 -device virtio-blk-device,drive=x0,bus=virtio-mmio-bus.0 -S -gdb tcp::26003 之后进入qemu monitor中查看memory map和寄存器状态。 可以发现，在CPU reset后，除pc值为0x1000外，其余寄存器值均为0x0。而在第一张图中可以发现内存的0x1000 到 0x11fff通过mmio映射到了rom中， 通过gdb检查内存可以发现，rom中存储了以上几条指令。 通过这几条指令，pc将跳转到0x80000000处开始执行，而从memory map中也可以得知0x80000000是我们为virt分配的 128M 内存的开始，我们所写OS的第一条指令也位于这个位置。 kernel的链接 上一节说到，我们的OS的第一条指令位于0x80000000处，而kernel是被当作可执行文件通过-kernel选项直接加载进地址空间的。 通过readelf命令查看kernel的信息如下： 可以看到到，加载位置的确是写在elf文件中的，qemu也确实忠实地按照elf的说明加载了可执行文件kernel。 但是为什么kernel会存储这样的信息呢？是谁决定Entry Point的位置，是谁决定的VirtAddr与Size呢？这些都是通过 -T kernel/kernel.ld为链接器指定的linkScript确定。 （另外，默认的linkScript可以通过ld --verbose查看） // kernel/kernel.ld OUTPUT_ARCH( \"riscv\" ) ENTRY( _entry ) SECTIONS { /* * ensure that entry.S / _entry is at 0x80000000, * where qemu's -kernel jumps. */ . = 0x80000000; .text : { *(.text .text.*) . = ALIGN(0x1000); _trampoline = .; *(trampsec) . = ALIGN(0x1000); ASSERT(. - _trampoline == 0x1000, \"error: trampoline larger than one page\"); PROVIDE(etext = .); } .rodata : { . = ALIGN(16); *(.srodata .srodata.*) /* do not need to distinguish this from .rodata */ . = ALIGN(16); *(.rodata .rodata.*) } .data : { . = ALIGN(16); *(.sdata .sdata.*) /* do not need to distinguish this from .data */ . = ALIGN(16); *(.data .data.*) } .bss : { . = ALIGN(16); *(.sbss .sbss.*) /* do not need to distinguish this from .bss */ . = ALIGN(16); *(.bss .bss.*) } PROVIDE(end = .); } riscv特权模式 在我们大体明白程序是如何被编译链接以及被加载到内存中之后，就可以开始去一行行读代码了。但在这之前，我们还得要清楚所谓对于硬件的控制最终是要通过机器代码来实现的，由于汇编代码和机器代码有着良好的对应关系，为了精准地控制硬件我们不得不用到一些汇编代码，这就要求我们对于riscv比较熟悉了。 riscv的规范详见(Volume I, Volume II)，其中卷一定义了实现通用计算的一些指令和寄存器，而卷二则定义了一些特权指令和CSR寄存器(Control and Status Registers)。通用指令不需多说，无非是内存访问与计算之类。而特权指令和CSR寄存器则既是实现OS所必须的，又是我所不熟悉的。 riscv定义了三种特权模式 —— user mode，supervisor mode，以及machine mode。在三种不同的特权模式下运行的代码也对硬件有着不同的控制权限，更高一级的级别能进行低级别的所有操作，反之不行。我们的用户程序一般运行在user mode中，而OS内核一般运行在supervisor mode中，machine mode是CPU reset之后的模式，仅用来做一些初始化配置。（部分嵌入式的riscv实现可能只支持machine mode，或者只支持machine mode和user mode，但现代化的操作系统一般都需要supervisor mode的支持） riscv定义了一组CSR寄存器，我们可以通过对CSR寄存器的读写来控制机器的状态。每种特权模式下都有自己对应的一组寄存器。由于寄存器数目较多，也并不需要全部理解，所以我们可以从阅读xv6代码开始，遇到没有见过的寄存器就去查阅手册，按需学习。 初始化 # qemu -kernel loads the kernel at 0x80000000 # and causes each CPU to jump there. # kernel.ld causes the following code to # be placed at 0x80000000. .section .text .global _entry _entry: # set up a stack for C. # stack0 is declared in start.c, # with a 4096-byte stack per CPU. # sp = stack0 + (hartid * 4096) la sp, stack0 li a0, 1024*4 csrr a1, mhartid addi a1, a1, 1 mul a0, a0, a1 add sp, sp, a0 # jump to start() in start.c call start spin: j spin xv6执行的第一段代码位于entry.s ，此时还处于machine mode。其中mhartid寄存器","date":"2022-08-11","objectID":"/posts/%E6%8E%A2%E7%A9%B6%E6%94%AF%E6%92%91os%E7%9A%84%E7%A1%AC%E4%BB%B6%E4%BB%A5xv6%E5%92%8Criscv%E4%B8%BA%E4%BE%8B/:0:0","tags":["OS","Reflection","Knowledge"],"title":"探究支撑os的硬件(以xv6和riscv为例)","uri":"/posts/%E6%8E%A2%E7%A9%B6%E6%94%AF%E6%92%91os%E7%9A%84%E7%A1%AC%E4%BB%B6%E4%BB%A5xv6%E5%92%8Criscv%E4%B8%BA%E4%BE%8B/"},{"categories":["Algorithm"],"content":" 凡治众如治寡，分数是也；斗众如斗寡，形名是也。 本篇博客内容大多都来自Jeff的算法书籍，因书籍内容充实和有趣，读之后又怕忘记，因此摘抄复述自己感觉有趣内容，并适时加以扩展。 Intro 在算术中，乘法是最基本的运算。数学往往只关心抽象的一般的东西，它只把数字看作数字本身，只把乘法看作为一种定义在数上的满足某种特定性质的运算，但是为了让数字能真正为我们所用，我们还必须定义数字实际上的表示方法，以及在一种表示方法下对数字进行运算的方法(比如乘法)——按照这种方法，任意给定两个数，我们都能得出其运算结果(乘积)。这样一种确定性的方法就可以称作一种算法。 通俗意义上的算法即是指一系列明晰确定的指令(步骤)的序列，它描述了一个问题的可行解决方案。注意到组成算法的是一系列明晰确定的指令，我们把这些指令叫做原子指令，所谓原子就是指这些指令是最简单的，不可再分的指令了。如果要使算法是真正可行的，我们还必须确保原子指令是可行的。如果有一个算法，描述了如何成为富翁。而这个算法的一个原子指令是“先定一个小目标，我先挣它一个亿！”，那么这个算法对于我来说是显然是不可行的了😂。 首先，如果数的表示方法为十进制整数，而个位数相乘或者相加都为原子指令的话，竖式计算法当然就是一种最好不过的算法了。 其次，如果把数字定位为线段的长度，原子指令定为基本的尺规作图步骤的话，一种乘法的算法就是如下这样。 当然，我们也需要有一些评价这些算法的指标以便在解决同一问题的不同算法间取舍。最重要的指标当然是算法正确性了，另外，算法执行的快慢也是一个很重要的指标，它可以用一个算法从开始到结束所执行的原子指令条数来衡量。由于计算指令条数的精确数目较为繁琐且意义不大，时间复杂度也就随之而出了。 在计算n位数×n位数时，在使用十进制数字表示和竖式算法时，时间复杂度为$O(n^2)$，因为第一个数的每一个数字都要和第二个数的每一个数字相乘；使用几何模型和上图所述算法时，由于只需要有限个步骤，算法时间复杂度就是$O(1)$了。 考虑到无论我们实际日常使用还是计算机表示，使用X进制表示法都是最常见的，把个位数的×和＋都作为原子指令也是最自然的，我们的研究对象也就主要集中在这种计算模型上了。那么问题来了，计算乘法的算法多种多样，有没有一种算法能以低于$O(n^2)$的时间复杂度来计算乘法呢？ SplitMultiply 分而治之的思想在算法上的应用往往能得很好的效果。比如说利用了分治法的Quick-Sort, Merge-Sort都能得到很好的时间复杂度。相应的，也许分治的思想也能在乘法中起到作用。 很显然，$(10^ma + b)(10^mc + d) = 10^{2m}ac + 10^m(bc + ad) + bd$，按照这个分解，就有了如下的分治算法： 这个算法的正确性显而易见，但是要计算这样的递归算法的时间复杂度就较为困难了。不过，其用时的递推式却很容易写出来，即$T(n)=4T(n/2)+O(n)$，而要根据这个递推式求出$T(n)$的渐进式，递归树法能给我们很多Insight。 将全部项求和，很容易得出时间复杂度任然为$O(n^2)$。 但是，为什么Quick-Sort, Merge-Sort都可以成功降低复杂度呢？这可以从二者用时的递推式中看出了:$T(n)=2T(n/2)+O(n)$。 再考虑递归树中的耗时大概分为两大部分，一部分是每次递归时的耗时，一部分是所有叶节点对应的最小子情况的耗时。假设递推式为$T(n)=aT(n/b)+f(n)$，不妨设当$n$为1时到达子节点，树高为H，叶节点数为L，则有$n/b^H=1 \\implies H=log_bn,L=b^H=a^{log_bn}=n^{log_ba}$,因此，叶节点对应的项的代价为为$\\Theta(n^{log_ba})$,而内部节点对应的代价为$\\Sigma_{j=0}^{log_bn-1}a^jf(n/b^j)$，因此，整个递归过程的用时就由这两项决定。由这两项的相对大小就可以得到整个过程的渐进复杂度。在对这两项进行分析之后，就得到了主定理，之后就可以以此为出发点分析算法的时间复杂度了。 SplitMultiply算法符合情况一，也就是由于SplitMultiply算法的递归过程中每层节点扩展得太快，导致叶节点完全占据了主导地位，因此复杂度完全由叶节点决定；而两个分治的排序算法都符合情况二，即每层节点代价的总和差不多相同，因此最终时间复杂度为$\\Theta(nlogn)$。 FastMultiply SplitMultiply的失败之处在于每层节点扩展得太快，即$log_ba$太大，因此有没有一种适用于乘法的分治算法使得$log_ba$较小呢？事实上，Karatsuba就把a从4降到了3，从而使得乘法的时间复杂度由$n^2$降至$n^{log_23}=n^{1.58496……}$。 Karatsuba的想法主要来源于他发现，上面的分解式中$bc+ad$是一个整体，如能一次就把这个整体算出来，就能只调用三个递归子过程。考虑到$ac+ab-(a-c)(c-d)=bc+ad$，我们只需要额外计算一个$(a-c)(c-d)$即可获得中间的$bc+ad$的值。（注意到$a \\times 10^m$的复杂度实际上为$O(n)$） 具体算法如下： Karatsuba在发现这个算法时还是一个23岁的学生。1950年代，苏联数学家Kolmogorov举办了一个研讨会，提出“任何n*n的乘法算法都不可能在$n^2$的时间复杂度以下”，可是在一周之后，Karatsuba就发现了这个算法。 ","date":"2022-03-20","objectID":"/posts/multiplication-part1/:0:0","tags":["Book"],"title":"Multiplication","uri":"/posts/multiplication-part1/"},{"categories":["System"],"content":"int setjmp(jmp_buf env) void longjmp(jmp_buf env, int val) setjmp 和 longjmp 是setjmp.h定义的相互协作的一组跳转函数。 调用 setjmp 时可以将当前的环境保存在一个jmp_buf类型的变量中，之后调用 longjmp 后会跳转到 setjmp 执行后的下一条语句执行，就好像刚刚从 setjmp返回一样。 函数行为描述见man，源码见glibc。 其中,jmp_buf的定义如下: typedef long int __jmp_buf[8]; /* Calling environment, plus possibly a saved signal mask. */ struct __jmp_buf_tag { /* NOTE: The machine-dependent definitions of `__sigsetjmp' assume that a `jmp_buf' begins with a `__jmp_buf' and that `__mask_was_saved' follows it. Do not move these members or add others before it. */ __jmp_buf __jmpbuf; /* Calling environment. */ int __mask_was_saved; /* Saved the signal mask? */ __sigset_t __saved_mask; /* Saved signal mask. */ }; typedef struct __jmp_buf_tag jmp_buf[1]; 本来预想jmp_buf应该是简单的一个存储寄存器信息的数组，却发现其定义较为复杂。在阅读其定义的时候，又牵扯出了许多不熟悉的c知识点。试解析定义如下： 其中typedef struct __jmp_buf_tag jmp_buf[1]定义了一个名为jmp_buf的变量类型,它实际上是一个大小为1的struct __jmp_buf_tag数组。而结构体struct __jmp_buf_tag包含三个成员，后两个与信号机制有关，不做讨论。第一个成员为__jmp_buf类型，用来保存寄存器信息。而__jmp_buf类型实际上是一个大小为8的long int数组。 那么为什么要把实际上存储信息的结构体__jmp_buf_tag包含在一个数组里面呢？也许是因为将数组当作参数传递时总是传递数组的地址，而将结构体当作参数传递时却总是将整个结构体的值赋值一遍传给被调用函数。我们的jmp_buf作为一个在函数调用间保存信息的实体应该满足数组的特征，因此将其定义为数组更合适一些。当然，如果不这样做，每次被调用函数需要结构体__jmp_buf_tag时传入它的指针也是可行的，只是略显麻烦罢了。 hint: 结构体定义了一种变量类型，作为一个整体复制和赋值。在行为上更加类似于int而非int[]; 变量名是与值绑定的符号，而指针是与一个地址值绑定的符号。 ","date":"2021-11-14","objectID":"/posts/%E6%B5%85%E6%9E%90jump-buf%E7%9A%84%E5%AE%9A%E4%B9%89/:0:0","tags":["C","Knowledge"],"title":"浅析jmp_buf的定义","uri":"/posts/%E6%B5%85%E6%9E%90jump-buf%E7%9A%84%E5%AE%9A%E4%B9%89/"},{"categories":["Algorithm"],"content":"由于树本身定义的递归性，置于树上的操作往往也是递归性的的。 在某些语言中，递归是自然的，最基本的语言要素(比如说scheme)，然而在另外一些语言中，递归却不是最基本的要素。 图灵丘奇论题证明了图灵机和lambda演算的等价性，既然纯递归的lambda演算和给人毫无递归印象的图灵机的计算能力是相同的，那么一切递归方法自然都能用非递归方法模拟了。考虑到现实计算机中递归函数的调用就是通过栈实现的，因此我们可以在任何一门语言简单地利用栈来模拟递归。因此，对于树的任何递归操作都有与之对应的非递归方法了(尽管这种非递归方法任然是模拟递归的)。 树的定义以及构造方法如下(用节点Node来表示树，用树的表达式字符串来构造树): public class Node\u003cItem\u003e { Item item; Node left; Node right; Node(Item item, Node left, Node right) { this.item = item; this.left = left; this.right = right; } /** * @param tree 树的表达式 * 形如:\"1(5(6(3,2),),5(5,3(1,)))\" * \"1(1(1(1(1,),),),)\" * @return 树的头节点 */ public static Node makeTree(String tree) { if (tree == \"\") return null; if (tree.length() == 1) return new Node(Integer.valueOf(tree), null, null); char[] t = tree.toCharArray(); int item = Integer.valueOf(tree.substring(0, 1)); int mid = 0; int bra = 0; for (int i = 2; i \u003c t.length; i++) { if (t[i] == '(') bra++; else if (t[i] == ')') bra--; else if (t[i] == ',') { if (bra == 0) { mid = i; break; } } } Node left = makeTree(tree.substring(2, mid)); Node right = makeTree(tree.substring(mid + 1, tree.length() - 1)); return new Node(item, left, right); } } 对于栈中每一个frame的模拟如下: static class frameSim { int retAddr; Node t; frameSim(int retAddr, Node t) { this.retAddr = retAddr; this.t = t; } } 前序，中序，后序的递归以及非递归遍历方法如下： 其中用switch case语句模拟地址跳转。 public static void preOrder(Node t) { if (t == null) { return; } System.out.print(t.item); preOrder(t.left); preOrder(t.right); } public static void preOrderNonRec(Node t) { Stack\u003cframeSim\u003e stack = new Stack\u003c\u003e(); frameSim current = new frameSim(-1, t); int pc = 0; while (true) { switch (pc) { case 0: System.out.print(current.t.item); case 1: if (current.t.left != null) { stack.push(current); current = new frameSim(2, current.t.left); pc = 0; continue; } case 2: if (current.t.right != null) { stack.push(current); current = new frameSim(3, current.t.right); pc = 0; continue; } case 3: } pc = current.retAddr; if (pc == -1) break; current = stack.pop(); } } public static void inOrder(Node t) { if (t == null) { return; } inOrder(t.left); System.out.print(t.item); inOrder(t.right); } public static void inOrderNonRec(Node t) { Stack\u003cframeSim\u003e stack = new Stack\u003c\u003e(); frameSim current = new frameSim(-1, t); int pc = 0; while (true) { switch (pc) { case 0: if (current.t.left != null) { stack.push(current); current = new frameSim(1, current.t.left); pc = 0; continue; } case 1: System.out.print(current.t.item); case 2: if (current.t.right != null) { stack.push(current); current = new frameSim(3, current.t.right); pc = 0; continue; } case 3: } pc = current.retAddr; if (pc == -1) break; current = stack.pop(); } } public static void postOrder(Node t) { if (t == null) { return; } postOrder(t.left); postOrder(t.right); System.out.print(t.item); } public static void postOrderNonRec(Node t) { Stack\u003cframeSim\u003e stack = new Stack\u003c\u003e(); frameSim current = new frameSim(-1, t); int pc = 0; while (true) { switch (pc) { case 0: if (current.t.left != null) { stack.push(current); current = new frameSim(1, current.t.left); pc = 0; continue; } case 1: if (current.t.right != null) { stack.push(current); current = new frameSim(2, current.t.right); pc = 0; continue; } case 2: System.out.print(current.t.item); case 3: } pc = current.retAddr; if (pc == -1) break; current = stack.pop(); } } ","date":"2021-11-09","objectID":"/posts/%E6%A0%91%E7%9A%84%E9%9D%9E%E9%80%92%E5%BD%92%E9%81%8D%E5%8E%86%E7%94%A8%E6%A0%88%E6%A8%A1%E6%8B%9F%E9%80%92%E5%BD%92/:0:0","tags":["Knowledge"],"title":"树的非递归遍历—用栈模拟递归","uri":"/posts/%E6%A0%91%E7%9A%84%E9%9D%9E%E9%80%92%E5%BD%92%E9%81%8D%E5%8E%86%E7%94%A8%E6%A0%88%E6%A8%A1%E6%8B%9F%E9%80%92%E5%BD%92/"},{"categories":["Backend"],"content":" 万物皆流，无物常驻 导引 ","date":"2021-08-18","objectID":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/:0:0","tags":["Java","Stream","Knowledge"],"title":"浅谈java8中的流","uri":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/"},{"categories":["Backend"],"content":"过程的抽象 流(stream)是在java8中出现的一种新的数据抽象，它对数据的处理有着较大的简化作用。 流的概念可能最早来自于列表(List)，列表可以理解为按顺序排列的一组对象(数组和链表都是其具体实现)。 大多数程序的最外在特征是给定一个输入后，按照某种规则得出相应的输出。编写由输入到输出的规则就是programmer所做的事情了。许多程序的规则都可以被抽象为三部分: 根据输入产生一组数据 对第一步产生的数据组进行处理 对处理过后的数据约简而得到最终的输出 当然，最后约简的操作也可以算作数据处理的一部分。但由于它是最后一步操作，所以往往将它独立出来。 这种抽象可以类比为国家选拔人才的机制。 随着形式的变化，国家向大学提出了向H部门输送X专业的高级人才的要求。(这相当于用户输入) 大学招收了一群X专业的本科新生。(产生了一组数据) 大学对这些新生进行专业教育，淘汰掉挂科的学生。(处理数据) 毕业之时，将成绩优异的学生推荐给H部门。(约简得到输出) 也许正因为这种对过程的抽象方式天然地存在于人的大脑结构之中(是某种先验的思维模式)，我们才会很自然地将无论是社会还是计算机中的许多过程都按照这种方式进行抽象。 ","date":"2021-08-18","objectID":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/:1:0","tags":["Java","Stream","Knowledge"],"title":"浅谈java8中的流","uri":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/"},{"categories":["Backend"],"content":"从List到Stream 程序的三部分抽象中有两个关键部分:一是如何表示数据，二是如何处理数据。 对于数据的表示，我们很自然地会想到使用List这样的计算机能支持的最简单数据集合来表示。 对于数据的处理方式，我们抽象出了许多种类，比如说: map: 对于List中的每一项数据都进行某种操作 filter: 删除List中某些不需要的元素 count: 得到List中总的元素数目 有些处理方式(比如说count)，对List操作之后得到的并不是List，不再能连续地进行下一步操作，所以只能作为最后一步约简地处理方式。 然而用一般计算机语言中的List表示数据组，却有以下两种缺点: 一，不能表示无限数据组 二，每次处理都必须对每个元素都进行处理，造成了资源的浪费。(但实际上我们的程序可能只需要处理前几个数据就可以得出结果了) 因此，出现了一种新的数据抽象，流(stream)。流的主要特征即是惰性求值。而惰性求值很好地避免了以上两个问题。所谓惰性求值，即需要的时候再进行求值。 比方说我们的数据组是一串5个白色乒乓球。要对这些乒乓球进行如下处理，首先是染蓝色颜料，其次染黄色颜料，最后我们要拿到第二个染色后的乒乓球。按照List的处理逻辑，我们要先把所有的球染成蓝色，然后将所有的球染成黄色，最后再取出第二个球。但是按照stream的处理逻辑，我们首先知道了要把球 染成蓝色，但我们先记住这个命令，却不实际操作。然后记住要染黄色的命令，也不实际操作。在最后一步，我们要拿出第二个染色后的球。这时候我们再依次对这些球进行处理。先处理完第一个球，然后处理第二个球，这时直接拿出第二个球即可， 而不需要对剩余球进行染色。 此处笔者自感表达不清，关于stream的解释详见SICP3.5。 ","date":"2021-08-18","objectID":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/:2:0","tags":["Java","Stream","Knowledge"],"title":"浅谈java8中的流","uri":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/"},{"categories":["Backend"],"content":"stream API 由于stream的强大抽象能力，java8中新引入了stream API。java8中的stream即是上述概念模型的一种实现，并无特殊性。其主要操作自然也是分为stream的构造，处理以及约简三部分。下面三部分将分别记录常用的API。 构造 ","date":"2021-08-18","objectID":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/:3:0","tags":["Java","Stream","Knowledge"],"title":"浅谈java8中的流","uri":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/"},{"categories":["Backend"],"content":"由collection或Array转化 Collection: default Stream\u003cE\u003e stream() Array: public static \u003cT\u003e Stream\u003cT\u003e stream(T[] array) public static \u003cT\u003e Stream\u003cT\u003e stream(T[] array, int startInclusive, int endExclusive) public static IntStream stream(int[] array) public static IntStream stream(int[] array, int startInclusive, int endExclusive) 以及类似的DoubleStream和LongStream方法 ","date":"2021-08-18","objectID":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/:4:0","tags":["Java","Stream","Knowledge"],"title":"浅谈java8中的流","uri":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/"},{"categories":["Backend"],"content":"由Stream直接创建 Stream: static \u003cT\u003e Stream\u003cT\u003e empty() Returns an empty sequential Stream. static \u003cT\u003e Stream\u003cT\u003e of(T t) Returns a sequential Stream containing a single element. static \u003cT\u003e Stream\u003cT\u003e ofNullable(T t) Returns a sequential Stream containing a single element, if non-null, otherwise returns an empty Stream. @SafeVarargs static \u003cT\u003e Stream\u003cT\u003e of(T... values) Returns a sequential ordered stream whose elements are the specified values. static \u003cT\u003e Stream\u003cT\u003e iterate(T seed, UnaryOperator\u003cT\u003e f) Returns an infinite sequential ordered Stream produced by iterative application of a function f to an initial element seed, producing a Stream consisting of seed, f(seed), f(f(seed)), etc. static \u003cT\u003e Stream\u003cT\u003e generate(Supplier\u003c? extends T\u003e s) Returns an infinite sequential unordered stream where each element is generated by the provided Supplier. This is suitable for generating constant streams, streams of random elements, etc. static \u003cT\u003e Stream\u003cT\u003e concat(Stream\u003c? extends T\u003e a, Stream\u003c? extends T\u003e b) Creates a lazily concatenated stream whose elements are all the elements of the first stream followed by all the elements of the second stream. 另外也可以通过streamBuilder类创建stream 处理 Stream: Stream\u003cT\u003e filter(Predicate\u003c? super T\u003e predicate) Returns a stream consisting of the elements of this stream that match the given predicate. This is an intermediate operation. \u003cR\u003e Stream\u003cR\u003e map(Function\u003c? super T,​? extends R\u003e mapper) Returns a stream consisting of the results of applying the given function to the elements of this stream. This is an intermediate operation. Stream\u003cT\u003e limit(long maxSize) Returns a stream consisting of the elements of this stream, truncated to be no longer than maxSize in length. This is a short-circuiting stateful intermediate operation. Stream\u003cT\u003e skip(long n) Returns a stream consisting of the remaining elements of this stream after discarding the first n elements of the stream. If this stream contains fewer than n elements then an empty stream will be returned. This is a stateful intermediate operation. Stream\u003cT\u003e sorted() Returns a stream consisting of the elements of this stream, sorted according to natural order. If the elements of this stream are not Comparable, a java.lang.ClassCastException may be thrown when the terminal operation is executed. For ordered streams, the sort is stable. For unordered streams, no stability guarantees are made. This is a stateful intermediate operation. Stream\u003cT\u003e sorted(Comparator\u003c? super T\u003e comparator) Returns a stream consisting of the elements of this stream, sorted according to the provided Comparator. For ordered streams, the sort is stable. For unordered streams, no stability guarantees are made. This is a stateful intermediate operation. 约简 Stream: void forEach(Consumer\u003c? super T\u003e action) Performs an action for each element of this stream. This is a terminal operation. Optional\u003cT\u003e findFirst() Returns an Optional describing the first element of this stream, or an empty Optional if the stream is empty. If the stream has no encounter order, then any element may be returned. This is a short-circuiting terminal operation. Optional\u003cT\u003e max(Comparator\u003c? super T\u003e comparator) Returns the maximum element of this stream according to the provided Comparator. This is a special case of a reduction. This is a terminal operation. Optional\u003cT\u003e min(Comparator\u003c? super T\u003e comparator) Returns the minimum element of this stream according to the provided Comparator. This is a special case of a reduction. This is a terminal operation. T reduce(T identity, BinaryOperator\u003cT\u003e accumulator) Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; but is not constrained to execute sequentially. The identity value must be an identity for the accumulator function. This means that for all t","date":"2021-08-18","objectID":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/:5:0","tags":["Java","Stream","Knowledge"],"title":"浅谈java8中的流","uri":"/posts/%E6%B5%85%E8%B0%88java8%E4%B8%AD%E7%9A%84%E6%B5%81/"}]