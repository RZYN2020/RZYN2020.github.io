<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>深度学习基础知识总结 - Ekstasis&#39;s Blog</title><meta name="Description" content="Ekstasis&#39;s Blog"><meta property="og:url" content="https://rzyn2020.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">
  <meta property="og:site_name" content="Ekstasis&#39;s Blog">
  <meta property="og:title" content="深度学习基础知识总结">
  <meta property="og:description" content="Ekstasis&#39;s Blog">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-01-13T19:29:29+08:00">
    <meta property="article:modified_time" content="2025-01-13T19:29:29+08:00">
    <meta property="article:tag" content="DeepLearning">
    <meta property="article:tag" content="Summary">
    <meta property="og:image" content="https://rzyn2020.github.io/images/yinyang.webp">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://rzyn2020.github.io/images/yinyang.webp">
  <meta name="twitter:title" content="深度学习基础知识总结">
  <meta name="twitter:description" content="Ekstasis&#39;s Blog">
<meta name="application-name" content="Ekstasis">
<meta name="apple-mobile-web-app-title" content="Ekstasis"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://rzyn2020.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/" /><link rel="prev" href="https://rzyn2020.github.io/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/" /><link rel="next" href="https://rzyn2020.github.io/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "深度学习基础知识总结",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/rzyn2020.github.io\/posts\/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93\/"
        },"genre": "posts","keywords": "DeepLearning, Summary","wordcount":  7320 ,
        "url": "https:\/\/rzyn2020.github.io\/posts\/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93\/","datePublished": "2025-01-13T19:29:29+08:00","dateModified": "2025-01-13T19:29:29+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Ekstasis"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Ekstasis&#39;s Blog"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="./images/yinyang.webp"
        data-srcset="./images/yinyang.webp, ./images/yinyang.webp 1.5x, ./images/yinyang.webp 2x"
        data-sizes="auto"
        alt="./images/yinyang.webp"
        title="./images/yinyang.webp" />Ekstasis&#39;s Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> All Posts </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/tags/"> Tags </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Ekstasis&#39;s Blog"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="./images/yinyang.webp"
        data-srcset="./images/yinyang.webp, ./images/yinyang.webp 1.5x, ./images/yinyang.webp 2x"
        data-sizes="auto"
        alt="./images/yinyang.webp"
        title="./images/yinyang.webp" />Ekstasis&#39;s Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">All Posts</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/tags/" title="">Tags</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">深度学习基础知识总结</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Ekstasis</a></span>&nbsp;<span class="post-category">included in <a href="/categories/ai/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>AI</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-01-13">2025-01-13</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;7320 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;15 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#基本概念">基本概念</a></li>
    <li><a href="#对学习的理解">对学习的理解</a></li>
    <li><a href="#模型评估和选择">模型评估和选择</a>
      <ul>
        <li><a href="#经验误差与过拟合">经验误差与过拟合</a></li>
        <li><a href="#评估方法">评估方法</a>
          <ul>
            <li><a href="#1-留出法hold-out-method">1. <strong>留出法（Hold-out Method）</strong></a></li>
            <li><a href="#2-交叉验证法cross-validation">2. <strong>交叉验证法（Cross-validation）</strong></a></li>
            <li><a href="#3-自助法bootstrapping">3. <strong>自助法（Bootstrapping）</strong></a></li>
            <li><a href="#参数调节与最终模型选择">参数调节与最终模型选择</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#神经网络httpsml-cheatsheetreadthedocsioenlatestnn_conceptshtml"><a href="https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html">神经网络</a></a>
      <ul>
        <li><a href="#神经元模型">神经元模型</a></li>
        <li><a href="#感知机和多层网络">感知机和多层网络</a></li>
        <li><a href="#反向传播算法">反向传播算法</a></li>
      </ul>
    </li>
    <li><a href="#深度学习训练技巧">深度学习训练技巧</a>
      <ul>
        <li><a href="#应对欠拟合问题">应对欠拟合问题</a>
          <ul>
            <li><a href="#1-局部最优与鞍点">1. 局部最优与鞍点</a></li>
            <li><a href="#2-小批量与噪声">2. 小批量与噪声</a></li>
            <li><a href="#3-自适应学习率">3. 自适应学习率</a></li>
            <li><a href="#4-动量momentum">4. 动量（Momentum）</a></li>
            <li><a href="#5-批归一化batch-normalization">5. 批归一化（Batch Normalization）</a></li>
            <li><a href="#6-激活函数的选择">6. 激活函数的选择</a></li>
          </ul>
        </li>
        <li><a href="#应对过拟合问题">应对过拟合问题</a>
          <ul>
            <li><a href="#1-增加数据量">1. 增加数据量</a></li>
            <li><a href="#2-正则化方法">2. 正则化方法</a></li>
            <li><a href="#3-提前停止early-stopping">3. 提前停止（Early Stopping）</a></li>
            <li><a href="#4-更简单的模型结构">4. 更简单的模型结构</a></li>
          </ul>
        </li>
        <li><a href="#分类任务中的优化策略">分类任务中的优化策略</a>
          <ul>
            <li><a href="#1-分类问题转化为回归问题">1. 分类问题转化为回归问题</a></li>
            <li><a href="#2-损失函数的选择">2. 损失函数的选择</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#pytorch手写数字识别的例子">Pytorch——手写数字识别的例子</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><ul>
<li><a href="https://stanford.edu/~shervine/teaching/" target="_blank" rel="noopener noreffer ">cheat sheet</a></li>
<li><a href="https://ml-cheatsheet.readthedocs.io/en/latest/" target="_blank" rel="noopener noreffer ">Machine Learning Glossary</a></li>
</ul>
<h2 id="基本概念">基本概念</h2>
<blockquote>
<p>什么是机器学习？</p>
</blockquote>
<p>机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。</p>
<p>经验通常以&quot;<strong>数据</strong>&ldquo;形式存在，因此机器学习研究的主要内容，是关于在计算机上从数据中产生&rdquo;<strong>模型</strong>&quot;(model) 的算法，即&quot;<strong>学习算法</strong>&quot; (learning algorithm).</p>
<p>记录的集合称为一个&quot;<strong>数据集</strong>&quot; (data set) ，其中每条记录是关于一个事件或对象的描述，称为一个&quot;<strong>示例</strong>&quot;(instance) 或&quot;<strong>样本</strong>&quot;(sample). 反映事件或对象在某方面的表现或性质的事项称为&quot;<strong>属性</strong>&quot;(attribute) 或&quot;<strong>特征</strong>&quot;(feature); 属性上的取值称为&quot;<strong>属性值</strong>&quot; (attribute value). 属性张成的空间称为&quot;<strong>属性空间</strong>&quot; (attribute space) 、&quot;<strong>样本空间</strong>&quot; (sample space)或&quot;<strong>输入空间</strong>&quot;。我们也把一个示例称为一个&quot;<strong>特征向量</strong>&quot; (feature vector).属性的数目被称为样本的&quot;<strong>维数</strong>&quot; (dimensionality).</p>
<p>从数据中学得模型的过程称为&quot;<strong>学习</strong>&quot;(learning)或&quot;<strong>训练</strong>&quot; (training),这个过程通过执行某个学习算法来完成.训练过程中使用的数据称为&quot;<strong>训练数据</strong>&quot; (training data) ，其中每个样本称为一个&quot;<strong>训练样本</strong>&quot; (training sample),训练样本组成的集合称为&quot;<strong>训练集</strong>&quot; (training set). 学得模型对应了关于数据的某种潜在的规律，因此亦称&quot;<strong>假设</strong>&quot;
(hypothesis); 这种潜在规律自身，则称为&quot;<strong>真相</strong>&ldquo;或&rdquo;<strong>真实</strong>&quot;(ground-truth) ，学习过程就是为了找出或逼近真相.</p>
<p>学得模型后，使用其母行预测的过程称为&quot;<strong>测试</strong>&quot;(testing) ，被预测的样本
称为&quot;<strong>测试样本</strong>&quot; (testing sample).</p>
<blockquote>
<p>都有哪些机器学习算法？</p>
</blockquote>
<p><a href="https://s3.amazonaws.com/assets.datacamp.com/email/other/ML&#43;Cheat&#43;Sheet_2.pdf" target="_blank" rel="noopener noreffer ">一份算法列表</a></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/CheatSheet.png"
        data-srcset="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/CheatSheet.png, /posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/CheatSheet.png 1.5x, /posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/CheatSheet.png 2x"
        data-sizes="auto"
        alt="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/CheatSheet.png"
        title="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/CheatSheet.png" width="1280" height="721" /></p>
<p>除如图所示的监督和非监督学习外，<strong>强化学习</strong>也是一类基本学习算法。</p>
<blockquote>
<p>什么是深度学习</p>
</blockquote>
<p>深度学习（Deep Learning）是机器学习的一个分支，是一种基于人工神经网络的算法模型。它模仿人类大脑处理信息的方式，采用<strong>多层神经网络</strong>从大量数据中提取特征并进行决策。深度学习在监督，非监督，强化学习中均有应用。</p>
<h2 id="对学习的理解">对学习的理解</h2>
<p>我们可以把学习过程看作一个在所有假设 (hypothesis)组成的空间中进行搜索的过程，搜索目标是找到与训练集&quot;匹配&quot;(fit) 的假设，即能够将训练集中的瓜判断正确的假设.假设的表示一旦确定，假设空间及其规模大小就确定了.</p>
<p>可能有多个假设与训练集一致，即存在着一个与训练集一致的&quot;<strong>假设集合</strong>&quot;，我们称之为&quot;版本空间&quot; (version space).</p>
<p>机器学习算法在学习过程中对某种类型假设的偏好，称为&quot;<strong>归纳偏好</strong>&quot; (inductive bias) ,或简称为&quot;偏好&quot;.任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上&quot;等效&quot;的假设所迷惑，而无法产生确定的学习结果.</p>
<p>归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或&quot;价值观&quot;。&ldquo;奥卡姆剃刀&rdquo; (Occam&rsquo;s razor)是一种常用的、自然科学研究中最基本的原则，即&quot;若有多个假设与观察一致，则选最简单的那个。</p>
<p>事实上，归纳偏好对应了学习算法本身所做出的关于&quot;什么样的模型更好&quot;的假设.在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能.</p>
<blockquote>
<p>不同的神经网络架构既体现了不同的假设空间，也体现了不同的归纳偏好</p>
<p><strong>假设空间</strong>：由架构定义，是模型可以表示的所有可能函数。</p>
<blockquote>
<p><a href="https://www.wikiwand.com/en/articles/Universal_approximation_theorem" target="_blank" rel="noopener noreffer ">理论上来说</a>，一层全连接神经网络（具有足够的神经元和非线性激活函数）就能逼近任意连续函数。</p>
</blockquote>
<p><strong>归纳偏好</strong>：由架构及其优化过程（例如损失函数、正则化策略）定义，是模型在有限数据下更倾向选择的函数。</p>
</blockquote>
<p>而同时，根据<a href="https://www.wikiwand.com/en/articles/No_free_lunch_theorem" target="_blank" rel="noopener noreffer ">NFL 定理</a>，<strong>没有任何一种学习算法在所有问题上都优于其他算法</strong>。</p>
<p>直观来说：</p>
<ul>
<li>如果我们对<strong>问题分布</strong>一无所知，那么所有算法在这些问题上成功的概率是等价的。</li>
<li>要在特定任务中取得好性能，算法必须<strong>对任务的特定性质做出假设</strong>（即引入归纳偏好）。</li>
</ul>
<h2 id="模型评估和选择">模型评估和选择</h2>
<h3 id="经验误差与过拟合">经验误差与过拟合</h3>
<p>在机器学习中，我们通常用以下指标来衡量模型的性能：</p>
<ul>
<li><strong>错误率（Error Rate）</strong>：分类错误的样本数占样本总数的比例。例如，如果在 $m$ 个样本中有 $\alpha$ 个样本被分类错误，则： $\text{错误率} = \frac{\alpha}{m}$</li>
<li><strong>精度（Accuracy）</strong>：分类正确的样本数占样本总数的比例，与错误率互补： $\text{精度} = 1 - \text{错误率}$</li>
</ul>
<p>更广义地，我们把<strong>学习器的实际预测输出与样本的真实输出之间的差异</strong>称为“误差（Error）”：</p>
<ul>
<li><strong>训练误差（Training Error）</strong>：学习器在训练集上的误差，又称为“经验误差（Empirical Error）”。</li>
<li><strong>泛化误差（Generalization Error）</strong>：学习器在新样本上的误差。</li>
</ul>
<p>我们的目标是构建一个泛化误差尽可能小的学习器。然而，<strong>由于事先无法得知新样本的分布</strong>，我们通常通过最小化经验误差来间接优化泛化误差。</p>
<p>在实践中，我们希望学习器能对新样本做出准确预测，这要求它从训练数据中学到能适用于所有潜在样本的“普遍规律”。但模型的学习能力可能出现两种极端情况：</p>
<ol>
<li><strong>过拟合（Overfitting）</strong>：
学习器对训练样本学得过于“精准”，甚至将训练样本中特有的噪声或不普遍的特性当成一般规律。这会导致模型对新样本的泛化性能下降。</li>
<li><strong>欠拟合（Underfitting）</strong>：
学习器没有充分捕捉训练样本的一般规律，导致模型在训练集和测试集上的表现都不理想。</li>
</ol>
<p><strong>常见原因：</strong></p>
<ul>
<li><strong>过拟合</strong>：学习能力过强，模型复杂度过高。</li>
<li><strong>欠拟合</strong>：学习能力不足，模型复杂度过低。</li>
</ul>
<p>过拟合是机器学习中的一个普遍现象，无法彻底避免，但可以通过以下方法缓解或减小其影响：</p>
<ul>
<li>增加训练数据。</li>
<li>使用正则化（如 $L_1$ 或 $L_2$ 正则化）。</li>
<li>采用早停（Early Stopping）策略。</li>
<li>使用更简单的模型（降低复杂度）。</li>
<li>引入交叉验证（Cross-Validation）评估模型性能。</li>
</ul>
<p><strong>本质限制</strong>：过拟合无法完全避免，因为机器学习问题通常是 NP 难或更复杂的，而有效的学习算法必然是多项式时间内运行完成。要彻底避免过拟合，经验误差最小化就能找到最优解，而这在理论上等同于 $P=NP$（构造性地证明），目前尚未被证明。</p>
<blockquote>
<p>许多学习问题（如神经网络权重优化、支持向量机中的核参数选择等）涉及<strong>非凸优化</strong>问题，或需要在一个离散的组合空间中搜索最佳解。这类问题通常被证明为 NP难。</p>
</blockquote>
<p>在实际任务中，我们面临许多选择，包括：</p>
<ul>
<li><strong>不同的学习算法</strong>（如决策树、支持向量机、深度学习）。</li>
<li><strong>同一算法的不同参数配置</strong>（如正则化强度、学习率、网络结构）。</li>
</ul>
<p>这引出了<strong>模型选择</strong>（Model Selection）问题： 如何选择一个泛化性能最好的模型？</p>
<p><strong>理想目标</strong>：直接评估候选模型的泛化误差，选择泛化误差最小的模型。
<strong>现实困境</strong>：泛化误差无法直接获得，训练误差又可能因过拟合而无法准确反映模型性能。</p>
<h3 id="评估方法">评估方法</h3>
<p>在机器学习中，模型的性能评估是关键的一环。为了衡量模型的泛化能力（即在新样本上的表现），我们通常通过实验测试来评估泛化误差。以下将介绍几种常用的数据集划分与评估方法，并探讨参数调节的技巧。</p>
<p>在评估模型时，需将原始数据集 $D$（包含 $m$ 个样本）划分为训练集 $S$ 和测试集 $T$，即 $D = S \cup T$ 且 $S \cap T = \emptyset$。训练集用于训练模型，测试集用于评估模型在未见样本上的表现。测试误差（Testing Error）被视为泛化误差的近似，假设测试样本与训练样本是从相同分布中独立同分布抽取的。</p>
<h4 id="1-留出法hold-out-method">1. <strong>留出法（Hold-out Method）</strong></h4>
<p>留出法是最简单的评估方法，将数据集 $D$ 随机划分为训练集和测试集。例如，常用 2/3 至 4/5 的样本作为训练集，其余作为测试集。</p>
<p><strong>注意事项：</strong></p>
<ul>
<li>数据划分需尽量保持分布一致性（如分类任务中保持类别比例），可通过<strong>分层采样</strong>（Stratified Sampling）实现。</li>
<li>单次划分的评估结果可能不稳定，因此需多次随机划分，取平均值作为最终评估结果。</li>
</ul>
<p><strong>权衡：</strong></p>
<ul>
<li>训练集比例过大：模型更接近使用全数据训练的情况，但测试集过小可能导致结果不稳定。</li>
<li>测试集比例过大：测试结果更稳定，但训练集与全数据差距较大，可能影响评估结果的保真性（Fidelity）。</li>
</ul>
<h4 id="2-交叉验证法cross-validation">2. <strong>交叉验证法（Cross-validation）</strong></h4>
<p>交叉验证将数据集 $D$ 分为 $k$ 个大小相等的互斥子集：
$D = D_1 \cup D_2 \cup &hellip; \cup D_k$​，且 $D_i \cap D_j = \emptyset , (i \neq j)$</p>
<p>每次用 $k−1$ 个子集作为训练集，剩余一个子集作为测试集，重复 $k$ 次后取测试结果的均值作为最终评估结果。常见的 $k$ 值为 10，称为<strong>10 折交叉验证</strong>（10-fold Cross-validation）。</p>
<p><strong>优点：</strong></p>
<ul>
<li>比留出法更稳定可靠。</li>
<li>可随机重复多次交叉验证（如 10 次 10 折交叉验证），进一步减少划分差异的影响。</li>
</ul>
<p><strong>特例：留一法（Leave-One-Out Cross-validation, LOO）</strong></p>
<ul>
<li>令 $k=m$，即每次仅用一个样本作为测试集，其余样本作为训练集。</li>
<li>LOO 不受随机划分影响，训练集与全数据集非常接近，但计算复杂度较高，尤其在样本量较大时。</li>
</ul>
<blockquote>
<p>在理论上，多次留出法的随机划分结果有可能与交叉验证法某次划分完全一致</p>
</blockquote>
<h4 id="3-自助法bootstrapping">3. <strong>自助法（Bootstrapping）</strong></h4>
<p>自助法通过<strong>自助采样</strong>（Bootstrap Sampling）生成新的训练集。每次从原始数据集 $D$ 中随机抽取一个样本放入训练集，并将其放回，再次采样，重复 $m$ 次。采样后：</p>
<ul>
<li>$D′$ 为训练集，包含 $m$ 个样本，其中部分样本重复出现。</li>
<li>$D \setminus D&rsquo;$ 作为测试集，包含约 36.8% 的未被采样样本（概率计算如下）： $P(\text{样本未被采样}) = (1 - \frac{1}{m})^m \approx e^{-1} \approx 36.8%$</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>适用于数据量较小或难以划分的场景。</li>
<li>可生成多个训练集，用于集成学习等方法。</li>
</ul>
<p><strong>局限：</strong></p>
<ul>
<li>改变了数据分布，可能引入估计偏差。</li>
</ul>
<hr>
<h4 id="参数调节与最终模型选择">参数调节与最终模型选择</h4>
<p>机器学习模型通常有多个参数需设定，不同参数配置会显著影响模型性能。<strong>调参</strong>（Parameter Tuning）是优化模型性能的重要步骤。</p>
<p><strong>常见调参方法：</strong></p>
<ol>
<li><strong>网格搜索（Grid Search）：</strong>
<ul>
<li>为每个参数设定取值范围和步长，逐一评估所有参数组合，选择性能最佳者。</li>
<li>计算复杂度较高，但可行性依赖于参数数量和候选值范围。</li>
<li>示例：3 个参数，每个参数有 5 个候选值，需训练 $5^3 = 125$ 个模型。</li>
</ul>
</li>
<li><strong>随机搜索（Random Search）：</strong>
<ul>
<li>随机采样参数组合，降低计算成本。</li>
</ul>
</li>
<li><strong>验证集（Validation Set）：</strong>
<ul>
<li>在训练数据中留出一部分作为验证集，用于模型评估和参数调节。</li>
<li>实际测试数据则保留用于最终测试。</li>
<li>常用的比例是训练集占 70%~80%，测试集占 20%~30%。再从训练集划分出验证集，常见比例是：训练集占原始数据的 60%~70%，验证集占 10%~20%。</li>
<li>验证集允许模型在开发过程中尝试不同的算法或参数设置，为模型选择和调参提供依据，而测试集不参与这个过程，以<strong>防止评估偏差</strong>。</li>
</ul>
</li>
</ol>
<h2 id="神经网络httpsml-cheatsheetreadthedocsioenlatestnn_conceptshtml"><a href="https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html" target="_blank" rel="noopener noreffer ">神经网络</a></h2>
<h3 id="神经元模型">神经元模型</h3>
<p><strong>神经网络</strong>是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。神经网络中最基本的成分是<strong>神经元</strong> (neuron)模型。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neuron.png"
        data-srcset="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neuron.png, /posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neuron.png 1.5x, /posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neuron.png 2x"
        data-sizes="auto"
        alt="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neuron.png"
        title="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neuron.png" width="800" height="295" /></p>
<p>（M-P 神经元）</p>
<h3 id="感知机和多层网络">感知机和多层网络</h3>
<p>感知机 (Perceptron) 由两层 神经元 组成，感知机能容易地实现逻辑与、或、非运算。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/image-20250113181958415.png"
        data-srcset="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/image-20250113181958415.png, /posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/image-20250113181958415.png 1.5x, /posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/image-20250113181958415.png 2x"
        data-sizes="auto"
        alt="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/image-20250113181958415.png"
        title="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/image-20250113181958415.png" width="678" height="620" /></p>
<p>史一般的，常见的神经网络是形如图所示的层级结构，每层神经元与下
层神经元全互连。神经元之间不存在同层连接，也不存在跨层连接。这样 的
神经网络结构通常称为&quot; 多层前馈神经网络 &quot; (multi-layer feedforward neural networks)</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neural-network-en.png"
        data-srcset="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neural-network-en.png, /posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neural-network-en.png 1.5x, /posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neural-network-en.png 2x"
        data-sizes="auto"
        alt="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neural-network-en.png"
        title="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/img/neural-network-en.png" width="1500" height="350" /></p>
<h3 id="反向传播算法">反向传播算法</h3>
<p>最常用的神经网络训练方法，详见： <a href="https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html" target="_blank" rel="noopener noreffer ">https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html</a></p>
<h2 id="深度学习训练技巧">深度学习训练技巧</h2>
<p>深度学习训练中的变量：</p>
<ol>
<li>数据集</li>
<li>模型结构（model bias）</li>
<li>超参数
<ol>
<li>epoch数目，batch大小等</li>
<li>loss function</li>
<li>优化算法</li>
</ol>
</li>
</ol>
<p>解决问题都得从这几个方面入手</p>
<h3 id="应对欠拟合问题">应对欠拟合问题</h3>
<p>优化过程中，神经网络的目标是最小化损失函数。然而，优化过程可能会遇到多种挑战，尤其是在高维空间中。</p>
<p>Adam算法结合了RMSProp和动量方法，是目前最常用的优化算法之一，能够有效提高训练速度，并避免梯度消失问题。最为常用。</p>
<h4 id="1-局部最优与鞍点">1. 局部最优与鞍点</h4>
<ul>
<li><strong>局部最优解</strong>：损失函数在某个局部区域内达到最小值，这可能导致优化算法在该点停滞，从而影响模型的训练。</li>
<li><strong>鞍点</strong>：在高维空间中，鞍点比局部最优解更多。鞍点的梯度为零，但它既不是最小值也不是最大值，优化算法容易被卡在这种点上。
<ul>
<li><strong>解决鞍点</strong>：可以通过计算<strong>Hessian矩阵</strong>来判断一个点是否为鞍点。训练过程中使用<strong>动量</strong>和<strong>自适应学习率</strong>等技术可以帮助避免被卡住。</li>
</ul>
</li>
</ul>
<h4 id="2-小批量与噪声">2. 小批量与噪声</h4>
<ul>
<li><strong>小批量的优势</strong>：小批量训练时，由于每个批次的样本较少，噪声较大，这种噪声有助于模型跳出局部最小值，避免陷入“峡谷”中。实验证明，小批量训练有助于提高模型的泛化能力，并且在测试集上的表现通常更好。
<ul>
<li><strong>劣势</strong>：小批量训练需要更多的时间来完成一个epoch，因为每个batch的训练时间较长。</li>
</ul>
</li>
<li><strong>大批量的劣势</strong>：大批量训练时，由于噪声较小，模型更容易停留在局部最小值，从而导致训练效果不佳。</li>
</ul>
<h4 id="3-自适应学习率">3. 自适应学习率</h4>
<p>学习率是控制梯度下降步伐的重要超参数。过大的学习率可能导致优化过程不稳定，过小的学习率则可能导致收敛缓慢。</p>
<ul>
<li><strong>自适应学习率</strong>：使用<strong>Adagrad</strong>、<strong>RMSProp</strong>、<strong>Adam</strong>等自适应学习率算法，可以根据每个参数的梯度大小调整步伐，从而提高训练效率和收敛速度。</li>
<li><strong>学习率调度</strong>：逐渐降低学习率（如通过<strong>学习率衰减</strong>或<strong>warm-up</strong>策略），能够避免震荡，帮助网络找到更精确的解。</li>
</ul>
<h4 id="4-动量momentum">4. 动量（Momentum）</h4>
<p>动量方法通过结合当前的梯度和前一时刻的梯度信息，帮助网络在遇到局部最小值时继续向前推进，避免停滞在局部最小值。动量方法模仿物理中小球滚动的现象，在训练过程中具有较强的稳定性。</p>
<h4 id="5-批归一化batch-normalization">5. 批归一化（Batch Normalization）</h4>
<p>批归一化通过对每层输入进行标准化，消除不同维度间的差异，使得网络训练更加稳定。</p>
<ul>
<li><strong>平坦化误差表面</strong>：批归一化能有效平滑损失函数，减少训练过程中出现不均匀的误差。</li>
<li><strong>训练与测试的区别</strong>：在训练时，批归一化使用当前批次的数据进行标准化，而在测试时使用训练期间计算的均值和方差。</li>
</ul>
<h4 id="6-激活函数的选择">6. 激活函数的选择</h4>
<p>激活函数对优化过程有着重要影响。不同的激活函数会决定网络的非线性能力，从而影响优化效果。</p>
<ul>
<li><strong>ReLU（Rectified Linear Unit）</strong>：ReLU是目前最常用的激活函数，能够有效避免梯度消失问题，并且计算效率高。</li>
<li><strong>Sigmoid和Tanh</strong>：这些激活函数适用于浅层网络，但在深层网络中容易出现梯度消失问题，影响训练效果。</li>
</ul>
<h3 id="应对过拟合问题">应对过拟合问题</h3>
<p>过拟合是深度学习训练过程中常见的问题，特别是当模型在训练集上表现很好，但在测试集上性能较差时。为了减少过拟合，可以采用以下方法：</p>
<h4 id="1-增加数据量">1. 增加数据量</h4>
<ul>
<li><strong>数据增强</strong>：通过对现有数据进行旋转、平移、缩放等变换，生成更多的样本，以减少模型对训练数据的依赖，从而提高泛化能力。</li>
<li><strong>正则化技术</strong>：如<strong>L2正则化</strong>和<strong>dropout</strong>等，能够有效抑制模型的过拟合。</li>
</ul>
<h4 id="2-正则化方法">2. 正则化方法</h4>
<ul>
<li><strong>L2正则化</strong>：通过在损失函数中增加惩罚项，迫使模型参数保持较小的值，从而避免模型过于复杂。</li>
<li><strong>Dropout</strong>：在训练过程中随机丢弃一定比例的神经元，这样可以避免网络过度依赖某些特定神经元，从而提高泛化能力。</li>
</ul>
<h4 id="3-提前停止early-stopping">3. 提前停止（Early Stopping）</h4>
<p>在训练过程中，通过监控验证集的损失，若验证集损失不再下降或开始上升，则提前停止训练。这样可以避免模型在训练集上过拟合。</p>
<h4 id="4-更简单的模型结构">4. 更简单的模型结构</h4>
<ul>
<li><strong>减小网络规模</strong>：使用较小的网络结构（例如减少网络层数或神经元数量），可以减少模型的复杂度，从而降低过拟合的风险。</li>
</ul>
<h3 id="分类任务中的优化策略">分类任务中的优化策略</h3>
<p>在分类任务中，优化策略需要根据任务的特点进行调整：</p>
<h4 id="1-分类问题转化为回归问题">1. 分类问题转化为回归问题</h4>
<p>有时将分类问题转化为回归问题进行解决，网络输出一个连续值。这种方法可能会引入伪关系，因此更常见的是使用<strong>One-hot向量</strong>作为输出，并结合<strong>Softmax</strong>函数进行标准化。</p>
<h4 id="2-损失函数的选择">2. 损失函数的选择</h4>
<ul>
<li><strong>交叉熵损失 (Cross-Entropy Loss)</strong>：对于分类任务，尤其是多分类任务，交叉熵损失是更合适的选择，特别是与Softmax结合使用时，能够更好地指导优化过程。</li>
</ul>
<h2 id="pytorch手写数字识别的例子">Pytorch——手写数字识别的例子</h2>
<p><a href="http://blog.ezyang.com/2019/05/pytorch-internals/" target="_blank" rel="noopener noreffer ">pytorch internals</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.optim <span style="color:#66d9ef">as</span> optim
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> datasets, transforms
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> random_split
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 超参数</span>
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
</span></span><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.001</span>
</span></span><span style="display:flex;"><span>epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>dropout_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. 数据加载和预处理</span>
</span></span><span style="display:flex;"><span>transform <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose([
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>ToTensor(),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>Normalize((<span style="color:#ae81ff">0.5</span>,), (<span style="color:#ae81ff">0.5</span>,))
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 加载MNIST数据集</span>
</span></span><span style="display:flex;"><span>train_dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>MNIST(root<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./data&#39;</span>, train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, download<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, transform<span style="color:#f92672">=</span>transform)
</span></span><span style="display:flex;"><span>test_dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>MNIST(root<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./data&#39;</span>, train<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, download<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, transform<span style="color:#f92672">=</span>transform)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 数据分批次处理</span>
</span></span><span style="display:flex;"><span>train_loader <span style="color:#f92672">=</span> DataLoader(train_dataset, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>test_loader <span style="color:#f92672">=</span> DataLoader(test_dataset, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. 定义卷积神经网络 (CNN)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SimpleCNN</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(SimpleCNN, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 第一卷积层，输入1个通道，输出32个通道</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">32</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">32</span>)  <span style="color:#75715e"># 批归一化</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 第二卷积层，输入32个通道，输出64个通道</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">64</span>)  <span style="color:#75715e"># 批归一化</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 第三卷积层，输入64个通道，输出128个通道</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">128</span>)  <span style="color:#75715e"># 批归一化</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 线性层（全连接层）</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">128</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">10</span>)  <span style="color:#75715e"># 输出10个类别（MNIST是0-9的数字）</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Dropout层，防止过拟合</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span>dropout_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 卷积层 + 激活函数 + 批归一化 + 池化</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>conv1(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(x, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>conv2(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(x, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn3(self<span style="color:#f92672">.</span>conv3(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(x, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 将特征图展平（Flatten）</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">128</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 全连接层</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)  <span style="color:#75715e"># 在全连接层后应用Dropout</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc2(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. 模型、损失函数和优化器</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SimpleCNN()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Adam优化器，并使用L2正则化（weight decay）</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span>learning_rate, weight_decay<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用交叉熵损失函数（适用于分类任务）</span>
</span></span><span style="display:flex;"><span>criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. 训练过程</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(model, train_loader, optimizer, criterion, epoch):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>    running_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    total <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> batch_idx, (data, target) <span style="color:#f92672">in</span> enumerate(train_loader):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 将数据移到GPU（如果可用）</span>
</span></span><span style="display:flex;"><span>        data, target <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>cuda(), target<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 清除梯度</span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 向前传播</span>
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 计算损失</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> criterion(output, target)
</span></span><span style="display:flex;"><span>        running_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 向后传播</span>
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 更新参数</span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 计算准确度</span>
</span></span><span style="display:flex;"><span>        _, predicted <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>max(output, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        total <span style="color:#f92672">+=</span> target<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        correct <span style="color:#f92672">+=</span> (predicted <span style="color:#f92672">==</span> target)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 打印每个epoch的损失和准确率</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#e6db74">}</span><span style="color:#e6db74">, Loss: </span><span style="color:#e6db74">{</span>running_loss<span style="color:#f92672">/</span>len(train_loader)<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, Accuracy: </span><span style="color:#e6db74">{</span><span style="color:#ae81ff">100.</span> <span style="color:#f92672">*</span> correct<span style="color:#f92672">/</span>total<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5. 测试过程</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(model, test_loader, criterion):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()  <span style="color:#75715e"># 切换为评估模式</span>
</span></span><span style="display:flex;"><span>    test_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    total <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():  <span style="color:#75715e"># 测试时不需要计算梯度</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> data, target <span style="color:#f92672">in</span> test_loader:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 将数据移到GPU（如果可用）</span>
</span></span><span style="display:flex;"><span>            data, target <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>cuda(), target<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 向前传播</span>
</span></span><span style="display:flex;"><span>            output <span style="color:#f92672">=</span> model(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 计算损失</span>
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> criterion(output, target)
</span></span><span style="display:flex;"><span>            test_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 计算准确度</span>
</span></span><span style="display:flex;"><span>            _, predicted <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>max(output, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            total <span style="color:#f92672">+=</span> target<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>            correct <span style="color:#f92672">+=</span> (predicted <span style="color:#f92672">==</span> target)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 打印测试集上的损失和准确率</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Test Loss: </span><span style="color:#e6db74">{</span>test_loss<span style="color:#f92672">/</span>len(test_loader)<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, Test Accuracy: </span><span style="color:#e6db74">{</span><span style="color:#ae81ff">100.</span> <span style="color:#f92672">*</span> correct<span style="color:#f92672">/</span>total<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 6. 训练和测试模型</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 如果有GPU，使用GPU训练</span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, epochs <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>    train(model, train_loader, optimizer, criterion, epoch)
</span></span><span style="display:flex;"><span>    test(model, test_loader, criterion)
</span></span></code></pre></div></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-01-13</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://rzyn2020.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/" data-title="深度学习基础知识总结" data-hashtags="DeepLearning,Summary"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://rzyn2020.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/" data-hashtag="DeepLearning"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://rzyn2020.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/" data-title="深度学习基础知识总结"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://rzyn2020.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/" data-title="深度学习基础知识总结"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://rzyn2020.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/" data-title="深度学习基础知识总结"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/deeplearning/">DeepLearning</a>,&nbsp;<a href="/tags/summary/">Summary</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/%E5%90%89%E7%99%BB%E6%96%AF%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%AC%94%E8%AE%B0/" class="prev" rel="prev" title="吉登斯「社会学」笔记"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>吉登斯「社会学」笔记</a>
            <a href="/posts/mysql%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/" class="next" rel="next" title="MySQL 知识点总结 ">MySQL 知识点总结 <i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2021 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Ekstasis</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"","algoliaIndex":"","algoliaSearchKey":"","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
