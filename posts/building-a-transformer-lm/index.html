<!DOCTYPE html>
<html><head lang="en">
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Building a Transformer LM - Ekstasis&#39;s Blog</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Once upon a time, there was a mountain." />
	<meta property="og:image" content=""/>
	<meta property="og:url" content="https://rzyn2020.github.io/posts/building-a-transformer-lm/">
  <meta property="og:site_name" content="Ekstasis&#39;s Blog">
  <meta property="og:title" content="Building a Transformer LM">
  <meta property="og:description" content="Once upon a time, there was a mountain.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-04-06T09:58:31+08:00">
    <meta property="article:modified_time" content="2025-04-06T09:58:31+08:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Code">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Building a Transformer LM">
  <meta name="twitter:description" content="Once upon a time, there was a mountain.">

        <link href="https://rzyn2020.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="https://rzyn2020.github.io/css/main.df3f44613463554998dacba673a067eb0777f76fe3d12bc0a1de5bad493ef666.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://rzyn2020.github.io/css/dark.50b57e12d401420df23965fed157368aba37b76df0ecefd0b1ecd4da664f01a0.css" media="(prefers-color-scheme: dark)"  />
  	
   	 	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['$', '$']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
  	

	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="https://rzyn2020.github.io/">Ekstasis&#39;s Blog</a>
	</div>
	<nav>
		
		<a href="/posts/">All Posts</a>
		
		<a href="/categories/">Categories</a>
		
		<a href="/tags/">Tags</a>
		
		<a href="https://rzyn2020.github.io/algorithm/">Algorithm</a>
		
		
	</nav>
</header>

<main>
  <article>
    <div class="post-container">
      
      <div class="post-content">
        <div class="title">
          <h1 class="title">Building a Transformer LM</h1>
          <div class="meta">Posted on Apr 6, 2025</div>
        </div>
        
        <section class="body">
          <p>Once upon a time, there was a mountain.</p>
<blockquote>
<p>输入提示 ➜ Once upon a time, there was a mountain.</p>
<p>生成结果 ➜
Once upon a time, there was a mountain. In the sky, there was a big, round ball. A little boy named Tim and his mom went to the park to play. The sun was shining, and the sky was blue.
In the park, Tim saw a big tree. He wanted to play near it. His mom said, &ldquo;Okay, but be careful.&rdquo; Tim was happy. He played near the tree. He got up and down.
While playing, Tim saw a small bird. The bird was sad. Tim said, &ldquo;The bird has a hurt wing. Can you help me?&rdquo; The bird said, &ldquo;Yes, I can help you.&rdquo; Tim took the bird to his mom. He gave her a magic rock. The bird said, &ldquo;Thank you, Tim. You are a good friend.&rdquo; Now, Tim and the bird were friends. They played together every day.</p></blockquote>
<h2 id="intro">Intro</h2>
<p><img src="./assets/large-language-model-market-size.webp" alt="Large Language Model Market Size 2025 to 2034"></p>
<p>自从 2022 年 ChatGPT 发布后，LLM 开始席卷世界，俨然有一次新的科技浪潮之势。大模型资本市场以每年近 40% 的速度狂飙突进，在 2025 年初已接近 <a href="https://www.precedenceresearch.com/large-language-model-market#:~:text=work%20for%20everyone.-,U.S.%20Large%20Language%20Model%20Market%20Size%20and%20Growth%202025%20to,36.17%25%20from%202025%20to%202034.">77 亿美元</a>，且预计在 2034 年可达千亿级别，与之相比，较为成熟但应用领域略窄的的计算机视觉市场规模也有 250 亿美元。</p>
<p>资本市场的火热与否在于应用的前景，而大模型的应用前景也显然十分广阔。现在许多 app 已接入 AI， 增强功能，而 RAG, Agent 等技术也正在迅速发展，更何况未来还有大模型辅助智能制造的广阔的场景。</p>
<p>应用前景引来资本投资，资本投资引起硬件，算法，以及系统的全方面发展。在摩尔定律失效之后，<a href="https://dl.acm.org/doi/pdf/10.1145/3282307">领域专用硬件成为了提升计算力的绝妙法门</a>。英伟达的 GPU 针对训练和推理做了许多优化，华为，Google 也推出了许多深度学习专用计算设备。算法层面，模型结构和参数量也都在不断演化。而系统层面，也不断有新技术和新项目来适应新的模型和硬件。在这<a href="https://hardwarelottery.github.io/">几者的共同协作</a>下，即使科技树点偏了，其探索之深也足以改变世界了。</p>
<p>在这个背景下，无论是出于好奇还是功利，学习大模型，理解其基本原理就显得有必要了。因此，我参考<a href="https://stanford-cs336.github.io/spring2024/">cs336课程</a>,，试着 build a transformer-language model from scratch。在过程中，我也大量参考了 <a href="https://www.youtube.com/@umarjamilai">Umar Jamil</a> 的 live coding 以及李宏毅的<a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php">机器学习课程</a>。</p>
<p>最终我使用 <a href="https://arxiv.org/abs/2305.07759">TinyStory</a> 数据集，在一张 4090 显卡上花费一个半小时训练出了一个讲故事模型，且称之为 StoryLM。文章开头的故事续写就由 StoryLM 生成。</p>
<p>这篇文章介绍该模型构建的全过程，并解释相关概念。目标读者是有一些深度学习基础的计算机专业学生（也就是年初的我）。</p>
<h2 id="architecture">Architecture</h2>
<p><img src="./assets/image-20250406162458067.png#half" alt="image-20250406162458067"></p>
<p>首先，从宏观上来介绍大模型的结构以及其生命周期。</p>
<p>就像任何深度学习模型一样，大模型的训练离不开<strong>模型结构，优化器，损失函数</strong>三个组成部分。优化器一般为常见的 AdamW，损失函数为分类任务常用的交叉熵，而几乎所有大模型的结构都基于 Transformer。Transformer 可分为编码器和解码器两部分，最初用于语言翻译等任务。编码器理解源语言，自回归解码器生成目标语言。而在生成式语言模型中，我们仅仅保留解码器即可。</p>
<blockquote>
<p>在实际训练中，为了防止过拟合，梯度爆炸/消失，训练难以进行三个问题，会对上面三个组成部分做诸多调整</p></blockquote>
<p><img src="./assets/image-20250406235457400.png" alt="image-20250406235457400"></p>
<p>除了上面三者之外，训练数据也需要做一些处理。具体而言，我们需要通过 Tokenizer 加上词嵌入把句子变为模型能够理解的形式。具体可见 Tokenizer  和 Model 两小节。简而言之，一个句子会被切分成许多 token，每个 token 会被转化为一个词向量。输入一个句子后，模型会输出一个表示接在该句子后可能的 token 的概率分布。</p>
<p>StoryLM 的训练数据只有2GB，而真实世界大模型的训练数据动辄数TB，为了爬取并清洗得到这些训练数据，真实的 LLM system 也包含了许多数据处理相关技术。本文暂时不涉及。</p>
<p>同时 StoryLM 的参数量仅为 22M，而真实的大模型参数量均为 Billion 以上，因此其训练和推理过程的性能优化也是必不可少的。我会在后续文章介绍相关优化。</p>
<p>最后，通过大量语料进行的自监督训练只是预训练，得到的是基座模型，只具有续写能力。为了使模型更像语言助手，还需要进行监督微调与 RLHF。不过我们现在只考虑训练基座模型。</p>
<h2 id="tokenizer">Tokenizer</h2>
<h3 id="byte-pair-encoding">Byte-Pair Encoding</h3>
<p>Tokenizer 的目的是把句子转化为 Token 序列，每个 Token 都可以用一个数字表示，以便于模型处理。一个最简单的想法是 word-level tokenizer，但是 word 太多会导致词汇表过大，降低计算效率。而且会将如 &ldquo;played&rdquo; &ldquo;playing&rdquo; 编码为完全不同的两个Token，没能考虑到两者之间关系。因此，就有了第二个想法—— byte-level tokenizer：既然所有语言在计算机中的表示都是字节序列，那么我们把字节序列送给模型就好了！但是它也有一个问题：这种编码方式有太多冗余，word level 中的一个 Token 可能在 byte level 中就对应了好几个 Token，这样也会降低计算效率。</p>
<p>而 <a href="https://huggingface.co/learn/llm-course/en/chapter2/4?fw=pt#and-more">Byte Pair Encoding</a> 是一种<strong>折中</strong>的算法：首先基于 byte-level tokenizer，这样就可以使用很小的词汇表处理各种语言了，但同时为了防止 Token 数目过多，BPE 会不断将最常见的 Token Pair 合并为一个新的 Token，直到达到预设的词汇表数目上限，以压缩 Token 长度。</p>
<p>在实践中，BEP有训练和运行两个过程。训练阶段会学习到哪些 Token 可以合并，构建词汇表。而运行阶段接受一个句子，根据训练好的词汇表将其编码为 Token 数组。</p>
<h3 id="bpe-implementation">BPE Implementation</h3>
<h4 id="train-bpe">Train BPE</h4>
<p>首先考虑训练。训练时我们要不断识别最频繁出现的 Token-Pair 并将其合并。最简单的暴力算法就是每次合并时都扫描一遍，找到最出现最多的 Pair。假设 <code>n</code> 为语料长度，<code>m</code> 为词汇表大小上限。显然这个 <code>O(mn)</code> 的算法开销太大。</p>
<p>但是考虑到每合并一个 Pair 后，只有与这个 Pair 相邻的 Pair 数目才产生变化，我们可以这样设计程序：</p>
<p>数据结构：</p>
<ol>
<li>将 Pair 与其对应 count 存在一个数据结构 PairCounter 中，我们可以获取频率最高的 Pair，并方便地进行增删查改
<ol>
<li>比如
<ol>
<li>存储子树最值的平衡搜索树</li>
<li>支持<code>O(logn)</code>删除的堆
<ol>
<li>延迟删除法（原堆+删除堆）</li>
<li>即时删除法（哈希表记录元素索引）</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>训练语料转化而来的 Token 链表，L</li>
<li>一个从 Pair 到其出现位置的 map，M</li>
</ol>
<p>算法：</p>
<ol>
<li>每次获取最频繁 Pair
<ol>
<li>增加到词汇表中</li>
<li>通过 M 修改 Token 链表 L</li>
<li>通过 Token 链表 L 信息更新 PairCounter 和 M</li>
</ol>
</li>
</ol>
<p>但考虑到大部分语言中常见单词只为几千个，如果忽略跨词 Pair，我们就可以考虑把 Token 链表 L 替换为一个 <code>从单词到该单词 Token 链的 map</code> 加上一个<code>从单词到该单词在训练集中出现数目的map</code>。这样，并把 M 修改 <code>从 Pair 到其出现的单词的 map</code>。这样，又能大大降低实际计算的开销。</p>
<p>这样，假设<code>k1</code>为最大 Pair 数目，<code>k2</code>为最大单词数目，该算法就可以在<code>O(n + mlogk1 + mk2)</code>的时间内完成训练。</p>
<p>此外，这个算法比较复杂，实现时要注意：</p>
<ol>
<li>及时把大函数拆分为小函数，方便理解</li>
<li>采用 OOP 的设计方法，把数据结构和对其操作的方法结合起来</li>
</ol>
<h4 id="run-bpe">Run BPE</h4>
<p>训练完成后，实际运行时，我们要编码的语料更多，因此更要考虑性能问题。</p>
<p><strong>首先是算法上的性能。</strong></p>
<p>假设语料长度为<code>n</code>，词汇表长度为<code>m</code>，简单的暴力算法时间复杂度显然为 <code>O(mn)</code>。</p>
<p>考虑到我们在训练时，合并的 Pair 都处于一个单词内，所以可以这样设计程序：</p>
<p>数据结构：</p>
<ol>
<li>一个代表原语料的单词 ID List</li>
<li>一个从单词 ID 到该单词 Token 链的 map，M</li>
<li>一个从 Pair 到其出现的单词的 map</li>
</ol>
<p>算法：</p>
<ol>
<li>仿照训练过程，初始化这些数据结构</li>
<li>根据训练过程得到的词汇表，不断进行 Pair 合并</li>
<li>根据单词 ID List 和 M 得到最终编码结果</li>
</ol>
<p>假设<code>k</code>为最大单词数目，则时间复杂度为 <code>O(n + mk)</code></p>
<p><strong>其次是工程上的性能。</strong></p>
<p>考虑到 TinyStory 语料文件在 GB 级别，我们没办法一次把所有内容都放在内存中，因此只能process incrementally, save incrementally。</p>
<p>考虑到编码时会在词汇表中指定一个分隔符，而 Pair 是不能跨分隔符的。我们就可以每次从源文件中读取一个 chunk，找到距末尾最近的分隔符，并将之后的内容放进 buffer。然后处理该 chunk 并写入目标文件。处理第二个 chunk 前先在它之前加上 buffer 中内容。</p>
<div class="highlight"><pre tabindex="0" style="color:#abb2bf;background-color:#282c34;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#c678dd">def</span> <span style="color:#61afef;font-weight:bold">encode_file</span>(<span style="color:#e06c75">self</span>, <span style="color:#e06c75">file_path</span>: <span style="color:#e5c07b">str</span>, <span style="color:#e06c75">out_path</span>: <span style="color:#e5c07b">str</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#c678dd">with</span> <span style="color:#e5c07b">open</span>(<span style="color:#e06c75">file_path</span>, <span style="color:#98c379">&#34;r&#34;</span>) <span style="color:#c678dd">as</span> <span style="color:#e06c75">f</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#e06c75">encoded</span> <span style="color:#56b6c2">=</span> []
</span></span><span style="display:flex;"><span>            <span style="color:#e06c75">i</span> <span style="color:#56b6c2">=</span> <span style="color:#d19a66">0</span>
</span></span><span style="display:flex;"><span>            <span style="color:#c678dd">while</span> <span style="color:#e5c07b">True</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#e06c75">chunk</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">f</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">read</span>(<span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">chunk_size</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#c678dd">if</span> <span style="color:#56b6c2">not</span> <span style="color:#e06c75">chunk</span>:
</span></span><span style="display:flex;"><span>                    <span style="color:#c678dd">break</span>
</span></span><span style="display:flex;"><span>                <span style="color:#e06c75">i</span> <span style="color:#56b6c2">+=</span> <span style="color:#d19a66">1</span>
</span></span><span style="display:flex;"><span>                <span style="color:#e06c75">encoded_chunk</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">_process_chunk</span>(<span style="color:#e06c75">chunk</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#e06c75">encoded</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">extend</span>(<span style="color:#e06c75">encoded_chunk</span>)
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>                <span style="color:#7f848e"># 增量保存（每处理100个分块写入一次）</span>
</span></span><span style="display:flex;"><span>                <span style="color:#c678dd">if</span> <span style="color:#e5c07b">len</span>(<span style="color:#e06c75">encoded</span>) <span style="color:#56b6c2">&gt;</span> <span style="color:#d19a66">100</span><span style="color:#56b6c2">*</span><span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">chunk_size</span>:
</span></span><span style="display:flex;"><span>                    <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">_save_incrementally</span>(<span style="color:#e06c75">encoded</span>, <span style="color:#e06c75">out_path</span>)
</span></span><span style="display:flex;"><span>                    <span style="color:#e06c75">encoded</span> <span style="color:#56b6c2">=</span> []
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#7f848e"># 处理最终缓冲区的剩余数据</span>
</span></span><span style="display:flex;"><span>            <span style="color:#c678dd">if</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">buffer</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#e06c75">final_tokens</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">_process_chunk</span>(<span style="color:#98c379">&#34;&#34;</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#e06c75">encoded</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">extend</span>(<span style="color:#e06c75">final_tokens</span>)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">_save_incrementally</span>(<span style="color:#e06c75">encoded</span>, <span style="color:#e06c75">out_path</span>)   
</span></span></code></pre></div><h2 id="model">Model</h2>
<p>当 Tokenizer 把输入转化为 int 数组后，我们就可以直接把该数组输入模型，并得到 <a href="https://www.zhihu.com/question/60751553">logit</a> 了。</p>
<p>下图是 TinyStory 的模型结构：</p>
<p><img src="./assets/image-20250406221311897.png" alt="image-20250406221311897"></p>
<p>下面是其代码实现：</p>
<div class="highlight"><pre tabindex="0" style="color:#abb2bf;background-color:#282c34;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#c678dd">class</span> <span style="color:#e5c07b">TransformerLM</span>(<span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Module</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">def</span> <span style="color:#56b6c2;font-weight:bold">__init__</span>(
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">vocab_size</span>: <span style="color:#e5c07b">int</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">context_length</span>: <span style="color:#e5c07b">int</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">d_model</span>: <span style="color:#e5c07b">int</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">num_layers</span>: <span style="color:#e5c07b">int</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">num_heads</span>: <span style="color:#e5c07b">int</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">d_ff</span>: <span style="color:#e5c07b">int</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">attn_pdrop</span>: <span style="color:#e5c07b">float</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">residual_pdrop</span>: <span style="color:#e5c07b">float</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#56b6c2">**</span><span style="color:#e06c75">kwargs</span>
</span></span><span style="display:flex;"><span>    ):
</span></span><span style="display:flex;"><span>        <span style="color:#e5c07b">super</span>()<span style="color:#56b6c2">.</span><span style="color:#56b6c2;font-weight:bold">__init__</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">embed</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Embedding</span>(<span style="color:#e06c75">vocab_size</span>, <span style="color:#e06c75">d_model</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">pos_embed</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Parameter</span>(<span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">zeros</span>(<span style="color:#e06c75">context_length</span>, <span style="color:#e06c75">d_model</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">drop</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Dropout</span>(<span style="color:#e06c75">residual_pdrop</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">layers</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Sequential</span>(<span style="color:#56b6c2">*</span>[<span style="color:#e06c75">TransformerBlock</span>(<span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">num_heads</span>, <span style="color:#e06c75">d_ff</span>, <span style="color:#e06c75">attn_pdrop</span>, <span style="color:#e06c75">residual_pdrop</span>) <span style="color:#c678dd">for</span> <span style="color:#e06c75">_</span> <span style="color:#56b6c2">in</span> <span style="color:#e5c07b">range</span>(<span style="color:#e06c75">num_layers</span>)])
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">ln_f</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">RMSNorm</span>(<span style="color:#e06c75">d_model</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">head</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Linear</span>(<span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">vocab_size</span>, <span style="color:#e06c75">bias</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">context_length</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">context_length</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">def</span> <span style="color:#61afef;font-weight:bold">forward</span>(<span style="color:#e06c75">self</span>, <span style="color:#e06c75">x</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#7f848e"># x (batch_size, context_length)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">x</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">embed</span>(<span style="color:#e06c75">x</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">x</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">x</span> <span style="color:#56b6c2">+</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">pos_embed</span>[:<span style="color:#e06c75">x</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">size</span>(<span style="color:#d19a66">1</span>), :]
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">x</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">drop</span>(<span style="color:#e06c75">x</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">x</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">layers</span>(<span style="color:#e06c75">x</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">x</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">ln_f</span>(<span style="color:#e06c75">x</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">x</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">head</span>(<span style="color:#e06c75">x</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#c678dd">return</span> <span style="color:#e06c75">x</span>
</span></span></code></pre></div><p>可以看到，模型基本由 Embedding，Attention，FFN，Linear 以及负责把函数变平滑的 Norm，Dropout 和 Residual Connection 组成。不同的大模型结构也大体类似。</p>
<p>Embedding 层负责生成词向量，TransformerBlock 是主要计算发生的地方，而最后的 Linear 层负责生成下一个 Token 的概率分布。</p>
<p><img src="./assets/image-20250406223955121.png" alt="image-20250406223955121"></p>
<p>下面分别介绍每个层。</p>
<h3 id="positional-embedding">Positional Embedding</h3>
<p>这个 layer 负责将 Token 转化为词向量（Word Vector），并在附加上位置信息。</p>
<p>词向量处于一个高维语义空间中，两个向量间的位置关系即可表示他们的语义关系。如 <code>V(法国)-V(巴黎) = V(德国)-V(柏林)</code>。</p>
<p>具体的词向量信息我们在神经网络中学习即可。我们首先要做的是把 Token 转化为高维向量。这个功能由 <code>nn.Embeding</code> 实现。该层会维护一个形状为 <code>(num_embeddings, embedding_dim)</code> 的查找表，把每一个 Token 都映射为高维向量。具体的参数在梯度下降时更新。</p>
<p>除了词嵌入外，我们还需要记录 Token 的位置信息，这个功能由 Positional Embedding 完成。 Positional Embedding 层会给每个不同位置的词都生成一个表示位置的向量，该向量与 Token Embedding 生成的向量相加，得到最终的词向量。</p>
<p><img src="./assets/image-20250407100002999.png" alt="image-20250407100002999"></p>
<p><img src="./assets/image-20250407100118331.png" alt="image-20250407100118331"></p>
<p>Positional Embedding 存在多种实现方法，其中旋转位置编码（RoPE）已被证明效果最优。但鉴于其实现复杂度较高，我们选择采用基础的 Absolute Positional Embeddings。具体实现方式为：创建一个可训练的位置嵌入矩阵，其行数设置为预定义的上下文最大长度（<code>context_length</code>）。在模型前向传播时，根据实际输入序列长度对矩阵进行动态截取，并与词嵌入向量进行逐元素相加。其优点是位置编码可以由学习得到，效果好，缺点是不能处理长度超出 <code>context_length</code> 的句子。</p>
<h3 id="transformerblock">TransformerBlock</h3>
<p><img src="./assets/encoder_with_tensors_2.png" alt="img"></p>
<p>上图是一个简化版的 Transformer，其核心组件由两个，Self-Attention 和 FFN。Attention 负责捕捉序列信息，将单独的词向量变为语境中的词向量（真正的能指）；而 FFN 负责存储知识信息，是整个 LM 中参数量最多的地方，也是训练推理时计算量最大的地方。</p>
<h4 id="attention">Attention</h4>
<p>对于变长序列信息，我们需要有一种办法来考虑序列信息。在 attention 之前，我们一般使用 RNN 来完成这点。RNN 会引入隐状态，每个 Token 的计算都会依赖前一个 Token 的隐状态，并重新写入隐状态，以此来传递序列信息。</p>
<p><img src="./assets/image-20250407102918670.png" alt="image-20250407102918670"></p>
<p>但是因为隐状态的相邻依赖关系，RNN 难以<strong>并行化</strong>，且难以捕捉<strong>长距离依赖</strong>，因此就有了 self-attention。最初的 attention 的在 RNN 语言模型的 input 序列和 output 序列间传递信息，但是 Transformer 的研究者发现让序列自己做 self-attention，就可以取代 RNN 了。</p>
<p>虽然 RNN 有着上述缺点，但是因为其恒定推理成本和在短序列中的低成本，也被广泛使用。</p>
<p><img src="./assets/image-20250407103148923.png" alt="image-20250407103148923"></p>
<p>在 attention 机制中，我们有 Q, K, V 三个可学习矩阵。每个词向量分别乘着三个矩阵得到 q，k，v 三个向量。每个词的 q 向量和其他词的 k 向量先点积再做 softmax，得到注意力分数。再根据注意力分数对所有 v 做加权和，得到 attention 值。此时的 attention 值就包括了句中其他词的信息。</p>
<p><img src="./assets/image-20250407112727986.png" alt="image-20250407112727986"></p>
<p>上述过程写成矩阵的形式如下：</p>
<p><img src="./assets/image-20250407105446994.png" alt="image-20250407105446994"></p>
<p>下面是<a href="https://jalammar.github.io/illustrated-transformer/">图示</a>：</p>
<p><img src="./assets/self-attention-matrix-calculation.png" alt="img"></p>
<p><img src="./assets/self-attention-matrix-calculation-2.png" alt="img"></p>
<p>但在 decoder-only LM 中，我们要预测下一个 Token，因此计算 attention 时，一个 Token 的 q 只能于它之前的 Token 的 k 做计算，否则在训练时就提前泄露了未来信息。所以我们在计算注意力分数时要加上掩码，不需要的分数设置为一个极小值，这样 softmax 后其概率就接近为 0 了。</p>
<p>一个 attention 头（即一组 Q,K,V）可以学习一种关联模式，我们可以使用多组 attention 头来实现多头注意力机制（Multi-Head Attention）。简而言之，多头注意力就是使用多个 Q,K,V计算多个 attention 值，然后把所有的 attention 值连在一起，经过一个 Linear 层得到最后的 attention。</p>
<p><img src="./assets/image-20250407113734426.png" alt="image-20250407113734426"></p>
<p><img src="./assets/image-20250407113508298.png#half" alt="image-20250407113508298"></p>
<p><img src="./assets/transformer_multi-headed_self-attention-recap.png" alt="img"></p>
<p>在实际实现中，多个 Q K V 可以拼成一个矩阵，一起运算，这样就减少了计算开销。</p>
<div class="highlight"><pre tabindex="0" style="color:#abb2bf;background-color:#282c34;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#c678dd">def</span> <span style="color:#61afef;font-weight:bold">scaled_dot_product_attention</span>(<span style="color:#e06c75">K</span>, <span style="color:#e06c75">Q</span>, <span style="color:#e06c75">V</span>, <span style="color:#e06c75">mask</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">None</span>, <span style="color:#e06c75">pdrop</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">None</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#7f848e"># K, Q (batch_size, [num_heads,] seq_len, k_dim)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#7f848e"># V (batch_size, [num_heads,] seq_len, v_dim)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#7f848e"># attn_scores (batch_size, [num_heads,] seq_len, seq_len)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">matmul</span>(<span style="color:#e06c75">Q</span>, <span style="color:#e06c75">K</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">transpose</span>(<span style="color:#56b6c2">-</span><span style="color:#d19a66">2</span>, <span style="color:#56b6c2">-</span><span style="color:#d19a66">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">/</span> <span style="color:#e06c75">math</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">sqrt</span>(<span style="color:#e06c75">K</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">shape</span>[<span style="color:#56b6c2">-</span><span style="color:#d19a66">1</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">if</span> <span style="color:#e06c75">mask</span> <span style="color:#56b6c2">is</span> <span style="color:#56b6c2">not</span> <span style="color:#e5c07b">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">attn_scores</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">masked_fill</span>(<span style="color:#e06c75">mask</span>, <span style="color:#56b6c2">-</span><span style="color:#d19a66">1e9</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">softmax</span>(<span style="color:#e06c75">attn_scores</span>, <span style="color:#e06c75">dim</span><span style="color:#56b6c2">=-</span><span style="color:#d19a66">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">attn_scores</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">masked_fill</span>(<span style="color:#e06c75">mask</span>, <span style="color:#d19a66">0.0</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">softmax</span>(<span style="color:#e06c75">attn_scores</span>, <span style="color:#e06c75">dim</span><span style="color:#56b6c2">=-</span><span style="color:#d19a66">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">if</span> <span style="color:#e06c75">pdrop</span> <span style="color:#56b6c2">is</span> <span style="color:#56b6c2">not</span> <span style="color:#e5c07b">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Dropout</span>(<span style="color:#e06c75">pdrop</span>)(<span style="color:#e06c75">attn_scores</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">return</span> <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">matmul</span>(<span style="color:#e06c75">attn_scores</span>, <span style="color:#e06c75">V</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#c678dd">class</span> <span style="color:#e5c07b">MultiHeadSelfAttention</span>(<span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Module</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">def</span> <span style="color:#56b6c2;font-weight:bold">__init__</span>(<span style="color:#e06c75">self</span>, <span style="color:#e06c75">d_model</span>: <span style="color:#e5c07b">int</span>, <span style="color:#e06c75">num_heads</span>: <span style="color:#e5c07b">int</span>, <span style="color:#e06c75">dropout</span>: <span style="color:#e06c75">Optional</span>[<span style="color:#e5c07b">float</span>] <span style="color:#56b6c2">=</span> <span style="color:#e5c07b">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e5c07b">super</span>()<span style="color:#56b6c2">.</span><span style="color:#56b6c2;font-weight:bold">__init__</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#c678dd">assert</span> <span style="color:#e06c75">d_model</span> <span style="color:#56b6c2">%</span> <span style="color:#e06c75">num_heads</span> <span style="color:#56b6c2">==</span> <span style="color:#d19a66">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">num_heads</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">num_heads</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">d_model</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">d_model</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">d_k</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">d_model</span> <span style="color:#56b6c2">//</span> <span style="color:#e06c75">num_heads</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w_q</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Linear</span>(<span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">bias</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w_k</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Linear</span>(<span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">bias</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w_v</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Linear</span>(<span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">bias</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w_o</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Linear</span>(<span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">bias</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">dropout</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">dropout</span>
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">def</span> <span style="color:#61afef;font-weight:bold">forward</span>(
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">in_features</span>: <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">FloatTensor</span>,
</span></span><span style="display:flex;"><span>    ):
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">Q</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w_q</span>(<span style="color:#e06c75">in_features</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">K</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w_k</span>(<span style="color:#e06c75">in_features</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">V</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w_v</span>(<span style="color:#e06c75">in_features</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">batch_size</span>, <span style="color:#e06c75">seq_len</span>, <span style="color:#e06c75">_</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">Q</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">size</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">Q</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">Q</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">view</span>(<span style="color:#e06c75">batch_size</span>, <span style="color:#e06c75">seq_len</span>, <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">num_heads</span>, <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">d_k</span>)<span style="color:#56b6c2">.</span><span style="color:#e06c75">transpose</span>(<span style="color:#d19a66">1</span>, <span style="color:#d19a66">2</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">K</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">K</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">view</span>(<span style="color:#e06c75">batch_size</span>, <span style="color:#e06c75">seq_len</span>, <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">num_heads</span>, <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">d_k</span>)<span style="color:#56b6c2">.</span><span style="color:#e06c75">transpose</span>(<span style="color:#d19a66">1</span>, <span style="color:#d19a66">2</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">V</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">V</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">view</span>(<span style="color:#e06c75">batch_size</span>, <span style="color:#e06c75">seq_len</span>, <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">num_heads</span>, <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">d_k</span>)<span style="color:#56b6c2">.</span><span style="color:#e06c75">transpose</span>(<span style="color:#d19a66">1</span>, <span style="color:#d19a66">2</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">mask</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">triu</span>(<span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">ones</span>(<span style="color:#e06c75">seq_len</span>, <span style="color:#e06c75">seq_len</span>, <span style="color:#e06c75">device</span><span style="color:#56b6c2">=</span><span style="color:#e06c75">K</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">device</span>), <span style="color:#e06c75">diagonal</span><span style="color:#56b6c2">=</span><span style="color:#d19a66">1</span>)<span style="color:#56b6c2">.</span><span style="color:#e06c75">bool</span>()
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">scaled_dot_product_attention</span>(<span style="color:#e06c75">K</span>, <span style="color:#e06c75">Q</span>, <span style="color:#e06c75">V</span>, <span style="color:#e06c75">mask</span>, <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">dropout</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">attn_scores</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">attn_scores</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">transpose</span>(<span style="color:#d19a66">1</span>, <span style="color:#d19a66">2</span>)<span style="color:#56b6c2">.</span><span style="color:#e06c75">contiguous</span>()<span style="color:#56b6c2">.</span><span style="color:#e06c75">view</span>(<span style="color:#e06c75">batch_size</span>, <span style="color:#e06c75">seq_len</span>, <span style="color:#56b6c2">-</span><span style="color:#d19a66">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#c678dd">return</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w_o</span>(<span style="color:#e06c75">attn_scores</span>)
</span></span></code></pre></div><h4 id="ffn">FFN</h4>
<p>FFN 层是 Transformer 中计算最多的地方。这里可以控制的变量有：</p>
<ol>
<li>隐藏层的维度</li>
<li>激活函数的选择</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#abb2bf;background-color:#282c34;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#c678dd">class</span> <span style="color:#e5c07b">PointwiseFeedForward</span>(<span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Module</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">def</span> <span style="color:#56b6c2;font-weight:bold">__init__</span>(<span style="color:#e06c75">self</span>, <span style="color:#e06c75">d_model</span>: <span style="color:#e5c07b">int</span>, <span style="color:#e06c75">d_ff</span>: <span style="color:#e5c07b">int</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e5c07b">super</span>()<span style="color:#56b6c2">.</span><span style="color:#56b6c2;font-weight:bold">__init__</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w1</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Linear</span>(<span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">d_ff</span>, <span style="color:#e06c75">bias</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w2</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">nn</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">Linear</span>(<span style="color:#e06c75">d_ff</span>, <span style="color:#e06c75">d_model</span>, <span style="color:#e06c75">bias</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">def</span> <span style="color:#61afef;font-weight:bold">forward</span>(<span style="color:#e06c75">self</span>, <span style="color:#e06c75">x</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#c678dd">return</span> <span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w2</span>(<span style="color:#e06c75">gelu</span>(<span style="color:#e06c75">self</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">w1</span>(<span style="color:#e06c75">x</span>)))
</span></span></code></pre></div><p>此外，现在许多 LLM 在 FFN 层采用 <a href="https://www.youtube.com/watch?v=sOPDGQjFcuM&amp;ab_channel=MaartenGrootendorst">MoE</a>  架构。关键在于，把 FFN 拆成许多小 FFN，运行时选择一个 FFN （专家/export）传播。这样就能保证模型参数的同时提升推理速度。</p>
<h3 id="others">Others</h3>
<p>模型剩下的组件还包括，RMSNorm，Residual Connection，以及 Dropout。</p>
<p>RMSNorm 和 Residual Connection 都可以避免梯度消失和梯度保证，保证函数的 <a href="https://www.bilibili.com/video/BV1B64y157DC/?share_source=copy_web&amp;vd_source=70a8bfe51ff36d2bca97cf3df1c52fed">Lipschitz 连续</a>性质。</p>
<p>而 Dropout 属于一种正则化方法，通过随机将某些参数置为 0，来避免模型过分依赖某些参数，从而保证了其泛化性。</p>
<p>此外，模型参数的初始化也十分重要。有许多实验表示初始化的好坏对于模型训练过程有很大影响，并且训练结果与初始化偏差并不大。通过合理的初始化（比如对于方差，均值的调整），我们可以使得函数更易于训练。</p>
<h2 id="train">Train</h2>
<p>深度学习除了作为 hypothesis 的模型结构，损失函数和优化方法也是训练时不可或缺的组成部分。</p>
<h3 id="cross-entropy-loss">Cross-entropy loss</h3>
<p>在熵的公式中，熵越大，分布越均匀（不确定性越高）；熵越小，分布越集中（确定性越高）。</p>
<p><img src="./assets/image-20250407203040513.png" alt="image-20250407203040513"></p>
<p>交叉熵衡量两个概率分布之间的差异，对于真实分布 <em>p</em> 和模型预测分布 <em>q</em>，交叉熵的公式为：</p>
<p><img src="./assets/image-20250407201910928.png" alt="image-20250407201910928"></p>
<p>若 <em>q</em>(<em>x</em>) 与 <em>p</em>(<em>x</em>) 完全一致，则交叉熵等于熵 <em>H</em>(<em>p</em>)，否则 <em>H</em>(<em>p</em>,<em>q</em>)&gt;<em>H</em>(<em>p</em>)。</p>
<p>在训练中 p(x) 一般为真实标签的 <strong>one-hot 分布</strong>（仅正确类别为1，其他为0），因此只用考虑正确类别。因此，LLM 中使用的交叉熵公式如下：</p>
<p><img src="./assets/image-20250407202734306.png" alt="image-20250407202734306"></p>
<p>对于 one-hot 分布，熵为0，因此交叉熵的最小值也为0。 在训练语言模型时，假设初始状态为均匀分布，收敛状态预测正确 token 概率为 0.9，则交叉熵在区间 [0.1,10.82] 内。</p>
<p>代码实现如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#abb2bf;background-color:#282c34;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#c678dd">def</span> <span style="color:#61afef;font-weight:bold">cross_entropy_loss</span>(<span style="color:#e06c75">inputs</span>, <span style="color:#e06c75">target</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">max_vals</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">max</span>(<span style="color:#e06c75">inputs</span>, <span style="color:#e06c75">dim</span><span style="color:#56b6c2">=-</span><span style="color:#d19a66">1</span>, <span style="color:#e06c75">keepdim</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">True</span>)<span style="color:#56b6c2">.</span><span style="color:#e06c75">values</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">shifted_inputs</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">inputs</span> <span style="color:#56b6c2">-</span> <span style="color:#e06c75">max_vals</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">exp_inputs</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">exp</span>(<span style="color:#e06c75">shifted_inputs</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">exp_inputs_sum</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">sum</span>(<span style="color:#e06c75">exp_inputs</span>, <span style="color:#e06c75">dim</span><span style="color:#56b6c2">=-</span><span style="color:#d19a66">1</span>, <span style="color:#e06c75">keepdim</span><span style="color:#56b6c2">=</span><span style="color:#e5c07b">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">log_softmax_inputs</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">shifted_inputs</span> <span style="color:#56b6c2">-</span> <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">log</span>(<span style="color:#e06c75">exp_inputs_sum</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">loss</span> <span style="color:#56b6c2">=</span> <span style="color:#56b6c2">-</span><span style="color:#e06c75">log_softmax_inputs</span>[<span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">arange</span>(<span style="color:#e06c75">target</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">shape</span>[<span style="color:#d19a66">0</span>]), <span style="color:#e06c75">target</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">return</span> <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">mean</span>(<span style="color:#e06c75">loss</span>)
</span></span></code></pre></div><h3 id="adamw">AdamW</h3>
<p>神经网络最经典的优化算法为随机梯度下降法。<a href="https://arxiv.org/abs/1711.05101v3">AdamW</a> 在此基础上增加了：</p>
<ol>
<li>一阶矩 动量 m：可以减小随机batch带来的震荡</li>
<li>二阶矩 自适应学习率 v：不同参数的梯度数值大小不同，通过除二阶矩的开放来使得梯度稳定，避免大梯度爆炸，小梯度消失</li>
<li>第四步对学习率的调整实际上是为了应对一开始 m 和 v 太小的情况</li>
<li>weight decay：减去参数，增强泛化性
<ol>
<li>L2 正则中，正则项梯度会被自适应学习率缩放，导致实际衰减强度与学习率耦合，影响正则化效果。</li>
</ol>
</li>
</ol>
<p><img src="./assets/image-20250407204359210.png" alt="image-20250407204359210"></p>
<h3 id="learning-rate-scheduling">Learning rate scheduling</h3>
<p>学习率调整的核心作用有：</p>
<ol>
<li>加速初期收敛：训练初期使用较大学习率，快速逼近最优解区域。</li>
<li>避免后期震荡：后期逐步降低学习率，防止在最优解附近震荡。</li>
<li>逃离局部极小值：周期性或突变的调整策略（如重启）可帮助跳出局部最优。</li>
</ol>
<p>通过<strong>余弦函数</strong>平滑地将学习率从初始值（<em>η</em>max）降低到最小值（<em>η</em>min），形成一个周期性的退火过程。其特点是<strong>平滑过渡</strong>，避免学习率突变导致的训练不稳定。</p>
<p><img src="./assets/image-20250407212302447.png" alt="image-20250407212302447"></p>
<p>代码实现如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#abb2bf;background-color:#282c34;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#c678dd">def</span> <span style="color:#61afef;font-weight:bold">get_lr_cosine_schedule</span>(<span style="color:#e06c75">t</span>: <span style="color:#e5c07b">int</span>, <span style="color:#e06c75">alpha_max</span>: <span style="color:#e5c07b">float</span>, <span style="color:#e06c75">alpha_min</span>: <span style="color:#e5c07b">float</span>, 
</span></span><span style="display:flex;"><span>                          <span style="color:#e06c75">Tw</span>: <span style="color:#e5c07b">int</span>, <span style="color:#e06c75">Tc</span>: <span style="color:#e5c07b">int</span>) <span style="color:#56b6c2">-&gt;</span> <span style="color:#e5c07b">float</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">if</span> <span style="color:#e06c75">t</span> <span style="color:#56b6c2">&lt;</span> <span style="color:#e06c75">Tw</span> <span style="color:#56b6c2">and</span> <span style="color:#e06c75">Tw</span> <span style="color:#56b6c2">&gt;</span> <span style="color:#d19a66">0</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#c678dd">return</span> <span style="color:#e06c75">alpha_max</span> <span style="color:#56b6c2">*</span> (<span style="color:#e06c75">t</span> <span style="color:#56b6c2">/</span> <span style="color:#e06c75">Tw</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">if</span> <span style="color:#e06c75">Tw</span> <span style="color:#56b6c2">&lt;=</span> <span style="color:#e06c75">t</span> <span style="color:#56b6c2">&lt;=</span> <span style="color:#e06c75">Tc</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">ratio</span> <span style="color:#56b6c2">=</span> (<span style="color:#e06c75">t</span> <span style="color:#56b6c2">-</span> <span style="color:#e06c75">Tw</span>) <span style="color:#56b6c2">/</span> (<span style="color:#e06c75">Tc</span> <span style="color:#56b6c2">-</span> <span style="color:#e06c75">Tw</span>)  <span style="color:#7f848e"># Progress through cosine phase</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">cosine_decay</span> <span style="color:#56b6c2">=</span> <span style="color:#d19a66">0.5</span> <span style="color:#56b6c2">*</span> (<span style="color:#d19a66">1</span> <span style="color:#56b6c2">+</span> <span style="color:#e06c75">math</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">cos</span>(<span style="color:#e06c75">math</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">pi</span> <span style="color:#56b6c2">*</span> <span style="color:#e06c75">ratio</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#c678dd">return</span> <span style="color:#e06c75">alpha_min</span> <span style="color:#56b6c2">+</span> (<span style="color:#e06c75">alpha_max</span> <span style="color:#56b6c2">-</span> <span style="color:#e06c75">alpha_min</span>) <span style="color:#56b6c2">*</span> <span style="color:#e06c75">cosine_decay</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">return</span> <span style="color:#e06c75">alpha_min</span>
</span></span></code></pre></div><h3 id="gradient-clipping">Gradient clipping</h3>
<p>梯度裁剪用于控制梯度的大小，防止训练过程中因梯度爆炸导致的数值不稳定。它通过限制梯度的最大值或范数，确保参数更新步长合理，从而提升模型训练的稳定性和收敛性。</p>
<p><img src="./assets/image-20250407212810318.png" alt="image-20250407212810318"></p>
<p>代码实现如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#abb2bf;background-color:#282c34;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#c678dd">def</span> <span style="color:#61afef;font-weight:bold">clip_gradient</span>(<span style="color:#e06c75">parameters</span>, <span style="color:#e06c75">max_norm</span>: <span style="color:#e5c07b">float</span>, <span style="color:#e06c75">eps</span>: <span style="color:#e5c07b">float</span> <span style="color:#56b6c2">=</span> <span style="color:#d19a66">1e-6</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">grads</span> <span style="color:#56b6c2">=</span> [<span style="color:#e06c75">p</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">grad</span> <span style="color:#c678dd">for</span> <span style="color:#e06c75">p</span> <span style="color:#56b6c2">in</span> <span style="color:#e06c75">parameters</span> <span style="color:#c678dd">if</span> <span style="color:#e06c75">p</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">grad</span> <span style="color:#56b6c2">is</span> <span style="color:#56b6c2">not</span> <span style="color:#e5c07b">None</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">if</span> <span style="color:#56b6c2">not</span> <span style="color:#e06c75">grads</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#c678dd">return</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">device</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">grads</span>[<span style="color:#d19a66">0</span>]<span style="color:#56b6c2">.</span><span style="color:#e06c75">device</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">total_norm</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">norm</span>(
</span></span><span style="display:flex;"><span>        <span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">stack</span>([<span style="color:#e06c75">torch</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">norm</span>(<span style="color:#e06c75">g</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">detach</span>(), <span style="color:#d19a66">2</span>) <span style="color:#c678dd">for</span> <span style="color:#e06c75">g</span> <span style="color:#56b6c2">in</span> <span style="color:#e06c75">grads</span>]), 
</span></span><span style="display:flex;"><span>        <span style="color:#d19a66">2</span>
</span></span><span style="display:flex;"><span>    )<span style="color:#56b6c2">.</span><span style="color:#e06c75">to</span>(<span style="color:#e06c75">device</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">scaling_factor</span> <span style="color:#56b6c2">=</span> <span style="color:#e06c75">max_norm</span> <span style="color:#56b6c2">/</span> (<span style="color:#e06c75">total_norm</span> <span style="color:#56b6c2">+</span> <span style="color:#e06c75">eps</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#c678dd">if</span> <span style="color:#e06c75">total_norm</span> <span style="color:#56b6c2">&gt;</span> <span style="color:#e06c75">max_norm</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#c678dd">for</span> <span style="color:#e06c75">grad</span> <span style="color:#56b6c2">in</span> <span style="color:#e06c75">grads</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#e06c75">grad</span><span style="color:#56b6c2">.</span><span style="color:#e06c75">mul_</span>(<span style="color:#e06c75">scaling_factor</span>)
</span></span></code></pre></div><h3 id="resource-accounting">Resource accounting</h3>
<p>一个简单的资源估算：</p>
<p>基本想法：考虑 Y = WX，反向传播时需要分别对 W 和 X 求导，因此反向传播计算量约为正向传播的两倍。</p>
<p><img src="./assets/image-20250407151357762.png" alt="image-20250407151357762"></p>
<h3 id="train-in-practice">Train in Practice</h3>
<p>把上面所有组件组合在一起后，就可以进行训练了。</p>
<p>实际训练时的参数配置如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#abb2bf;background-color:#282c34;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#7f848e">// 模型超参数
</span></span></span><span style="display:flex;"><span><span style="color:#7f848e"></span>    <span style="color:#e06c75">&#34;vocab_size&#34;</span>: <span style="color:#d19a66">10000</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;context_length&#34;</span>: <span style="color:#d19a66">256</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;d_model&#34;</span>: <span style="color:#d19a66">512</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;num_layers&#34;</span>: <span style="color:#d19a66">4</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;num_heads&#34;</span>: <span style="color:#d19a66">16</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;d_ff&#34;</span>: <span style="color:#d19a66">2048</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;attn_pdrop&#34;</span>: <span style="color:#d19a66">0.1</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;residual_pdrop&#34;</span>: <span style="color:#d19a66">0.1</span>,
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#7f848e">// 优化器超参数
</span></span></span><span style="display:flex;"><span><span style="color:#7f848e"></span>    <span style="color:#e06c75">&#34;learning_rate&#34;</span>: <span style="color:#d19a66">0.001</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;beta1&#34;</span>: <span style="color:#d19a66">0.9</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;beta2&#34;</span>: <span style="color:#d19a66">0.95</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;epsilon&#34;</span>: <span style="color:#d19a66">1e-8</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;weight_decay&#34;</span>: <span style="color:#d19a66">0.01</span>,
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#7f848e">// 训练过程超参
</span></span></span><span style="display:flex;"><span><span style="color:#7f848e"></span>    <span style="color:#e06c75">&#34;batch_size&#34;</span>: <span style="color:#d19a66">64</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;epochs&#34;</span>: <span style="color:#d19a66">10</span>,
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#7f848e">// 其他训练相关参数
</span></span></span><span style="display:flex;"><span><span style="color:#7f848e"></span>    <span style="color:#e06c75">&#34;save_every&#34;</span>: <span style="color:#d19a66">1000</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;save_path&#34;</span>: <span style="color:#98c379">&#34;checkpoints/&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;train_data&#34;</span>: <span style="color:#98c379">&#34;valid.npy&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;valid_data&#34;</span>: <span style="color:#98c379">&#34;valid.npy&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;result_model&#34;</span>: <span style="color:#98c379">&#34;model/model.pth&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e06c75">&#34;log_path&#34;</span>: <span style="color:#98c379">&#34;logs/log.txt&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>实际训练时，在一张 4090 显卡上运行了 1h30min。</p>
<p>训练时出了许多 bug（比如奇奇怪怪的  cuda 报错），checkpoint 非常实用。</p>
<p><img src="./assets/image-20250407143713850.png" alt="image-20250407143713850"></p>
<p>下图是 loss 曲线，可以看到在 10000 个迭代后 loss 曲线就不怎动了。最终 loss 为 0.9105。</p>
<blockquote>
<p>可能因为每隔 100 个迭代才记录一次学习率，所以从图中看不出周期性</p>
<p>同时，Tc 可能设置过小，导致后期 loss 降不下去</p></blockquote>
<p><img src="./assets/image-20250407144619463.png" alt="image-20250407144619463"></p>
<h2 id="run">Run</h2>
<p>模型训练好之后，就是推理了！</p>
<p>当输入一个句子时，模型会生成表示下一个Token可能性的logits值。这些logits会除以称为Temperature的超参数——Temperature值越大，模型选择低频词的概率越高。之后我们筛选出概率最高的前k个候选Token（top-k sampling），通过softmax函数将筛选后的logits转换为概率分布，然后依据该概率分布随机选取一个Token作为输出。</p>
<p>之后，我们将新生成的Token追加至输入序列末尾，重新输入模型进行下一轮预测，循环直至出现终止符（如）或达到最大生成长度。</p>
<blockquote>
<p>文本生成中，也常常使用 <a href="https://www.wikiwand.com/en/articles/Beam_search">beam search</a> 来选择 Token</p></blockquote>
<p><img src="./assets/image-20250407220917943.png" alt="image-20250407220917943"></p>
        </section>
        <div class="post-tags">
          
          
          <nav class="nav tags">
            <ul class="tags">
              
              <li><a href="/tags/llm">LLM</a></li>
              
              <li><a href="/tags/code">Code</a></li>
              
            </ul>
          </nav>
          
          
        </div>
      </div>

      
      
      <div class="toc">
        <strong>Table of contents:</strong>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#intro">Intro</a></li>
    <li><a href="#architecture">Architecture</a></li>
    <li><a href="#tokenizer">Tokenizer</a>
      <ul>
        <li><a href="#byte-pair-encoding">Byte-Pair Encoding</a></li>
        <li><a href="#bpe-implementation">BPE Implementation</a></li>
      </ul>
    </li>
    <li><a href="#model">Model</a>
      <ul>
        <li><a href="#positional-embedding">Positional Embedding</a></li>
        <li><a href="#transformerblock">TransformerBlock</a></li>
        <li><a href="#others">Others</a></li>
      </ul>
    </li>
    <li><a href="#train">Train</a>
      <ul>
        <li><a href="#cross-entropy-loss">Cross-entropy loss</a></li>
        <li><a href="#adamw">AdamW</a></li>
        <li><a href="#learning-rate-scheduling">Learning rate scheduling</a></li>
        <li><a href="#gradient-clipping">Gradient clipping</a></li>
        <li><a href="#resource-accounting">Resource accounting</a></li>
        <li><a href="#train-in-practice">Train in Practice</a></li>
      </ul>
    </li>
    <li><a href="#run">Run</a></li>
  </ul>
</nav>
      </div>
      
    </div>

    </article>
</main>
<footer>
  <div style="display:flex"><a class="soc" href="https://github.com/RZYN2020" rel="me" title="Github"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#%25!s%28%3cnil%3e%29" />
</svg></a><a class="border"></a><a class="soc" href="mailto:zhaoyzzz@outlook.com" rel="me" title="Email"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#%25!s%28%3cnil%3e%29" />
</svg></a><a class="border"></a><a class="soc" href="https://rzyn2020.github.io/resume/resume.html" rel="me" title="Resume"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#%25!s%28%3cnil%3e%29" />
</svg></a><a class="border"></a><a class="soc" href="https://neodb.social/users/Ekstasis" rel="me" title="Neodb"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#%25!s%28%3cnil%3e%29" />
</svg></a><a class="border"></a><a class="soc" href="https://myanimelist.net/animelist/yuki960" rel="me" title="Mal"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#%25!s%28%3cnil%3e%29" />
</svg></a><a class="border"></a></div>
  <div class="footer-info">
    2025  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>



</div>
    </body>
</html>
